{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Prompting the Past: Large Language Models as Versatile Tools for Digital Historians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "contributor"
    ]
   },
   "source": [
    " ### Daniel Hutchinson [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0003-2759-5318) \n",
    "\n",
    "Belmont Abbey College"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "cover"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAACWBAMAAABkyf1EAAAAG1BMVEXMzMyWlpacnJyqqqrFxcWxsbGjo6O3t7e+vr6He3KoAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEcElEQVR4nO2aTW/bRhCGh18ij1zKknMkbbf2UXITIEeyMhIfRaF1exQLA/JRclslRykO+rs7s7s0VwytNmhJtsA8gHZEcox9PTs7uysQgGEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmGYr2OWRK/ReIKI8Zt7Hb19wTcQ0uTkGh13bQupcw7gPOvdo12/5CzNtNR7xLUtNtT3CGBQ6g3InjY720pvofUec22LJPr8PhEp2OMPyI40PdwWUdronCu9yQpdPx53bQlfLKnfOVhlnDYRBXve4Ov+IZTeMgdedm0NR+xoXJeQvdJ3CvziykSukwil16W/Oe7aGjIjqc/9ib4jQlJy0uArtN4A0+cvXFvDkmUJ47sJ1Y1ATLDNVXZkNPIepQzxy1ki9fqiwbUj/I+64zxWNzyZnPuhvohJ9K70VvXBixpcu2SAHU+Xd9EKdEJDNpYP3AQr3bQSpPQ6Y6/4dl1z7ZDbArsszjA7L0g7ibB0CDcidUWVoErvIMKZh2Xs0LUzcLW6V5NfiUgNEbaYmAVL6bXl0nJRc+1S72ua/D/cTjGPlQj7eUqd7A096rYlRjdPYlhz7VIvxpVG3cemDKF+WAwLY/6XelOZKTXXzsC4xvDjjtSN6kHLhLke6PrwM8h1raf40qjrGO7H9aTEbduucjS04ZrYU/4iuS5Z2Hdt0rvCLFdmLEXcU30AGddST62o+sLcf5l6k7CP+ru4pLYqX/VFyxbm/utQbx/r22ZEbTb2f5I2kns1Y1OQR8ZyofX+TjJxj1Rz7QQVnf1QzR26Oth0ueJVYcRP6ZUPac/Rx/5M6ixO1dhSrT3Y1DpiYmx3tF4ZUdpz9LD/dSg9PXES0LB71BwcGjKROuV28lnvnv7HHJsezheBGH5+X2CfSfRbMKW+5aGs3JFjMrjGibJc0S7TJzqjHrh2hDybj9XRXNZa89Aro55XBdbW5wti2c/5WJ7jJ1RolVUn/HWpb0I58Tziup6Rx7Dm2hnbRP1GM9PW/NFmQ4PtVRVN63Wvxfmu5sowDMMwDMMwDMMwDMMwDMMwDMMwzL+CpT//F/6beoV8zb2Jmt4Qryx6lTUCsENQ75HOkhXAO3EPVgyQtKtUy3C/e+FJg17Zjnew1Xrdb9InbG4WqfUAftG+WhLwPVyfg536+MU7m4C1CMk4ZznpXZzDYI1PDL2nS1hpvc5cNd7E2sJg05Fe7/7d3Fln8Cvc3bwB616auxsKl4WPghjemHrDqyDWeu1UNW5s2btPnSQ75oOdunEwWazfwgVG0kqluYCM9OIjWOGnfA2b9G4Ha63XKpvQ8perTvTifJNhi6+WMWmi7smEZf6G8MmhlyGq+NqP8GV84TLuJr7UIQVx+bDEoEpRZIz42gs40OuN4Mv8hXzelV7KX1isH+ewTWckikyVv+CfHuqVF7I16gN0VKypX6wPsE+zFPzkinolU9UH8OMGvSpnZqKsv13p/RsMun6X5x/y2LeAr8O66lsBwzBMP/wJfyGq8pgBk6IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"./media/placeholder.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "Large language models, artifical intelligence, machine learning, historical methodology, optical character recognition, oral history, prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "This article examines how digital historians are using large language models (LLMs) in their research and teaching, along with the critical and ethical debates surrounding their use. The article first assesses the historical capacities of LLMs as measured by machine learning benchmarks, and how such assessments can help historians understand the capacities and limits of these technologies. The utility of LLMs as digital tools are then demonstrated through a series of case studies using GPT-4 and other generative AI models. LLMs are tasked with a variety of tasks for streamlining data preparation, such as oral history transcription, correcting optical character recognition (OCR) errors, and metadata extraction. These case studies also demonstrate how frameworks for using LLMs, such as prompt engineering and retrieval augmented generation (RAG), are used to ground LLM outputs for consistency and greater accuracy. Acknowledging the significant ethical challenges posed by LLMs, the article emphasizes the need for critical engagement and the development of responsible frameworks for implementing these technologies in historical scholarship. By combining disciplinary expertise with innovative computational approaches, historians are discovering new ways to navigate the \"unheard-of historical abundance\" of the digital age, contributing to approaches to generative AI that enriches, rather than distorts, our understanding of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first paragrah of running text with a citation example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "abj0n": [
       {
        "id": "27937/CJYNFHVI",
        "source": "zotero"
       }
      ],
      "kc733": [
       {
        "id": "27937/6DE3XGUT",
        "source": "zotero"
       }
      ],
      "x111d": [
       {
        "id": "27937/L2ILKERU",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "In 2003, Roy Rosenzweig predicted that digital historians would need to develop new techniques \"to research, write, and teach in a world of unheard-of historical abundance.\" (<cite id=\"abj0n\"><a href=\"#zotero%7C27937%2FCJYNFHVI\">Rosenzweig, “Scarcity or Abundance?”</a></cite>)\n",
    " Over the past two decades historians have risen to this challenge, embracing digital mapping, network analysis, distant reading of large text collections, and machine learning as part of their growing methodological toolkit. (<cite id=\"x111d\"><a href=\"#zotero%7C27937%2FL2ILKERU\">Graham, Milligan, and Weingart, <i>Exploring Big Historical Data</i>.</a></cite>) Generative artificial intelligence (AI) has emerged as another potential tool that historians are using to explore the past, particularly large language models (LLMs), the most prominent form of this technology. These models possess striking capacities to generate, interpret, and manipulate data across a range of modalities. The rapidly-expanding scope of these capabilities and their limits remain intensely debated, as do their broader social, economic, cultural, and environmental impacts. (<cite id=\"kc733\"><a href=\"#zotero%7C27937%2F6DE3XGUT\">Crane, “AI, Language, and the Humanities.”</a></cite>) Yet while still an emerging technology, historians are already demonstrating generative AI's potential as a versatile digital tool. Historians are also contributing to the critical discourse surrounding this new domain, raising key questions about how these models achieve their capabilities, their propensity to reinforce existing inequalities, and their potential to distort our understanding of the past. [include AHA forum citation]\n",
    "\n",
    "This article contributes to this discourse by demonstrating how digital historians are using generative AI to explore the past, as well as the disciplinary opportunities historians can offer to these broader debates. We begin by assessing the metrics commonly used to measure the historical knowledge of LLMs, and examine how such metrics can give us insights into the capacities and limits this technology. We then examine how generative AI can be used in tasks as varied as preparing datasets, exploring text collections, and offering novel (and controversial) methods of representing the past. We conclude with a call to historians to continue to contribute to ongoing research and debates concerning the ethical use of generative AI. Given the rapid pace of innovation in this field, it is crucial that the profession addresses the implications of this technology for our research and teaching. Historians will have much to contribute in contextualizing the innovative and disruptive potential of these breakthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do AIs Know About History? Assessing LLMs for Historical Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "86ncy": [],
      "e97pi": [],
      "rlvh9": [],
      "x5wiq": [],
      "y4y0i": []
     }
    }
   },
   "source": [
    "As historians explore the possibilities of generative AI, it is important to understand how these technologies are created and assessed. With this knowledge we can better evaluate their potential utility and their limits.\n",
    "\n",
    "At the most fundamental level, generative AI models like LLMs are statistical representations of the datasets on which they are trained. Machine learning techniques like deep learning and recent innovations like the Transformer network architecture [Vaswani citation] have enabled the creation of models capable of mimicking the data on which they are trained with a high degree of fidelity. But researchers have also discovered that with sufficient time and the application of (often immense) computational power, these models exhibit a range of \"emergent\" capabilities. (<cite id=\"rlvh9\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al., “Emergent Abilities of Large Language Models.”</a></cite>) For example, LLMs can summarize texts, perform language translation, write working computer code, and compose informative responses on a wide array of subjects - all without specific training on how to perform such tasks. (<cite id=\"e97pi\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>) Moreover, these emergent capacities seem to \"scale\", meaning new models exhibit enhanced performance through training on ever-greater quantities of data and computation. <cite id=\"x5wiq\"><a href=\"#zotero%7C27937%2FH9BUWE28\">Kaplan et al., “Scaling Laws for Neural Language Models.”</a></cite>[Kaplan citation] The nature of these emergent capacities remains a matter of intense research and debate, as do the ethical and legal questions surrounding their use. However, it is clear that LLMs can both interpret and generate data in ways that rival previous machine learning methods. Scholars studying these AI systems have labeled them \"foundational models\" due to their potential to enable new domains of computational analysis . Indeed, the remarkable versatility of LLMs has stimulated broader discussions about the potential implications of these technologies on society at large. [GPT citation]\n",
    "\n",
    "(<cite id=\"rlvh9\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al., “Emergent Abilities of Large Language Models.”</a></cite>)\n",
    "\n",
    "<cite id=\"e97pi\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "<cite id=\"x5wiq\"><a href=\"#zotero%7C27937%2FH9BUWE28\">Kaplan et al., “Scaling Laws for Neural Language Models.”</a></cite>\n",
    "\n",
    "<cite id=\"y4y0i\"><a href=\"#zotero%7C27937%2FF3XT4XAQ\">Bommasani et al., “On the Opportunities and Risks of Foundation Models.”</a></cite>\n",
    "\n",
    "<cite id=\"86ncy\"><a href=\"#zotero%7C27937%2FQD3X7XMD\">Eloundou et al., “GPTs Are GPTs.”</a></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "86ncy": [
       {
        "id": "27937/QD3X7XMD",
        "source": "zotero"
       }
      ],
      "e97pi": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "rlvh9": [
       {
        "id": "27937/56EE9N63",
        "source": "zotero"
       }
      ],
      "x5wiq": [
       {
        "id": "27937/H9BUWE28",
        "source": "zotero"
       }
      ],
      "y4y0i": [
       {
        "id": "27937/F3XT4XAQ",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "(<cite id=\"rlvh9\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al., “Emergent Abilities of Large Language Models.”</a></cite>)\n",
    "\n",
    "<cite id=\"e97pi\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "<cite id=\"x5wiq\"><a href=\"#zotero%7C27937%2FH9BUWE28\">Kaplan et al., “Scaling Laws for Neural Language Models.”</a></cite>\n",
    "\n",
    "<cite id=\"y4y0i\"><a href=\"#zotero%7C27937%2FF3XT4XAQ\">Bommasani et al., “On the Opportunities and Risks of Foundation Models.”</a></cite>\n",
    "\n",
    "<cite id=\"86ncy\"><a href=\"#zotero%7C27937%2FQD3X7XMD\">Eloundou et al., “GPTs Are GPTs.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "This is a hermeneutic paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "table-1"
    ]
   },
   "source": [
    "Editor|1641|1798|1916\n",
    "---|---|---|---\n",
    "Senan|0.55|0.4|0.3\n",
    "Henry|0.71|0.5|0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hidden"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your Python version\n",
    "from platform import python_version\n",
    "python_version()\n",
    "\n",
    "#!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas package needs to be added to the requirements.txt 's file \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PredominantDegree</th>\n",
       "      <th>HighestDegree</th>\n",
       "      <th>FundingModel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Geography</th>\n",
       "      <th>AdmissionRate</th>\n",
       "      <th>ACTMedian</th>\n",
       "      <th>SATAverage</th>\n",
       "      <th>AverageCost</th>\n",
       "      <th>Expenditure</th>\n",
       "      <th>AverageFacultySalary</th>\n",
       "      <th>MedianDebt</th>\n",
       "      <th>AverageAgeofEntry</th>\n",
       "      <th>MedianFamilyIncome</th>\n",
       "      <th>MedianEarnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>17</td>\n",
       "      <td>823</td>\n",
       "      <td>18888</td>\n",
       "      <td>7459</td>\n",
       "      <td>7079</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>20.629999</td>\n",
       "      <td>29039.0</td>\n",
       "      <td>27000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Alabama at Birmingham</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>25</td>\n",
       "      <td>1146</td>\n",
       "      <td>19990</td>\n",
       "      <td>17208</td>\n",
       "      <td>10170</td>\n",
       "      <td>16250.0</td>\n",
       "      <td>22.670000</td>\n",
       "      <td>34909.0</td>\n",
       "      <td>37200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Alabama in Huntsville</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>26</td>\n",
       "      <td>1180</td>\n",
       "      <td>20306</td>\n",
       "      <td>9352</td>\n",
       "      <td>9341</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>23.190001</td>\n",
       "      <td>39766.0</td>\n",
       "      <td>41500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama State University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>17</td>\n",
       "      <td>830</td>\n",
       "      <td>17400</td>\n",
       "      <td>7393</td>\n",
       "      <td>6557</td>\n",
       "      <td>15854.5</td>\n",
       "      <td>20.889999</td>\n",
       "      <td>24029.5</td>\n",
       "      <td>22400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The University of Alabama</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>26</td>\n",
       "      <td>1171</td>\n",
       "      <td>26717</td>\n",
       "      <td>9817</td>\n",
       "      <td>9605</td>\n",
       "      <td>17750.0</td>\n",
       "      <td>20.770000</td>\n",
       "      <td>58976.0</td>\n",
       "      <td>39200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>University of Connecticut-Avery Point</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>24</td>\n",
       "      <td>1020</td>\n",
       "      <td>12946</td>\n",
       "      <td>11730</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>University of Connecticut-Stamford</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>21</td>\n",
       "      <td>1017</td>\n",
       "      <td>13028</td>\n",
       "      <td>4958</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>California State University-Channel Islands</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>20</td>\n",
       "      <td>954</td>\n",
       "      <td>22570</td>\n",
       "      <td>12026</td>\n",
       "      <td>8434</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>24.850000</td>\n",
       "      <td>32103.0</td>\n",
       "      <td>35800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>DigiPen Institute of Technology</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.6635</td>\n",
       "      <td>28</td>\n",
       "      <td>1225</td>\n",
       "      <td>37848</td>\n",
       "      <td>5998</td>\n",
       "      <td>7659</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>68233.0</td>\n",
       "      <td>72800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>Neumont University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Rocky Mountains</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.7997</td>\n",
       "      <td>25</td>\n",
       "      <td>1104</td>\n",
       "      <td>37379</td>\n",
       "      <td>3298</td>\n",
       "      <td>6991</td>\n",
       "      <td>22313.0</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>39241.0</td>\n",
       "      <td>37300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1294 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name PredominantDegree  \\\n",
       "0                        Alabama A & M University        Bachelor's   \n",
       "1             University of Alabama at Birmingham        Bachelor's   \n",
       "2             University of Alabama in Huntsville        Bachelor's   \n",
       "3                        Alabama State University        Bachelor's   \n",
       "4                       The University of Alabama        Bachelor's   \n",
       "...                                           ...               ...   \n",
       "1289        University of Connecticut-Avery Point        Bachelor's   \n",
       "1290           University of Connecticut-Stamford        Bachelor's   \n",
       "1291  California State University-Channel Islands        Bachelor's   \n",
       "1292              DigiPen Institute of Technology        Bachelor's   \n",
       "1293                           Neumont University        Bachelor's   \n",
       "\n",
       "     HighestDegree        FundingModel           Region        Geography  \\\n",
       "0         Graduate              Public        Southeast    Mid-size City   \n",
       "1         Graduate              Public        Southeast    Mid-size City   \n",
       "2         Graduate              Public        Southeast    Mid-size City   \n",
       "3         Graduate              Public        Southeast    Mid-size City   \n",
       "4         Graduate              Public        Southeast       Small City   \n",
       "...            ...                 ...              ...              ...   \n",
       "1289      Graduate              Public      New England  Mid-size Suburb   \n",
       "1290      Graduate              Public      New England    Mid-size City   \n",
       "1291      Graduate              Public         Far West  Mid-size Suburb   \n",
       "1292      Graduate  Private For-Profit         Far West       Small City   \n",
       "1293    Bachelor's  Private For-Profit  Rocky Mountains    Mid-size City   \n",
       "\n",
       "      AdmissionRate  ACTMedian  SATAverage  AverageCost  Expenditure  \\\n",
       "0            0.8989         17         823        18888         7459   \n",
       "1            0.8673         25        1146        19990        17208   \n",
       "2            0.8062         26        1180        20306         9352   \n",
       "3            0.5125         17         830        17400         7393   \n",
       "4            0.5655         26        1171        26717         9817   \n",
       "...             ...        ...         ...          ...          ...   \n",
       "1289         0.5940         24        1020        12946        11730   \n",
       "1290         0.4107         21        1017        13028         4958   \n",
       "1291         0.6443         20         954        22570        12026   \n",
       "1292         0.6635         28        1225        37848         5998   \n",
       "1293         0.7997         25        1104        37379         3298   \n",
       "\n",
       "      AverageFacultySalary  MedianDebt  AverageAgeofEntry  MedianFamilyIncome  \\\n",
       "0                     7079     19500.0          20.629999             29039.0   \n",
       "1                    10170     16250.0          22.670000             34909.0   \n",
       "2                     9341     16500.0          23.190001             39766.0   \n",
       "3                     6557     15854.5          20.889999             24029.5   \n",
       "4                     9605     17750.0          20.770000             58976.0   \n",
       "...                    ...         ...                ...                 ...   \n",
       "1289                 14803     18983.0          20.120001             86510.0   \n",
       "1290                 14803     18983.0          20.120001             86510.0   \n",
       "1291                  8434     12500.0          24.850000             32103.0   \n",
       "1292                  7659     19000.0          21.209999             68233.0   \n",
       "1293                  6991     22313.0          24.750000             39241.0   \n",
       "\n",
       "      MedianEarnings  \n",
       "0              27000  \n",
       "1              37200  \n",
       "2              41500  \n",
       "3              22400  \n",
       "4              39200  \n",
       "...              ...  \n",
       "1289           49700  \n",
       "1290           49700  \n",
       "1291           35800  \n",
       "1292           72800  \n",
       "1293           37300  \n",
       "\n",
       "[1294 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/college.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {
    "zotero": {
     "27937/56EE9N63": {
      "URL": "http://arxiv.org/abs/2206.07682",
      "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Wei",
        "given": "Jason"
       },
       {
        "family": "Tay",
        "given": "Yi"
       },
       {
        "family": "Bommasani",
        "given": "Rishi"
       },
       {
        "family": "Raffel",
        "given": "Colin"
       },
       {
        "family": "Zoph",
        "given": "Barret"
       },
       {
        "family": "Borgeaud",
        "given": "Sebastian"
       },
       {
        "family": "Yogatama",
        "given": "Dani"
       },
       {
        "family": "Bosma",
        "given": "Maarten"
       },
       {
        "family": "Zhou",
        "given": "Denny"
       },
       {
        "family": "Metzler",
        "given": "Donald"
       },
       {
        "family": "Chi",
        "given": "Ed H."
       },
       {
        "family": "Hashimoto",
        "given": "Tatsunori"
       },
       {
        "family": "Vinyals",
        "given": "Oriol"
       },
       {
        "family": "Liang",
        "given": "Percy"
       },
       {
        "family": "Dean",
        "given": "Jeff"
       },
       {
        "family": "Fedus",
        "given": "William"
       }
      ],
      "id": "27937/56EE9N63",
      "issued": {
       "date-parts": [
        [
         2022,
         10,
         26
        ]
       ]
      },
      "note": "arXiv:2206.07682 [cs]",
      "number": "arXiv:2206.07682",
      "publisher": "arXiv",
      "system_id": "zotero|27937/56EE9N63",
      "title": "Emergent Abilities of Large Language Models",
      "type": "article"
     },
     "27937/6DE3XGUT": {
      "DOI": "10.1162/99608f92.e32f6dec",
      "URL": "https://hdsr.mitpress.mit.edu/pub/kyzf7fjv/release/5",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Crane",
        "given": "Gregory"
       }
      ],
      "container-title": "Harvard Data Science Review",
      "id": "27937/6DE3XGUT",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2019,
         7,
         3
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/6DE3XGUT",
      "title": "AI, Language, and the Humanities",
      "type": "article-journal",
      "volume": "1"
     },
     "27937/CJYNFHVI": {
      "DOI": "10.1086/ahr/108.3.735",
      "URL": "https://doi.org/10.1086/ahr/108.3.735",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Rosenzweig",
        "given": "Roy"
       }
      ],
      "container-title": "The American Historical Review",
      "id": "27937/CJYNFHVI",
      "issue": "3",
      "issued": {
       "date-parts": [
        [
         2003,
         6,
         1
        ]
       ]
      },
      "journalAbbreviation": "The American Historical Review",
      "page": "735-762",
      "shortTitle": "Scarcity or Abundance?",
      "system_id": "zotero|27937/CJYNFHVI",
      "title": "Scarcity or Abundance? Preserving the Past in a Digital Era",
      "type": "article-journal",
      "volume": "108"
     },
     "27937/F3XT4XAQ": {
      "URL": "http://arxiv.org/abs/2108.07258",
      "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Bommasani",
        "given": "Rishi"
       },
       {
        "family": "Hudson",
        "given": "Drew A."
       },
       {
        "family": "Adeli",
        "given": "Ehsan"
       },
       {
        "family": "Altman",
        "given": "Russ"
       },
       {
        "family": "Arora",
        "given": "Simran"
       },
       {
        "family": "von Arx",
        "given": "Sydney"
       },
       {
        "family": "Bernstein",
        "given": "Michael S."
       },
       {
        "family": "Bohg",
        "given": "Jeannette"
       },
       {
        "family": "Bosselut",
        "given": "Antoine"
       },
       {
        "family": "Brunskill",
        "given": "Emma"
       },
       {
        "family": "Brynjolfsson",
        "given": "Erik"
       },
       {
        "family": "Buch",
        "given": "Shyamal"
       },
       {
        "family": "Card",
        "given": "Dallas"
       },
       {
        "family": "Castellon",
        "given": "Rodrigo"
       },
       {
        "family": "Chatterji",
        "given": "Niladri"
       },
       {
        "family": "Chen",
        "given": "Annie"
       },
       {
        "family": "Creel",
        "given": "Kathleen"
       },
       {
        "family": "Davis",
        "given": "Jared Quincy"
       },
       {
        "family": "Demszky",
        "given": "Dora"
       },
       {
        "family": "Donahue",
        "given": "Chris"
       },
       {
        "family": "Doumbouya",
        "given": "Moussa"
       },
       {
        "family": "Durmus",
        "given": "Esin"
       },
       {
        "family": "Ermon",
        "given": "Stefano"
       },
       {
        "family": "Etchemendy",
        "given": "John"
       },
       {
        "family": "Ethayarajh",
        "given": "Kawin"
       },
       {
        "family": "Fei-Fei",
        "given": "Li"
       },
       {
        "family": "Finn",
        "given": "Chelsea"
       },
       {
        "family": "Gale",
        "given": "Trevor"
       },
       {
        "family": "Gillespie",
        "given": "Lauren"
       },
       {
        "family": "Goel",
        "given": "Karan"
       },
       {
        "family": "Goodman",
        "given": "Noah"
       },
       {
        "family": "Grossman",
        "given": "Shelby"
       },
       {
        "family": "Guha",
        "given": "Neel"
       },
       {
        "family": "Hashimoto",
        "given": "Tatsunori"
       },
       {
        "family": "Henderson",
        "given": "Peter"
       },
       {
        "family": "Hewitt",
        "given": "John"
       },
       {
        "family": "Ho",
        "given": "Daniel E."
       },
       {
        "family": "Hong",
        "given": "Jenny"
       },
       {
        "family": "Hsu",
        "given": "Kyle"
       },
       {
        "family": "Huang",
        "given": "Jing"
       },
       {
        "family": "Icard",
        "given": "Thomas"
       },
       {
        "family": "Jain",
        "given": "Saahil"
       },
       {
        "family": "Jurafsky",
        "given": "Dan"
       },
       {
        "family": "Kalluri",
        "given": "Pratyusha"
       },
       {
        "family": "Karamcheti",
        "given": "Siddharth"
       },
       {
        "family": "Keeling",
        "given": "Geoff"
       },
       {
        "family": "Khani",
        "given": "Fereshte"
       },
       {
        "family": "Khattab",
        "given": "Omar"
       },
       {
        "family": "Koh",
        "given": "Pang Wei"
       },
       {
        "family": "Krass",
        "given": "Mark"
       },
       {
        "family": "Krishna",
        "given": "Ranjay"
       },
       {
        "family": "Kuditipudi",
        "given": "Rohith"
       },
       {
        "family": "Kumar",
        "given": "Ananya"
       },
       {
        "family": "Ladhak",
        "given": "Faisal"
       },
       {
        "family": "Lee",
        "given": "Mina"
       },
       {
        "family": "Lee",
        "given": "Tony"
       },
       {
        "family": "Leskovec",
        "given": "Jure"
       },
       {
        "family": "Levent",
        "given": "Isabelle"
       },
       {
        "family": "Li",
        "given": "Xiang Lisa"
       },
       {
        "family": "Li",
        "given": "Xuechen"
       },
       {
        "family": "Ma",
        "given": "Tengyu"
       },
       {
        "family": "Malik",
        "given": "Ali"
       },
       {
        "family": "Manning",
        "given": "Christopher D."
       },
       {
        "family": "Mirchandani",
        "given": "Suvir"
       },
       {
        "family": "Mitchell",
        "given": "Eric"
       },
       {
        "family": "Munyikwa",
        "given": "Zanele"
       },
       {
        "family": "Nair",
        "given": "Suraj"
       },
       {
        "family": "Narayan",
        "given": "Avanika"
       },
       {
        "family": "Narayanan",
        "given": "Deepak"
       },
       {
        "family": "Newman",
        "given": "Ben"
       },
       {
        "family": "Nie",
        "given": "Allen"
       },
       {
        "family": "Niebles",
        "given": "Juan Carlos"
       },
       {
        "family": "Nilforoshan",
        "given": "Hamed"
       },
       {
        "family": "Nyarko",
        "given": "Julian"
       },
       {
        "family": "Ogut",
        "given": "Giray"
       },
       {
        "family": "Orr",
        "given": "Laurel"
       },
       {
        "family": "Papadimitriou",
        "given": "Isabel"
       },
       {
        "family": "Park",
        "given": "Joon Sung"
       },
       {
        "family": "Piech",
        "given": "Chris"
       },
       {
        "family": "Portelance",
        "given": "Eva"
       },
       {
        "family": "Potts",
        "given": "Christopher"
       },
       {
        "family": "Raghunathan",
        "given": "Aditi"
       },
       {
        "family": "Reich",
        "given": "Rob"
       },
       {
        "family": "Ren",
        "given": "Hongyu"
       },
       {
        "family": "Rong",
        "given": "Frieda"
       },
       {
        "family": "Roohani",
        "given": "Yusuf"
       },
       {
        "family": "Ruiz",
        "given": "Camilo"
       },
       {
        "family": "Ryan",
        "given": "Jack"
       },
       {
        "family": "Ré",
        "given": "Christopher"
       },
       {
        "family": "Sadigh",
        "given": "Dorsa"
       },
       {
        "family": "Sagawa",
        "given": "Shiori"
       },
       {
        "family": "Santhanam",
        "given": "Keshav"
       },
       {
        "family": "Shih",
        "given": "Andy"
       },
       {
        "family": "Srinivasan",
        "given": "Krishnan"
       },
       {
        "family": "Tamkin",
        "given": "Alex"
       },
       {
        "family": "Taori",
        "given": "Rohan"
       },
       {
        "family": "Thomas",
        "given": "Armin W."
       },
       {
        "family": "Tramèr",
        "given": "Florian"
       },
       {
        "family": "Wang",
        "given": "Rose E."
       },
       {
        "family": "Wang",
        "given": "William"
       },
       {
        "family": "Wu",
        "given": "Bohan"
       },
       {
        "family": "Wu",
        "given": "Jiajun"
       },
       {
        "family": "Wu",
        "given": "Yuhuai"
       },
       {
        "family": "Xie",
        "given": "Sang Michael"
       },
       {
        "family": "Yasunaga",
        "given": "Michihiro"
       },
       {
        "family": "You",
        "given": "Jiaxuan"
       },
       {
        "family": "Zaharia",
        "given": "Matei"
       },
       {
        "family": "Zhang",
        "given": "Michael"
       },
       {
        "family": "Zhang",
        "given": "Tianyi"
       },
       {
        "family": "Zhang",
        "given": "Xikun"
       },
       {
        "family": "Zhang",
        "given": "Yuhui"
       },
       {
        "family": "Zheng",
        "given": "Lucia"
       },
       {
        "family": "Zhou",
        "given": "Kaitlyn"
       },
       {
        "family": "Liang",
        "given": "Percy"
       }
      ],
      "id": "27937/F3XT4XAQ",
      "issued": {
       "date-parts": [
        [
         2022,
         7,
         12
        ]
       ]
      },
      "note": "arXiv:2108.07258 [cs]",
      "number": "arXiv:2108.07258",
      "publisher": "arXiv",
      "system_id": "zotero|27937/F3XT4XAQ",
      "title": "On the Opportunities and Risks of Foundation Models",
      "type": "article"
     },
     "27937/H9BUWE28": {
      "DOI": "10.48550/arXiv.2001.08361",
      "URL": "http://arxiv.org/abs/2001.08361",
      "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
      "accessed": {
       "date-parts": [
        [
         2024,
         9,
         30
        ]
       ]
      },
      "author": [
       {
        "family": "Kaplan",
        "given": "Jared"
       },
       {
        "family": "McCandlish",
        "given": "Sam"
       },
       {
        "family": "Henighan",
        "given": "Tom"
       },
       {
        "family": "Brown",
        "given": "Tom B."
       },
       {
        "family": "Chess",
        "given": "Benjamin"
       },
       {
        "family": "Child",
        "given": "Rewon"
       },
       {
        "family": "Gray",
        "given": "Scott"
       },
       {
        "family": "Radford",
        "given": "Alec"
       },
       {
        "family": "Wu",
        "given": "Jeffrey"
       },
       {
        "family": "Amodei",
        "given": "Dario"
       }
      ],
      "id": "27937/H9BUWE28",
      "issued": {
       "date-parts": [
        [
         2020,
         1,
         22
        ]
       ]
      },
      "note": "arXiv:2001.08361 [cs, stat]",
      "number": "arXiv:2001.08361",
      "publisher": "arXiv",
      "system_id": "zotero|27937/H9BUWE28",
      "title": "Scaling Laws for Neural Language Models",
      "type": "article"
     },
     "27937/KNEK45E4": {
      "URL": "http://arxiv.org/abs/2005.14165",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Brown",
        "given": "Tom B."
       },
       {
        "family": "Mann",
        "given": "Benjamin"
       },
       {
        "family": "Ryder",
        "given": "Nick"
       },
       {
        "family": "Subbiah",
        "given": "Melanie"
       },
       {
        "family": "Kaplan",
        "given": "Jared"
       },
       {
        "family": "Dhariwal",
        "given": "Prafulla"
       },
       {
        "family": "Neelakantan",
        "given": "Arvind"
       },
       {
        "family": "Shyam",
        "given": "Pranav"
       },
       {
        "family": "Sastry",
        "given": "Girish"
       },
       {
        "family": "Askell",
        "given": "Amanda"
       },
       {
        "family": "Agarwal",
        "given": "Sandhini"
       },
       {
        "family": "Herbert-Voss",
        "given": "Ariel"
       },
       {
        "family": "Krueger",
        "given": "Gretchen"
       },
       {
        "family": "Henighan",
        "given": "Tom"
       },
       {
        "family": "Child",
        "given": "Rewon"
       },
       {
        "family": "Ramesh",
        "given": "Aditya"
       },
       {
        "family": "Ziegler",
        "given": "Daniel M."
       },
       {
        "family": "Wu",
        "given": "Jeffrey"
       },
       {
        "family": "Winter",
        "given": "Clemens"
       },
       {
        "family": "Hesse",
        "given": "Christopher"
       },
       {
        "family": "Chen",
        "given": "Mark"
       },
       {
        "family": "Sigler",
        "given": "Eric"
       },
       {
        "family": "Litwin",
        "given": "Mateusz"
       },
       {
        "family": "Gray",
        "given": "Scott"
       },
       {
        "family": "Chess",
        "given": "Benjamin"
       },
       {
        "family": "Clark",
        "given": "Jack"
       },
       {
        "family": "Berner",
        "given": "Christopher"
       },
       {
        "family": "McCandlish",
        "given": "Sam"
       },
       {
        "family": "Radford",
        "given": "Alec"
       },
       {
        "family": "Sutskever",
        "given": "Ilya"
       },
       {
        "family": "Amodei",
        "given": "Dario"
       }
      ],
      "id": "27937/KNEK45E4",
      "issued": {
       "date-parts": [
        [
         2020,
         7,
         22
        ]
       ]
      },
      "note": "arXiv:2005.14165 [cs]",
      "number": "arXiv:2005.14165",
      "publisher": "arXiv",
      "system_id": "zotero|27937/KNEK45E4",
      "title": "Language Models are Few-Shot Learners",
      "type": "article"
     },
     "27937/L2ILKERU": {
      "ISBN": "9781783266371",
      "abstract": "The Digital Humanities have arrived at a moment when digital Big Data is becoming more readily available, opening exciting new avenues of inquiry but also new challenges. This pioneering book describes and demonstrates the ways these data can be explored to construct cultural heritage knowledge, for research and in teaching and learning. It helps humanities scholars to grasp Big Data in order to do their work, whether that means understanding the underlying algorithms at work in search engines, or designing and using their own tools to process large amounts of information.Demonstrating what digital tools have to offer and also what 'digital' does to how we understand the past, the authors introduce the many different tools and developing approaches in Big Data for historical and humanistic scholarship, show how to use them, what to be wary of, and discuss the kinds of questions and new perspectives this new macroscopic perspective opens up. Authored 'live' online with ongoing feedback from the wider digital history community, Exploring Big Historical Data breaks new ground and sets the direction for the conversation into the future. It represents the current state-of-the-art thinking in the field and exemplifies the way that digital work can enhance public engagement in the humanities.Exploring Big Historical Data should be the go-to resource for undergraduate and graduate students confronted by a vast corpus of data, and researchers encountering these methods for the first time. It will also offer a helping hand to the interested individual seeking to make sense of genealogical data or digitized newspapers, and even the local historical society who are trying to see the value in digitizing their holdings.",
      "author": [
       {
        "family": "Graham",
        "given": "Shawn"
       },
       {
        "family": "Milligan",
        "given": "Ian"
       },
       {
        "family": "Weingart",
        "given": "Scott"
       }
      ],
      "edition": "Reprint edition",
      "event-place": "London",
      "id": "27937/L2ILKERU",
      "issued": {
       "date-parts": [
        [
         2015,
         11,
         16
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "306",
      "publisher": "Icp",
      "publisher-place": "London",
      "shortTitle": "Exploring Big Historical Data",
      "system_id": "zotero|27937/L2ILKERU",
      "title": "Exploring Big Historical Data: The Historian's Macroscope",
      "type": "book"
     },
     "27937/QD3X7XMD": {
      "URL": "http://arxiv.org/abs/2303.10130",
      "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Eloundou",
        "given": "Tyna"
       },
       {
        "family": "Manning",
        "given": "Sam"
       },
       {
        "family": "Mishkin",
        "given": "Pamela"
       },
       {
        "family": "Rock",
        "given": "Daniel"
       }
      ],
      "id": "27937/QD3X7XMD",
      "issued": {
       "date-parts": [
        [
         2023,
         3,
         23
        ]
       ]
      },
      "note": "arXiv:2303.10130 [cs, econ, q-fin]",
      "number": "arXiv:2303.10130",
      "publisher": "arXiv",
      "shortTitle": "GPTs are GPTs",
      "system_id": "zotero|27937/QD3X7XMD",
      "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
      "type": "article"
     }
    }
   },
   "style": "chicago-note-bibliography.csl"
  },
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
