{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Prompting the Past: Large Language Models as Versatile Tools for Digital Historians\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    " ### Daniel Hutchinson [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0003-2759-5318) \n",
    "\n",
    "Belmont Abbey College\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "cover"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAACWBAMAAABkyf1EAAAAG1BMVEXMzMyWlpacnJyqqqrFxcWxsbGjo6O3t7e+vr6He3KoAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEcElEQVR4nO2aTW/bRhCGh18ij1zKknMkbbf2UXITIEeyMhIfRaF1exQLA/JRclslRykO+rs7s7s0VwytNmhJtsA8gHZEcox9PTs7uysQgGEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmGYr2OWRK/ReIKI8Zt7Hb19wTcQ0uTkGh13bQupcw7gPOvdo12/5CzNtNR7xLUtNtT3CGBQ6g3InjY720pvofUec22LJPr8PhEp2OMPyI40PdwWUdronCu9yQpdPx53bQlfLKnfOVhlnDYRBXve4Ov+IZTeMgdedm0NR+xoXJeQvdJ3CvziykSukwil16W/Oe7aGjIjqc/9ib4jQlJy0uArtN4A0+cvXFvDkmUJ47sJ1Y1ATLDNVXZkNPIepQzxy1ki9fqiwbUj/I+64zxWNzyZnPuhvohJ9K70VvXBixpcu2SAHU+Xd9EKdEJDNpYP3AQr3bQSpPQ6Y6/4dl1z7ZDbArsszjA7L0g7ibB0CDcidUWVoErvIMKZh2Xs0LUzcLW6V5NfiUgNEbaYmAVL6bXl0nJRc+1S72ua/D/cTjGPlQj7eUqd7A096rYlRjdPYlhz7VIvxpVG3cemDKF+WAwLY/6XelOZKTXXzsC4xvDjjtSN6kHLhLke6PrwM8h1raf40qjrGO7H9aTEbduucjS04ZrYU/4iuS5Z2Hdt0rvCLFdmLEXcU30AGddST62o+sLcf5l6k7CP+ru4pLYqX/VFyxbm/utQbx/r22ZEbTb2f5I2kns1Y1OQR8ZyofX+TjJxj1Rz7QQVnf1QzR26Oth0ueJVYcRP6ZUPac/Rx/5M6ixO1dhSrT3Y1DpiYmx3tF4ZUdpz9LD/dSg9PXES0LB71BwcGjKROuV28lnvnv7HHJsezheBGH5+X2CfSfRbMKW+5aGs3JFjMrjGibJc0S7TJzqjHrh2hDybj9XRXNZa89Aro55XBdbW5wti2c/5WJ7jJ1RolVUn/HWpb0I58Tziup6Rx7Dm2hnbRP1GM9PW/NFmQ4PtVRVN63Wvxfmu5sowDMMwDMMwDMMwDMMwDMMwDMMwzL+CpT//F/6beoV8zb2Jmt4Qryx6lTUCsENQ75HOkhXAO3EPVgyQtKtUy3C/e+FJg17Zjnew1Xrdb9InbG4WqfUAftG+WhLwPVyfg536+MU7m4C1CMk4ZznpXZzDYI1PDL2nS1hpvc5cNd7E2sJg05Fe7/7d3Fln8Cvc3bwB616auxsKl4WPghjemHrDqyDWeu1UNW5s2btPnSQ75oOdunEwWazfwgVG0kqluYCM9OIjWOGnfA2b9G4Ha63XKpvQ8perTvTifJNhi6+WMWmi7smEZf6G8MmhlyGq+NqP8GV84TLuJr7UIQVx+bDEoEpRZIz42gs40OuN4Mv8hXzelV7KX1isH+ewTWckikyVv+CfHuqVF7I16gN0VKypX6wPsE+zFPzkinolU9UH8OMGvSpnZqKsv13p/RsMun6X5x/y2LeAr8O66lsBwzBMP/wJfyGq8pgBk6IAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"./media/placeholder.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "Large language models, artifical intelligence, machine learning, historical methodology, optical character recognition, oral history, prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "This article examines how digital historians are using large language models (LLMs) in their research and teaching, along with the critical and ethical debates surrounding their use. The article first assesses the historical capacities of LLMs as measured by machine learning benchmarks, and how such assessments can help historians understand the capacities and limits of these technologies. The utility of LLMs as digital tools are then demonstrated through a series of case studies using GPT-4 and other generative AI models. LLMs are tasked with a variety of tasks for streamlining data preparation, such as oral history transcription, correcting optical character recognition (OCR) errors, and metadata extraction. These case studies also demonstrate how frameworks for using LLMs, such as prompt engineering and retrieval augmented generation (RAG), are used to ground LLM outputs for consistency and greater accuracy. Acknowledging the significant ethical challenges posed by LLMs, the article emphasizes the need for critical engagement and the development of responsible frameworks for implementing these technologies in historical scholarship. By combining disciplinary expertise with innovative computational approaches, historians are discovering new ways to navigate the \"unheard-of historical abundance\" of the digital age, contributing to approaches to generative AI that enriches, rather than distorts, our understanding of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This is the first paragrah of running text with a citation example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "abj0n": [
       {
        "id": "27937/CJYNFHVI",
        "source": "zotero"
       }
      ],
      "kc733": [
       {
        "id": "27937/6DE3XGUT",
        "source": "zotero"
       }
      ],
      "x111d": [
       {
        "id": "27937/L2ILKERU",
        "source": "zotero"
       }
      ]
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In 2003, Roy Rosenzweig predicted that digital historians would need to develop new techniques \"to research, write, and teach in a world of unheard-of historical abundance.\" (<cite id=\"abj0n\"><a href=\"#zotero%7C27937%2FCJYNFHVI\">Rosenzweig, “Scarcity or Abundance?”</a></cite>)\n",
    " Over the past two decades historians have risen to this challenge, embracing digital mapping, network analysis, distant reading of large text collections, and machine learning as part of their growing methodological toolkit. (<cite id=\"x111d\"><a href=\"#zotero%7C27937%2FL2ILKERU\">Graham, Milligan, and Weingart, <i>Exploring Big Historical Data</i>.</a></cite>) Generative artificial intelligence (AI) has emerged as another potential tool that historians are using to explore the past, particularly large language models (LLMs), the most prominent form of this technology. These models possess striking capacities to generate, interpret, and manipulate data across a range of modalities. The rapidly-expanding scope of these capabilities and their limits remain intensely debated, as do their broader social, economic, cultural, and environmental impacts. (<cite id=\"kc733\"><a href=\"#zotero%7C27937%2F6DE3XGUT\">Crane, “AI, Language, and the Humanities.”</a></cite>) Yet while still an emerging technology, historians are already demonstrating generative AI's potential as a versatile digital tool. Historians are also contributing to the critical discourse surrounding this new domain, raising key questions about how these models achieve their capabilities, their propensity to reinforce existing inequalities, and their potential to distort our understanding of the past. [include AHA forum citation]\n",
    "\n",
    "This article contributes to this discourse by demonstrating how digital historians are using generative AI to explore the past, as well as the disciplinary opportunities historians can offer to these broader debates. We begin by assessing the metrics commonly used to measure the historical knowledge of LLMs, and examine how such metrics can give us insights into the capacities and limits this technology. We then examine how generative AI can be used in tasks as varied as preparing datasets, exploring text collections, and offering novel (and controversial) methods of representing the past. We conclude with a call to historians to continue to contribute to ongoing research and debates concerning the ethical use of generative AI. Given the rapid pace of innovation in this field, it is crucial that the profession addresses the implications of this technology for our research and teaching. Historians will have much to contribute in contextualizing the innovative and disruptive potential of these breakthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## What Do AIs Know About History? Assessing LLMs for Historical Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "86ncy": [],
      "e97pi": [],
      "rlvh9": [],
      "x5wiq": []
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As historians explore the possibilities of generative AI, it is important to understand how these technologies are created and assessed. With this knowledge we can better evaluate their potential utility and their limits.\n",
    "\n",
    "At the most fundamental level, generative AI models like LLMs are statistical representations of the datasets on which they are trained. Machine learning techniques like deep learning and recent innovations like the Transformer network architecture  have enabled the creation of models capable of mimicking the data on which they are trained with a high degree of fidelity. But researchers have also discovered that with sufficient time and the application of (often immense) computational power, these models exhibit a range of \"emergent\" capabilities. (<cite id=\"rlvh9\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al., “Emergent Abilities of Large Language Models.”</a></cite>) For example, LLMs can summarize texts, perform language translation, write working computer code, and compose informative responses on a wide array of subjects - all without specific training on how to perform such tasks. (<cite id=\"e97pi\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>) Moreover, these emergent capacities seem to \"scale\", meaning new models exhibit enhanced performance through training on ever-greater quantities of data and computation. (<cite id=\"x5wiq\"><a href=\"#zotero%7C27937%2FH9BUWE28\">Kaplan et al., “Scaling Laws for Neural Language Models.”</a></cite>) The nature of these emergent capacities remains a matter of intense research and debate, as do the ethical and legal questions surrounding their use. However, it is clear that LLMs can both interpret and generate data in ways that rival previous machine learning methods. Scholars studying these AI systems have labeled them \"foundational models\" due to their potential to enable new domains of computational analysis . Indeed, the remarkable versatility of LLMs has stimulated broader discussions about the potential implications of these technologies on society at large. (<cite id=\"86ncy\"><a href=\"#zotero%7C27937%2FQD3X7XMD\">Eloundou et al., “GPTs Are GPTs.”</a></cite>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "5rfo9": [],
      "l99eo": [],
      "nppps": [],
      "q7whc": [],
      "r8lyr": []
     }
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "While the Generative Pre-trained Transformer (GPT) series from OpenAI is the best known of these foundational models, there has been a rapid proliferation of commercial and open-source alternatives. Notable recent LLMs include Google's Gemini, Anthropic's Claude, and open-source models offered by Meta and Mistral. \n",
    "\n",
    "Foundational models are also emerging in other domains, such as image, video, and audio synthesis. Architectures like CLIP (<cite id=\"r8lyr\"><a href=\"#zotero%7C27937%2FUYVGUT4C\">Radford et al., “Learning Transferable Visual Models From Natural Language Supervision.”</a></cite>) enable the creation of synthetic imagery in models like OpenAI's DALL-E, Midjourney, and the open-source community behind Stable Diffusion. Similar approaches for generating video, speech, and music have been developed by firms like Runway-XL, ElevenLabs, and Suno, along with open-source alternatives hosted on sites likes HuggingFace. Most notably, new forms of LLM-training have enabled the a combination of these capacities in multi-modal models capable of working across multiple domains, such as OpenAI's GPT-4 series. (<cite id=\"nppps\"><a href=\"#zotero%7C27937%2FU534FF7L\">OpenAI, “GPT-4 Technical Report.”</a></cite>)\n",
    "\n",
    "An accessible way to stay abreast of recent innovations in this field is by following the leaderboards used to measure performance on standard LLM benchmarks. LLMArena's Chatbot Arena (<cite id=\"q7whc\"><a href=\"#zotero%7C27937%2FZICATXAV\">“Chatbot Arena (Formerly LMSYS).”</a></cite>) offers an overview of leading contemporary models, while HuggingFace's Open LLM Leaderboard (<cite id=\"l99eo\"><a href=\"#zotero%7C27937%2FQPK6D3M9\">“Open LLM Leaderboard 2 - a Hugging Face Space by Open-Llm-Leaderboard.”</a></cite>) and the Open Multilingual LLM Evaluation Leaderboard (<cite id=\"5rfo9\"><a href=\"#zotero%7C27937%2FRRLN9TF5\">“Open Multilingual Llm Leaderboard - a Hugging Face Space by Uonlp.”</a></cite>) offer specialized metrics for particular domains and use-cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "7zssq": [],
      "pl7am": [],
      "sj7gk": [],
      "xw5wn": []
     }
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "While such claims have sparked both excitement and alarm, any assessment of LLMs must first be tempered with humility. LLMs are often described as possessing \"knowledge\" and \"understanding,\" yet direct engagement with these models can quickly reveal both their remarkable breadth and their narrow limits. Incisive critics of this technology characterize LLMs as \"stochastic parrots\" that excel at uncanny mimicry of human intelligence. (<cite id=\"pl7am\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">Bender et al., “On the Dangers of Stochastic Parrots | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.”</a></cite>) A form of this mimicry has proven convincing in the past. The first attribution of true artificial intelligence to a computer program occurred in 1966 with a scripted chatbot named ELIZA, developed by AI pioneer Joseph Weizenbaum. (<cite id=\"sj7gk\"><a href=\"#zotero%7C27937%2FEZNK3CE3\">McCorduck, <i>Machines Who Think a Personal Inquiry into the History and Prospects of Artificial Intelligence</i>.</a></cite>) A recent replication of this phenomenon occurred in June 2022 when a Google AI engineer declared the LLM he was training had become sentient. (<cite id=\"7zssq\"><a href=\"#zotero%7C27937%2FBXZEP65G\">“What Is LaMDA and What Does It Want? | by Blake Lemoine | Medium.”</a></cite>)\n",
    "Such attributions will likely increase as newer LLMs demonstrate increasing proficiency in seemingly distinct human qualities, like humor. (<cite id=\"xw5wn\"><a href=\"#zotero%7C27937%2FFMW5DCWM\">Chowdhery et al., “PaLM.”</a></cite>) The means by which LLMs process, interpret, and generate information is a highly technical field requiring specialization in natural language processing, statistics, computational linguistics, and machine learning. While most historians may lack the technical knowledge to effectively evaluate the merits of these debates, when it comes to our own domain we are well equipped to offer informed insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Indeed, the standard measurement for a LLM's historical knowledge was inadvertently created by historians. One widely-used measure for LLM performance is the Massive Multitask Language Understanding (MMLU) benchmark, developed in 2021 by researchers led by Dan Hendryks. This benchmark contains nearly 16,000 questions from 57 academic disciplines ranging in difficulty from an elementary educational level to postgraduate curricula in professional domains like law and medicine. History is measured in this benchmark through questions taken from the Advanced Placement (A.P.) curricula for U.S., European, and World history. Hundreds of thousands of secondary students across the globe annually enroll in these curricula, which are designed to replicate the rigors of an introductory university-level history course. The educators who developed and refined these programs likely never imagined their work would serve as a technical benchmark, and the appropriateness of such a standard can be debated. Yet this benchmark, however imperfect, offers historians an accessible means to evaluate this highly technical domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this benchmark LLMs are given an excerpt from a historical source followed a multiple-choice question, and are then instructed to identify the correct answer. Below is an example question drawn from the U.S. History curriculum: \n",
    "\n",
    "**U.S. History Benchmark, Question 5:**\n",
    "\n",
    "This question refers to the following information.\n",
    "\n",
    "\"I was once a tool of oppression  \n",
    "And as green as a sucker could be  \n",
    "And monopolies banded together  \n",
    "To beat a poor hayseed like me.\"    \n",
    "\n",
    "\"The railroads and old party bosses  \n",
    "Together did sweetly agree;  \n",
    "And they thought there would be little trouble  \n",
    "In working a hayseed like me. . . .\"  \n",
    "\n",
    "\"The Hayseed\"  \n",
    "\n",
    "The song, and the movement that it was connected to, highlight which of the following developments in the broader society in the late 1800s?  \n",
    "\n",
    "A: Corruption in government, especially as it related to big business, energized the public to demand increased popular control and reform of local, state, and national governments.  \n",
    "B: A large-scale movement of struggling African American and white farmers, as well as urban factory workers, was able to exert a great deal of leverage over federal legislation.  \n",
    "C: The two-party system of the era broke down and led to the emergence of an additional major party that was able to win control of Congress within ten years of its founding.  \n",
    "D: Continued skirmishes on the frontier in the 1890s with American Indians created a sense of fear and bitterness among western farmers.\n",
    "\n",
    "**Correct Answer: A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "wfmit": []
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The MMLU benchmarks were first tested in 2021 against the then-leading LLM, OpenAI's GPT-3. Twenty-five percent accuracy represented random chance; ninety percent performance reflected expert-level accuracy. GPT-3 initially achieved over fifty percent accuracy on all three A.P. curricula, and its performance in these subfields numbered among the top third of all the academic disciplines in the benchmarks. However, in no field did GPT-3 achieve expert-level accuracy, and the model demonstrated particularly poor performance in the fields of \"Moral Questions\" and \"Professional Law.\" As the authors note, this \"weakness is particularly concerning because it will be important for future models to have a strong understanding of what is legal and what is ethical.\" (<cite id=\"wfmit\"><a href=\"#zotero%7C27937%2FZS9JDNGD\">Hendrycks et al., “Measuring Massive Multitask Language Understanding.”</a></cite>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "q68mg": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "The specific accuracy rates for GPT-3 for the initial Hendryks study: US History, 52.9%; European History, 53.9%; and World History, 56.1%. Full data for questions for history and other disciplines can be found at: <cite id=\"q68mg\"><a href=\"#zotero%7C27937%2FA834FRJL\">Hendrycks, <i>Measuring Massive Multitask Language Understanding</i>.</a></cite> Many thanks to Dan Hendrycks for sharing the discipline-specific accuracy rates for these fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rapid advances in model development have occurred since 2021. Subsequent tests on newer models \"scaled\" on ever greater amounts of data and computation demonstrate substantial gains in performance on these historical benchmarks. Below are results from a replication study conducted in September 2024 across a series of leading LLMs, along with the initial Hendryks test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Dr-Hutchinson/jdh_article/main/media/Table%201%20-%20MMLU%20Benchmark%20Performance.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the image from the article GitHub URL\n",
    "table_1_url = 'https://raw.githubusercontent.com/Dr-Hutchinson/jdh_article/main/media/Table%201%20-%20MMLU%20Benchmark%20Performance.png'\n",
    "display(Image(url=table_1_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "oy7fh": [],
      "vge7c": []
     }
    }
   },
   "source": [
    "Data from this replication study can be accessed via the HELM Leaderboard for the MMLU Benchmark, hosted by the Center for Research on Foundation Models at Stanford University. (<cite id=\"vge7c\"><a href=\"#zotero%7C27937%2FGSIXPJ7P\">Mai and Liang, “Massive Multitask Language Understanding (MMLU) on HELM.”</a></cite>) You can directly experiment with LLM performance on these benchmarks via a digital history project accompanying this article, \"What Do AIs Know About History?\" (<cite id=\"oy7fh\"><a href=\"#zotero%7C27937%2F5AL5LZ2K\">“What Do AIs Know About History? A Digital History Experiment by Daniel Hutchinson · Streamlit.”</a></cite>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.2 Paragraph - The MMLU Benchmarks\n",
    "\n",
    "<cite id=\"wfmit\"><a href=\"#zotero%7C27937%2FZS9JDNGD\">Hendrycks et al., “Measuring Massive Multitask Language Understanding.”</a></cite>\n",
    "\n",
    "Hermuntical section after on individual scores\n",
    "\n",
    "<cite id=\"q68mg\"><a href=\"#zotero%7C27937%2FA834FRJL\">Hendrycks, <i>Measuring Massive Multitask Language Understanding</i>.</a></cite>\n",
    "\n",
    "Hermunitical section after scores\n",
    "\n",
    "<cite id=\"vge7c\"><a href=\"#zotero%7C27937%2FGSIXPJ7P\">Mai and Liang, “Massive Multitask Language Understanding (MMLU) on HELM.”</a></cite>\n",
    "\n",
    "<cite id=\"oy7fh\"><a href=\"#zotero%7C27937%2F5AL5LZ2K\">“What Do AIs Know About History? A Digital History Experiment by Daniel Hutchinson · Streamlit.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "table-1"
    ]
   },
   "source": [
    "Editor|1641|1798|1916\n",
    "---|---|---|---\n",
    "Senan|0.55|0.4|0.3\n",
    "Henry|0.71|0.5|0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "hidden"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your Python version\n",
    "from platform import python_version\n",
    "python_version()\n",
    "\n",
    "#!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas package needs to be added to the requirements.txt 's file \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PredominantDegree</th>\n",
       "      <th>HighestDegree</th>\n",
       "      <th>FundingModel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Geography</th>\n",
       "      <th>AdmissionRate</th>\n",
       "      <th>ACTMedian</th>\n",
       "      <th>SATAverage</th>\n",
       "      <th>AverageCost</th>\n",
       "      <th>Expenditure</th>\n",
       "      <th>AverageFacultySalary</th>\n",
       "      <th>MedianDebt</th>\n",
       "      <th>AverageAgeofEntry</th>\n",
       "      <th>MedianFamilyIncome</th>\n",
       "      <th>MedianEarnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>17</td>\n",
       "      <td>823</td>\n",
       "      <td>18888</td>\n",
       "      <td>7459</td>\n",
       "      <td>7079</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>20.629999</td>\n",
       "      <td>29039.0</td>\n",
       "      <td>27000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Alabama at Birmingham</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>25</td>\n",
       "      <td>1146</td>\n",
       "      <td>19990</td>\n",
       "      <td>17208</td>\n",
       "      <td>10170</td>\n",
       "      <td>16250.0</td>\n",
       "      <td>22.670000</td>\n",
       "      <td>34909.0</td>\n",
       "      <td>37200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Alabama in Huntsville</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>26</td>\n",
       "      <td>1180</td>\n",
       "      <td>20306</td>\n",
       "      <td>9352</td>\n",
       "      <td>9341</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>23.190001</td>\n",
       "      <td>39766.0</td>\n",
       "      <td>41500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama State University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>17</td>\n",
       "      <td>830</td>\n",
       "      <td>17400</td>\n",
       "      <td>7393</td>\n",
       "      <td>6557</td>\n",
       "      <td>15854.5</td>\n",
       "      <td>20.889999</td>\n",
       "      <td>24029.5</td>\n",
       "      <td>22400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The University of Alabama</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>26</td>\n",
       "      <td>1171</td>\n",
       "      <td>26717</td>\n",
       "      <td>9817</td>\n",
       "      <td>9605</td>\n",
       "      <td>17750.0</td>\n",
       "      <td>20.770000</td>\n",
       "      <td>58976.0</td>\n",
       "      <td>39200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>University of Connecticut-Avery Point</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>24</td>\n",
       "      <td>1020</td>\n",
       "      <td>12946</td>\n",
       "      <td>11730</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>University of Connecticut-Stamford</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>21</td>\n",
       "      <td>1017</td>\n",
       "      <td>13028</td>\n",
       "      <td>4958</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>California State University-Channel Islands</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>20</td>\n",
       "      <td>954</td>\n",
       "      <td>22570</td>\n",
       "      <td>12026</td>\n",
       "      <td>8434</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>24.850000</td>\n",
       "      <td>32103.0</td>\n",
       "      <td>35800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>DigiPen Institute of Technology</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.6635</td>\n",
       "      <td>28</td>\n",
       "      <td>1225</td>\n",
       "      <td>37848</td>\n",
       "      <td>5998</td>\n",
       "      <td>7659</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>68233.0</td>\n",
       "      <td>72800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>Neumont University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Rocky Mountains</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.7997</td>\n",
       "      <td>25</td>\n",
       "      <td>1104</td>\n",
       "      <td>37379</td>\n",
       "      <td>3298</td>\n",
       "      <td>6991</td>\n",
       "      <td>22313.0</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>39241.0</td>\n",
       "      <td>37300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1294 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name PredominantDegree  \\\n",
       "0                        Alabama A & M University        Bachelor's   \n",
       "1             University of Alabama at Birmingham        Bachelor's   \n",
       "2             University of Alabama in Huntsville        Bachelor's   \n",
       "3                        Alabama State University        Bachelor's   \n",
       "4                       The University of Alabama        Bachelor's   \n",
       "...                                           ...               ...   \n",
       "1289        University of Connecticut-Avery Point        Bachelor's   \n",
       "1290           University of Connecticut-Stamford        Bachelor's   \n",
       "1291  California State University-Channel Islands        Bachelor's   \n",
       "1292              DigiPen Institute of Technology        Bachelor's   \n",
       "1293                           Neumont University        Bachelor's   \n",
       "\n",
       "     HighestDegree        FundingModel           Region        Geography  \\\n",
       "0         Graduate              Public        Southeast    Mid-size City   \n",
       "1         Graduate              Public        Southeast    Mid-size City   \n",
       "2         Graduate              Public        Southeast    Mid-size City   \n",
       "3         Graduate              Public        Southeast    Mid-size City   \n",
       "4         Graduate              Public        Southeast       Small City   \n",
       "...            ...                 ...              ...              ...   \n",
       "1289      Graduate              Public      New England  Mid-size Suburb   \n",
       "1290      Graduate              Public      New England    Mid-size City   \n",
       "1291      Graduate              Public         Far West  Mid-size Suburb   \n",
       "1292      Graduate  Private For-Profit         Far West       Small City   \n",
       "1293    Bachelor's  Private For-Profit  Rocky Mountains    Mid-size City   \n",
       "\n",
       "      AdmissionRate  ACTMedian  SATAverage  AverageCost  Expenditure  \\\n",
       "0            0.8989         17         823        18888         7459   \n",
       "1            0.8673         25        1146        19990        17208   \n",
       "2            0.8062         26        1180        20306         9352   \n",
       "3            0.5125         17         830        17400         7393   \n",
       "4            0.5655         26        1171        26717         9817   \n",
       "...             ...        ...         ...          ...          ...   \n",
       "1289         0.5940         24        1020        12946        11730   \n",
       "1290         0.4107         21        1017        13028         4958   \n",
       "1291         0.6443         20         954        22570        12026   \n",
       "1292         0.6635         28        1225        37848         5998   \n",
       "1293         0.7997         25        1104        37379         3298   \n",
       "\n",
       "      AverageFacultySalary  MedianDebt  AverageAgeofEntry  MedianFamilyIncome  \\\n",
       "0                     7079     19500.0          20.629999             29039.0   \n",
       "1                    10170     16250.0          22.670000             34909.0   \n",
       "2                     9341     16500.0          23.190001             39766.0   \n",
       "3                     6557     15854.5          20.889999             24029.5   \n",
       "4                     9605     17750.0          20.770000             58976.0   \n",
       "...                    ...         ...                ...                 ...   \n",
       "1289                 14803     18983.0          20.120001             86510.0   \n",
       "1290                 14803     18983.0          20.120001             86510.0   \n",
       "1291                  8434     12500.0          24.850000             32103.0   \n",
       "1292                  7659     19000.0          21.209999             68233.0   \n",
       "1293                  6991     22313.0          24.750000             39241.0   \n",
       "\n",
       "      MedianEarnings  \n",
       "0              27000  \n",
       "1              37200  \n",
       "2              41500  \n",
       "3              22400  \n",
       "4              39200  \n",
       "...              ...  \n",
       "1289           49700  \n",
       "1290           49700  \n",
       "1291           35800  \n",
       "1292           72800  \n",
       "1293           37300  \n",
       "\n",
       "[1294 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/college.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {
    "zotero": {
     "27937/6DE3XGUT": {
      "DOI": "10.1162/99608f92.e32f6dec",
      "URL": "https://hdsr.mitpress.mit.edu/pub/kyzf7fjv/release/5",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Crane",
        "given": "Gregory"
       }
      ],
      "container-title": "Harvard Data Science Review",
      "id": "27937/6DE3XGUT",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2019,
         7,
         3
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/6DE3XGUT",
      "title": "AI, Language, and the Humanities",
      "type": "article-journal",
      "volume": "1"
     },
     "27937/CJYNFHVI": {
      "DOI": "10.1086/ahr/108.3.735",
      "URL": "https://doi.org/10.1086/ahr/108.3.735",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Rosenzweig",
        "given": "Roy"
       }
      ],
      "container-title": "The American Historical Review",
      "id": "27937/CJYNFHVI",
      "issue": "3",
      "issued": {
       "date-parts": [
        [
         2003,
         6,
         1
        ]
       ]
      },
      "journalAbbreviation": "The American Historical Review",
      "page": "735-762",
      "shortTitle": "Scarcity or Abundance?",
      "system_id": "zotero|27937/CJYNFHVI",
      "title": "Scarcity or Abundance? Preserving the Past in a Digital Era",
      "type": "article-journal",
      "volume": "108"
     },
     "27937/L2ILKERU": {
      "ISBN": "9781783266371",
      "abstract": "The Digital Humanities have arrived at a moment when digital Big Data is becoming more readily available, opening exciting new avenues of inquiry but also new challenges. This pioneering book describes and demonstrates the ways these data can be explored to construct cultural heritage knowledge, for research and in teaching and learning. It helps humanities scholars to grasp Big Data in order to do their work, whether that means understanding the underlying algorithms at work in search engines, or designing and using their own tools to process large amounts of information.Demonstrating what digital tools have to offer and also what 'digital' does to how we understand the past, the authors introduce the many different tools and developing approaches in Big Data for historical and humanistic scholarship, show how to use them, what to be wary of, and discuss the kinds of questions and new perspectives this new macroscopic perspective opens up. Authored 'live' online with ongoing feedback from the wider digital history community, Exploring Big Historical Data breaks new ground and sets the direction for the conversation into the future. It represents the current state-of-the-art thinking in the field and exemplifies the way that digital work can enhance public engagement in the humanities.Exploring Big Historical Data should be the go-to resource for undergraduate and graduate students confronted by a vast corpus of data, and researchers encountering these methods for the first time. It will also offer a helping hand to the interested individual seeking to make sense of genealogical data or digitized newspapers, and even the local historical society who are trying to see the value in digitizing their holdings.",
      "author": [
       {
        "family": "Graham",
        "given": "Shawn"
       },
       {
        "family": "Milligan",
        "given": "Ian"
       },
       {
        "family": "Weingart",
        "given": "Scott"
       }
      ],
      "edition": "Reprint edition",
      "event-place": "London",
      "id": "27937/L2ILKERU",
      "issued": {
       "date-parts": [
        [
         2015,
         11,
         16
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "306",
      "publisher": "Icp",
      "publisher-place": "London",
      "shortTitle": "Exploring Big Historical Data",
      "system_id": "zotero|27937/L2ILKERU",
      "title": "Exploring Big Historical Data: The Historian's Macroscope",
      "type": "book"
     }
    }
   },
   "style": "chicago-note-bibliography.csl"
  },
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
