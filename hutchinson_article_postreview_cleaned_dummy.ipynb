{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5b1ace",
            "metadata": {},
            "source": [
                "# Building an AI Research Assistant: Large Language Models as a Versatile Tool for Digital Historians"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ed94a",
            "metadata": {},
            "source": [
                " ### Daniel Hutchinson [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0003-2759-5318) \n",
                "Belmont Abbey College"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "540e59",
            "metadata": {},
            "source": [
                "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
                "\u00a9<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "44f746",
            "metadata": {},
            "source": [
                "[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n",
                "\u00a9<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "11910a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "display(Image(\"./media/placeholder.png\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d34691",
            "metadata": {},
            "source": [
                "Large language models, GPT-4, artifical intelligence, machine learning, historical methodology, optical character recognition, oral history, prompt engineering"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d74e5",
            "metadata": {},
            "source": [
                "This article explores into the potential applications and implications of large language models (LLMs) for historical research and pedagogy. The article examines the capabilities of GPT-4 and other machine learning models through case studies studying their utility in fostering greater accessibility to historical sources. GPT-4's performance is evaluated on a series of prompted tasks including data preparation, source analysis, and the ethical implications of simulated historical worldviews. GPT-4's proficiency in historical knowledge is also evaluated by using a widely recognized machine learning benchmark. A replication study demonstrates that GPT-4 exhibits expert-level performance in three distinct historical subfields. Given the rapid advances in LLMs, historians should contribute wider debates surrounding these technologies, as the unpredictable impacts of democratized AI on historical knowledge are already emerging.\n",
                "\n",
                "In the article's hermeneutical layer, the author explores the practice of prompt engineering, or the techniques for using natural language instructions to guide a LLM's output. Prompt engineering strategies are demonstrated through the use of few-shot prompting, chain-of-thought reasoning, and prompt chaining."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "829bfe",
            "metadata": {},
            "source": [
                "# Introduction\n",
                "\n",
                "In 2003, Roy Rosenzweig predicted that digital historians would need to develop new techniques \"to research, write, and teach in a world of unheard-of historical abundance.\" (<cite id=\"cite2c-27937/CJYNFHVI\"><a href=\"#zotero%7C27937%2FCJYNFHVI\">(Rosenzweig, 2003)</a></cite>) Over the past two decades historians have risen to this challenge, embracing digital mapping, network analysis, distant reading of large text collections, and machine learning as part of their growing methodological toolkit. (<cite id=\"cite2c-27937/L2ILKERU\"><a href=\"#zotero%7C27937%2FL2ILKERU\">(Graham et al., 2015)</a></cite>) Generative artificial intelligence (AI) has emerged as another potential tool that historians are using to explore the past, particularly large language models (LLMs), the most prominent form of this technology. These models possess striking capacities to generate, interpret, and manipulate data across a range of modalities. The rapidly-expanding scope of these capabilities and their limits remain intensely debated (<cite id=\"cite2c-27937/6DE3XGUT\"><a href=\"#zotero%7C27937%2F6DE3XGUT\">(Crane, 2019)</a></cite>), as do their broader social, economic, cultural, and environmental impacts. Yet while still an emerging technology, historians are already demonstrating generative AI's potential as a versatile digital tool. Historians are also contributing to the critical discourse surrounding this new domain, raising key questions about how these models achieve their capabilities, their propensity to reinforce existing inequalities, and their potential to distort our understanding of the past. \n",
                "\n",
                "This article contributes to this discourse by demonstrating how generative AI is being used to explore the past, and offer insights into the common approaches digital historians are using to effectively leverage LLMs. We begin by assessing the metrics commonly used to measure the historical knowledge of LLMs, and examine how such metrics can give us insights into the capacities and limits this technology. We then examine how generative AI can be used in tasks as varied as preparing datasets, exploring text collections, and offering novel methods of representing the past. We conclude with a call to historians to continue to contribute to ongoing research and debates concerning the ethical use of generative AI. Given the rapid pace of innovation in this field, it is crucial that the profession addresses the implications of this technology for our research and teaching. Historians will have much to contribute in contextualizing the innovative and disruptive potential of these breakthroughs."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b41e21",
            "metadata": {},
            "source": [
                "## What Do AIs Know About History? Assessing LLMs for Historical Knowledge"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15bc87",
            "metadata": {},
            "source": [
                "As historians explore the possibilities of generative AI, it is important to understand how these technologies are created and assessed. With this knowledge we can better evaluate their potential utility and their limits.\n",
                "\n",
                "At the most fundamental level, generative AI models like LLMs are statistical representations of the datasets on which they are trained. Machine learning techniques like deep learning and recent innovations like the Transformer network architecture (<cite id=\"cite2c-27937/BBYR8D86\"><a href=\"#zotero%7C27937%2FBBYR8D86\">(Vaswani et. al, 2017)</a></cite>) have enabled the creation of models capable of mimicking the data on which they are trained. But researchers have also discovered that with sufficient time and the application of (often immense) computational power, these models exhibit a range of \"emergent\" capabilities (<cite id=\"cite2c-27937/56EE9N63\"><a href=\"#zotero%7C27937%2F56EE9N63\">(Wei et al., 2022)</a></cite>). For example, LLMs can summarize texts, perform language translation, write working computer code, and compose informative responses on a wide array of subjects - all without specific training on how to perform such tasks. (<cite id=\"cite2c-27937/KNEK45E4\"><a href=\"#zotero%7C27937%2FKNEK45E4\">(Brown et al., 2020)</a></cite>). Moreover, these emergent capacities seem to \"scale\", meaning new models exhibit enhanced performance through training on ever-greater quantities of data and computation. (<cite id=\"cite2c-27937/H9BUWE28\"><a href=\"#zotero%7C27937%2FH9BUWE28\">(Kaplan et al., 2020)</a></cite>) The nature of these emergent capacities remains a matter of intense research and debate, as do the ethical and legal questions surrounding their use. However, it is clear that LLMs can both interpret and generate data in ways that rival previous machine learning methods. Scholars studying these AI systems have labeled them \"foundational models\" due to their potential to enable new domains of computational analysis (<cite id=\"cite2c-27937/F3XT4XAQ\"><a href=\"#zotero%7C27937%2FF3XT4XAQ\">(Bommasani et al., 2022)</a></cite>). Indeed, the remarkable versatility of LLMs is stimulating broader discussions about the potential implictions of these technologies on society at large. (<cite id=\"cite2c-27937/QD3X7XMD\"><a href=\"#zotero%7C27937%2FQD3X7XMD\">(Eloundou et al., 2023)</a></cite>)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb8823",
            "metadata": {},
            "source": [
                "While the Generative Pre-trained Transformer (GPT) series from OpenAI is the best known of these foundational models, there has been a rapid proliferation of commercial and open-source alternatives. Notable recent LLMs include Google's Gemini, Anthropic's Claude, and open-source models offered by Meta and Mistral. \n",
                "\n",
                "Foundational models are also emerging in other domains, such as image, video, and audio synthesis. Architectures like CLIP (<cite id=\"cite2c-27937/UYVGUT4C\"><a href=\"#zotero%7C27937%2FUYVGUT4C\">(Radford et al., 2021)</a></cite>) enable the creation of synthetic imagery in models like OpenAI's DALL-E, Midjourney, and the open-source community behind Stable Diffusion. Similar approaches for generating video, speech, and music have been developed by firms like Runway-XL, ElevenLabs, and Suno, along with open-source alternatives hosted on sites likes HuggingFace. Most notably, new forms of LLM-training have enabled the a combination of these capacities in multi-modal models capable of working across multiple domains, such as OpenAI's GPT-4 series. (<cite id=\"cite2c-27937/U534FF7L\"><a href=\"#zotero%7C27937%2FU534FF7L\">(OpenAI, 2023)</a></cite>)\n",
                "\n",
                "An accessible way to stay abreast of recent innovations in this field is by following the leaderboards used to measure performance on standard LLM benchmarks. LLMArena's Chatbot Arena (<cite id=\"cite2c-27937/ZICATXAV\"><a href=\"#zotero%7C27937%2FZICATXAV\">(Chiang et. al, 2024)</a></cite>) offers an overview of leading contemporary models, while HuggingFace's Open LLM Leaderboard (<cite id=\"cite2c-27937/QPK6D3M9\"><a href=\"#zotero%7C27937%2FQPK6D3M9\">(Fourrier et. al, 2024)</a></cite>) and the Open Multilingual LLM Evaluation Leaderboard (<cite id=\"cite2c-27937/RRLN9TF5\"><a href=\"#zotero%7C27937%2FRRLN9TF5\">(Lai et al., 2023)</a></cite>) offer specialized metrics for particular domains and use-cases."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71d307",
            "metadata": {},
            "source": [
                "While such claims have sparked both excitement and alarm, any assessment of LLMs must first be tempered with humility. LLMs are often described as possessing \"knowledge\" and \"understanding,\" yet direct engagement with these models can quickly reveal both their remarkable breadth and their narrow limits. Incisive critics of this technology characterize LLMs as \"stochastic parrots\" that excel at uncanny mimicry of human intelligence (<cite id=\"cite2c-27937/MVDFMR8K\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">(Bender et al., n.d.)</a></cite>). A form of this mimicry has proven convincing in the past. The first attribution of true artificial intelligence to a computer program occurred in 1966 with a scripted chatbot named ELIZA, developed by AI pioneer Joseph Weizenbaum (<cite id=\"cite2c-27937/KHF4TXQH\"><a href=\"#zotero%7C27937%2FKHF4TXQH\">(\u201cMachines Who Think,\u201d 2004, 289-298)</a></cite>). A recent replication of this phenomenon occurred in June 2022 when a Google AI engineer declared the LLM he was training had become sentient (<cite id=\"cite2c-27937/BXZEP65G\"><a href=\"#zotero%7C27937%2FBXZEP65G\">(<i>What Is LaMDA and What Does It Want? | by Blake Lemoine | Medium</i>, n.d.)</a></cite>). Such attributions will likely increase as newer LLMs demonstrate increasing proficiency in seemingly distinct human qualities, like humor (<cite id=\"cite2c-27937/FMW5DCWM\"><a href=\"#zotero%7C27937%2FFMW5DCWM\">(Chowdhery et al., 2022)</a></cite>, 39). The means by which LLMs process, interpret, and generate information is a highly technical field requiring specialization in natural language processing, statistics, computational linguistics, and machine learning. While most historians may lack the technical knowledge to effectively evaluate the merits of these debates, when it comes to our own domain we are well equipped to offer informed insights."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "99c2cb",
            "metadata": {},
            "source": [
                "Indeed, the standard measurement for a LLM's historical knowledge was inadvertently created by historians. Researchers have devised a series of benchmarks for measuring LLM performance on various forms of academic knowledge. One widely-used measurement is the Massive Multitask Language Understanding (MMLU) benchmark, developed in 2021 by a team led by Dan Hendryks. This benchmark contains nearly 16,000 questions from 57 academic disciplines ranging in difficulty from an elementary educational level to postgraduate curricula in professional domains. History is measured in this benchmark through questions taken from the Advanced Placement (A.P.) curricula for U.S., European, and World history. Hundreds of thousands of secondary students across the globe annually enroll in these curricula, which are designed to replicate the rigors of an introductory university-level history course. The educators who developed and refined these programs likely never imagined their work would serve as a technical benchmark, and the appropriateness of such a standard can be debated. Yet this benchmark, however imperfect, offers historians an accessible means to evaluate this highly technical domain."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e8a1b",
            "metadata": {},
            "source": [
                "Understanding the format of the benchmarks is important in understanding the performance of LLMs. LLMs are given an excerpt from a historical source followed a multiple-choice question and then asked to identify the answer. Below is an example question drawn from the U.S. History curriculum: \n",
                "\n",
                "**U.S. History Benchmark, Question 5:**\n",
                "\n",
                "This question refers to the following information.\n",
                "\n",
                "\"I was once a tool of oppression  \n",
                "And as green as a sucker could be  \n",
                "And monopolies banded together  \n",
                "To beat a poor hayseed like me.\"    \n",
                "\n",
                "\"The railroads and old party bosses  \n",
                "Together did sweetly agree;  \n",
                "And they thought there would be little trouble  \n",
                "In working a hayseed like me. . . .\"  \n",
                "\n",
                "\"The Hayseed\"  \n",
                "\n",
                "The song, and the movement that it was connected to, highlight which of the following developments in the broader society in the late 1800s?  \n",
                "\n",
                "A: Corruption in government, especially as it related to big business, energized the public to demand increased popular control and reform of local, state, and national governments.  \n",
                "B: A large-scale movement of struggling African American and white farmers, as well as urban factory workers, was able to exert a great deal of leverage over federal legislation.  \n",
                "C: The two-party system of the era broke down and led to the emergence of an additional major party that was able to win control of Congress within ten years of its founding.  \n",
                "D: Continued skirmishes on the frontier in the 1890s with American Indians created a sense of fear and bitterness among western farmers.\n",
                "\n",
                "**Answer: A**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3d3816",
            "metadata": {},
            "source": [
                "The MMLU benchmarks were first tested in 2021 against the then-leading LLM, OpenAI's GPT-3. Twenty-five percent accuracy represented random chance; ninety percent performance reflected expert-level accuracy. GPT-3 initially achieved over fifty percent accuracy on all three A.P. curricula, and its performance in these subfields numbered among the top third of all the academic disciplines in the benchmarks. However, in no field did GPT-3 achieve expert-level accuracy, and the model demonstrated particularly poor performance in the fields of \"Moral Questions\" and \"Professional Law.\" As the authors note, this \"weakness is particularly concerning because it will be important for future models to have a strong understanding of what is legal and what is ethical.\" (<cite id=\"cite2c-27937/ZS9JDNGD\"><a href=\"#zotero%7C27937%2FZS9JDNGD\"></a></cite>)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d9c87",
            "metadata": {},
            "source": [
                "The specific accuracy rates for GPT-3 for the initial Hendryks study: US History, 52.9%; European History, 53.9%; and World History, 56.1%. Full data for questions for history and other disciplines can be found at: (<cite id=\"cite2c-27937/A834FRJL\"><a href=\"#zotero%7C27937%2FA834FRJL\"></a></cite>) Many thanks to Dan Hendrycks for sharing the discipline-specific accuracy rates for these fields."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e104de",
            "metadata": {},
            "source": [
                "The moral and ethical grounding of LLMs remains a key concern. However, rapid advances in model development have occurred since 2021, and subsequent models demonstrate substantial gains in performance on these historical benchmarks. Below are results from a replication study conducted in September 2024 across a series of leading LLMs, along with the initial Hendryks test:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "93282b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image\n",
                "from IPython.display import display\n",
                "\n",
                "# Load the image from the article GitHub URL\n",
                "table_1_url = 'https://raw.githubusercontent.com/Dr-Hutchinson/jdh_article/main/media/Table%201%20-%20MMLU%20Benchmark%20Performance.png'\n",
                "display(Image(url=table_1_url))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d970f5",
            "metadata": {},
            "source": [
                "Data from this replication study can be accessed via the HELM Leaderboard for the MMLU Benchmark, hosted by the Center for Research on Foundation Models at Stanford University. (<cite id=\"cite2c-27937/GSIXPJ7P\"><a href=\"#zotero%7C27937%2FGSIXPJ7P\">(Mai and Laing, 2024)</a></cite>). You can directly experiment with LLM performance on these benchmarks via a digital history project accompanying this article, \"What Do AIs Know About History?\" (<cite id=\"cite2c-27937/5AL5LZ2K\"><a href=\"#zotero%7C27937%2F5AL5LZ2K\">(Hutchinson, 2022)</a></cite>)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "665d5c",
            "metadata": {},
            "source": [
                "Rapid improvement on this benchmark have been made in just a few years, with a variety of commercial and open-source LLMs now demonstrating expert-level accuracy on all three of the subject exams. These findings mirror the striking performance of models like GPT-4 in other knowledge domains such as medical school curricula (<cite id=\"cite2c-27937/VEDFUUBA\"><a href=\"#zotero%7C27937%2FVEDFUUBA\"></a></cite>), American bar exams (<cite id=\"cite2c-27937/A39GT7B4\"><a href=\"#zotero%7C27937%2FA39GT7B4\"></a></cite>), and a host of other standardized assessments. (<cite id=\"cite2c-27937/U534FF7L\"><a href=\"#zotero%7C27937%2FU534FF7L\">(OpenAI, 2023)</a></cite>)\n",
                "\n",
                "Yet, why do some LLMs perform better in some knowledge domains than others? How can a model get one question right, while other questions generate errors? There is a temptation to parse the model's performance in ways relatable to our human perspective. The human test taker might approach the question by assessing what types of historical thinking each question requires, what sort of knowledge is offered by the options, and how the historical source relates to the question. But, of course, LLMs aren't human - and unlike the human test taker, these models have likely already seen the questions in advance. In 2023 alone, over 467,000 students took the A.P. U.S. History exam. (insert citation) Significant online resources have emerged to serve the sizable population of students and instructors participating in this international curriculum. Hundreds of exam questions have migrated online via the collective efforts of the test prep publishing industry, various study apps, and uploaded example tests. \n",
                "\n",
                "Thus the capabilities of LLMs on these benchmarks directly relates to the vast dataset used to train them: the Internet itself. The data collection built for training GPT-3 encompassed the majority of English-language Wikipedia, Reddit's thousands of discussion forums, extensive corpora of digitized books, and a filtered (yet immense) collection of billions of web pages contained in the Common Crawl repository (<cite id=\"cite2c-27937/KNEK45E4\"><a href=\"#zotero%7C27937%2FKNEK45E4\">(Brown et al., 2020)</a></cite>, 8-9). The training sets used for subsequent LLMs remains largely unknown, as AI firms keep their data a closely guarded proprietary secret; indeed, the future of LLMs may depend on pending litigation concerning copyright infringement in the use of this data. Yet given the scale of such datasets, many of the A.P. History questions and their answers have likely ended up in the training data of these LLMs. If those who critique LLMs as \"stochastic parrots\" are correct, these gains in performance come from improvements in models memorizing their training data, and not through any analytical process. (<cite id=\"cite2c-27937/MVDFMR8K\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">(Bender et al., n.d.)</a></cite>, 618.) When LLMs encounter questions outside of their training data, their accuracy is likely to suffer - although the model will not indicate any uncertainty, but will instead confidently assert error as fact. \n",
                "\n",
                "Such inaccuracies are a phenomenon described by AI researchers as \"hallucinations.\" Such hallucinations represent a major challenge in LLM research and for many practical applications of this technology, particularly given the remarkable effectiveness of these models in generating convincing and otherwise accurate prose. (<cite id=\"cite2c-27937/9GQG6VFM\"><a href=\"#zotero%7C27937%2F9GQG6VFM\"></a></cite>) Detecting such errors can be difficult. Initial testing by OpenAI on the GPT series demonstrated that human readers often struggle to identify text generated by LLMs (<cite id=\"cite2c-27937/KNEK45E4\"><a href=\"#zotero%7C27937%2FKNEK45E4\">(Brown et al., 2020)</a></cite> 16, table 7.3). Rectifying such hallucinations is a significant area of LLM research. However, some scholars, like computational linguist Emily Bender, argue that such behaviors are inherent flaws in LLMs (<cite id=\"cite2c-27937/TPGPSRAI\"><a href=\"#zotero%7C27937%2FTPGPSRAI\"></a></cite>). \n",
                "\n",
                "Additional risks confront historians using these technologies. While AI firms seek to remove potentially offensive texts from their training sets, the sheer scale of these data make selective curation very challenging. Consequently, LLMs generate responses reflecting both the best and the worst of our online world. This reality has troubled previous AI implementations. Well-intentioned researchers have created chatbots that spew hateful invective, human resources applications that refuse to hire female applicants, and algorithms based on criminal justice sentencing guidelines that starkly reinforce racial disparities already prevalent in the carceral system (<cite id=\"cite2c-27937/5YDNQS4V\"><a href=\"#zotero%7C27937%2F5YDNQS4V\"></a></cite>). Early models in the GPT series have been known to unexpectedly generate responses in innocuous contexts containing violent imagery, sexually explicit language, and racial, ethnic, and religious slurs (<cite id=\"cite2c-27937/NYNDVYMM\"><a href=\"#zotero%7C27937%2FNYNDVYMM\"></a></cite>). These findings further confirm the prescient warnings offered by scholars such as Safiya Umoja Noble (<cite id=\"cite2c-27937/IEQ8GAVU\"><a href=\"#zotero%7C27937%2FIEQ8GAVU\"></a></cite>), Timnit Gebru (<cite id=\"cite2c-27937/S3ADX5DD\"><a href=\"#zotero%7C27937%2FS3ADX5DD\"></a></cite>), Ruha Benjamin (<cite id=\"cite2c-27937/X4D92B7V\"><a href=\"#zotero%7C27937%2FX4D92B7V\"></a></cite>), Kate Crawford (<cite id=\"cite2c-undefined\"><a href=\"#zotero%7C27937%2FYVTAGDKZ\"></a></cite>), and Trevor Paglen (<cite id=\"cite2c-27937/5GTQD5W9\"><a href=\"#zotero%7C27937%2F5GTQD5W9\"></a></cite>) on digital practices that reinforce analog inequalities. Some AI researchers consider such behaviors as lamentable but solvable problems through further technical advances, particularly with the use of methods like Reinforcement Learning from Human Feedback (RLHF). (<cite id=\"cite2c-27937/TGPDB8WX\"><a href=\"#zotero%7C27937%2FTGPDB8WX\">(Deep Learning Group, 2021)</a></cite>) Reducing the impact of such biases is a significant research area, particularly through the creation of smaller, more carefully curated datasets for AI training. However, many historians will likely share the skepticism of some researchers concerning such mitigations. (<cite id=\"cite2c-27937/MHRIEHH8\"><a href=\"#zotero%7C27937%2FMHRIEHH8\"></a></cite>) Bias emerges from more than just explicit language or imagery but from the very structures of societies. Can any historical source be separated from its context as a neutral artifact, free of its creator's perspective and the influences of its time? What about the untold millions of sources that make up the scale of an LLM's training set?\n",
                "\n",
                "To be sure, LLMs are imperfect digital tools, and given these flaws historians must exercise caution when employing this technology. Yet scholars are finding that within the confines of these imperfections there is real potential to advance historical research. While a LLM's facility with multiple-choice questions might be the product of memorization, such knowledge has long been a springboard for more advanced forms of historical inquiry. And A.P. study guides are not the only historical texts LLMs are trained on. Primary source collections, academic monographs, open-source scholarly journals - these too inform an LLM's training. The influence of these sources can be found when LLMs are posed more complex questions in a structured prompt. Let's return to the earlier A.P. question above featuring the Populist-era campaign song \"The Hayseed.\" In the code blocks below, GPT-4 is given the lyrics and publication history of the song. (<cite id=\"cite2c-27937/R5P23ZWU\"><a href=\"#zotero%7C27937%2FR5P23ZWU\"></a></cite>) GPT-4 is then prompted to identify the larger historical context of the source, the song's intended purpose and audience, and how the source might be interpreted via different historiographical approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "7028bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Source: Arthur L. Kellog, \u201cThe Hayseed,\u201d Farmers Alliance (4 October 1890). Nebraska Newspapers (University of Nebraska Libraries), https://nebnewspapers.unl.edu/lccn/2017270209/1890-10-04/ed-1/seq-1/. Original citation found in: John Donald Hicks, The Populist Revolt: A History of the Farmers' Alliance and the People's Party (University of Minnesota Press, 1931), 168, fn. 30.\n",
                "\n",
                "from IPython.display import Image, display, Markdown\n",
                "\n",
                "hayseed_url = 'https://raw.githubusercontent.com/Dr-Hutchinson/jdh_article/refs/heads/main/media/hayseed.png'\n",
                "display(Image(url=hayseed_url))\n",
                "\n",
                "# Display the citation\n",
                "display(Markdown(\"\"\"Arthur L. Kellog, \u201cThe Hayseed,\u201d *Farmers Alliance* (4 October 1890). Nebraska Newspapers (University of Nebraska Libraries), https://nebnewspapers.unl.edu/lccn/2017270209/1890-10-04/ed-1/seq-1/. \\n\\nOriginal citation found in: John Donald Hicks, *The Populist Revolt: A History of the Farmers' Alliance and the People's Party* (University of Minnesota Press, 1931), 168, fn. 30.\"\"\"))\n",
                "#display(resized_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "1544ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from IPython.display import Markdown, display\n",
                "\n",
                "# URL of the raw text file on GitHub\n",
                "file_url = 'https://raw.githubusercontent.com/Dr-Hutchinson/jdh_article/main/prompts/primary_source_analysis.txt'\n",
                "\n",
                "# Fetch the content of the file\n",
                "response = requests.get(file_url)\n",
                "\n",
                "# Download and format the prompt for primary source analysis\n",
                "primary_source_analysis_prompt = response.text.replace('\\\\n', '\\n')\n",
                "    \n",
                "# Display the content as markdown\n",
                "display(Markdown(\"**Primary Souce Analysis Prompt fed to GPT-4o:**\\n\\n\"+ primary_source_analysis_prompt))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "86ffaa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# installing openai\n",
                "!pip install openai\n",
                "!pip install jiwer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "60f24f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code for running primary source analysis of the \"Hayseed\" with OpenAI's GPT-4o model.\n",
                "\n",
                "from openai import OpenAI\n",
                "from IPython.display import Markdown, display\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "# Create the query for the LLM\n",
                "query = client.chat.completions.create(\n",
                "    model=\"gpt-4o\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": primary_source_analysis_prompt}\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Extract the output from the response\n",
                "output = query.choices[0].message.content\n",
                "\n",
                "# Display the output using Markdown\n",
                "display(Markdown(\"**GPT-4's Interpretation of 'The Hayseed'**\\n\\n\" + output))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9519e3",
            "metadata": {},
            "source": [
                "While one can debate aspects of GPT-4\u2019s interpretations, it nonetheless accurately captures much of the context and intent of the source. With the right design, LLMs could be automated to annotate an entire corpus of archival sources, becoming a tool of the digital historian overwhelmed by an abundance of historical data, as envisioned by Roy Rozenweig twenty years ago. Yet LLM hallucinations are already contributing this deluge. Further experimentation will be needed to more fully assess LLM's capabilities for historical interpretation, as well as the creation of new benchmarks to test different approaches to historical analysis. But progress moves quickly in the field of generative AI, and there is intense competition to build new models that advance the existing capabilities of LLMs while shedding their shortcomings. Yet progress remains uneven. Of significant concern are LLM's performance on benchmarks on ethics and morality, which continue to demonstrate troubling areas of weakness. (<cite id=\"cite2c-27937/USPCB8UK\"><a href=\"#zotero%7C27937%2FG5ESJ8NI\">(Hoffmann et al., 2022)</a></cite>, 31, table A6)\n",
                "\n",
                "Historians should contribute to the broader dialogue about the implications of these technologies, especially as they become increasingly embedded in our digital lives. While imperfect tools, their flaws do not mean that LLMs have no place in the historian's toolkit. In fact, by acknowledging and confronting these shortcomings, historians can better contribute our disciplinary perspectives on the debates concerning this technology, particularly in leveraging the strengths of these models to empower and broaden accessibility. The case studies below demonstrate how historians are using LLMs as a versatile tools for both researching and communicating the past."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f21693",
            "metadata": {},
            "source": [
                "# LLM Use Case Studies"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45848e",
            "metadata": {},
            "source": [
                "A promising approach for LLMs and other foundational AI models is for data preparation and cleanup. A general rule of thumb is that 80% of the labor involved in data analysis is dedicated to preparing the data (, ix)). AI models hold significant potential to streamline and accelerate the challenging work of creating \"tidy datasets\" ()."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "69c494",
            "metadata": {},
            "source": [
                "## Case Study: Oral History Transcriptions"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c6e59",
            "metadata": {},
            "source": [
                "Oral history provides a particularly useful case study for demonstrating the potential utility of generative AI. Transcribing audio recordings is a central labor in this methodology, but transcriptions often require considerable expense in both time and labor, typically requiring six to eight hours of processing for a single page. (<cite id=\"cite2c-27937/YWJAQ4V8\"><a href=\"#zotero%7C27937%2FYWJAQ4V8\">(Ritchie, 2nd ed, 2003)</a></cite>) However, advances in machine learning models have resulted in impressive gains in streamlining this task. \n",
                "\n",
                "Notable among these models is Whisper, an open-source audio transcription model developed by OpenAI that belongs to the same Transformer family as the GPT series. (<cite id=\"cite2c-27937/7VHKCH3M\"><a href=\"#zotero%7C27937%2F7VHKCH3M\"></a></cite>). Let's test Whisper on the first two minutes of a transcribed oral history of historian John Hope Franklin by the Southern Oral History Program (<cite id=\"cite2c-27937/IJWETJM7\"><a href=\"#zotero%7C27937%2FIJWETJM7\"></a></cite>). Recorded on audiotape in 1990, this segment features multiple voices, crosstalk, filler words, and background noise. You can listen to the segment in the code block below. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "f7852f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interview with John Hope Franklin by John Egerton, July 27, 1990. \n",
                "# Interview A-0339, Collection #4007.\n",
                "# Southern Oral History Program Collection, Southern Historical Collection, \n",
                "# Wilson Library, University of North Carolina at Chapel Hill. \n",
                "# https://docsouth.unc.edu/sohp/A-0339/menu.html \n",
                "\n",
                "from IPython.display import Audio\n",
                "\n",
                "file_path = \"./A-0339_edited.mp3\"\n",
                "Audio(file_path)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0c4b74",
            "metadata": {},
            "source": [
                "In the code below, we will use Whisper to transcribe the audio segment and compare it against the official transcript. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1cb0d",
            "metadata": {},
            "source": [
                "The Whisper series is offered as a series of open-source voice recognition and voice translation models across several tiers of computing power and across 57 languages, and freely available and hosted on HuggingFace. (<cite id=\"cite2c-27937/GJ2EYJWT\"><a href=\"#zotero%7C27937%2FGJ2EYJWT\"></a></cite>) However, for simplicity this demonstration code uses OpenAI's API for the Whisper-1 model. As of September 2024, OpenAI charged $0.36 per hour of recorded time for transcriptions using the API.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "39499f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "import time\n",
                "\n",
                "# Initialize the OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "audio_file = open(\"./A-0339_edited.mp3\", \"rb\")\n",
                "\n",
                "# Measure the transcription time for the audio file\n",
                "start_time = time.time()\n",
                "\n",
                "transcription = client.audio.transcriptions.create(\n",
                "  model=\"whisper-1\", \n",
                "  file=audio_file\n",
                ")\n",
                "whisper_transcript = transcription.text\n",
                "\n",
                "end_time = time.time()\n",
                "\n",
                "# Calculate the actual transcription time\n",
                "automation_time = end_time - start_time\n",
                "\n",
                "# Calculate the estimated transcription time for 1 hour (3600 seconds) based on the transcription time for audio segment\n",
                "audio_length_seconds = 153  # 2 minutes and 33 seconds in seconds\n",
                "estimated_time_for_one_hour = (automation_time / audio_length_seconds) * 3600  # Estimate time for 1 hour (3600 seconds)\n",
                "\n",
                "# Convert estimated time for better readability (hours, minutes, and seconds)\n",
                "hours = int(estimated_time_for_one_hour // 3600)\n",
                "minutes = int((estimated_time_for_one_hour % 3600) // 60)\n",
                "seconds = int(estimated_time_for_one_hour % 60)\n",
                "\n",
                "# Display the output in the desired format\n",
                "print(f\"Whisper Transcription time: {automation_time:.2f} seconds\")\n",
                "print(f\"Estimated Transcription Time for an hour recording at this rate: {hours} hours, {minutes} minutes, {seconds} seconds\\n\")\n",
                "print(\"Raw Whisper Transcript:\\n\")\n",
                "print(whisper_transcript)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a8a1b7",
            "metadata": {},
            "source": [
                "Due to the stochastic nature of these models, if you run this code multiple times you'll sometimes get different results, particularly in the most challenging segments. But the code block below visualizes a sample transcription that was annotated and compared against the original. Notable omissions and discrepancies are highlighted. The word error score is also calculated, providing a standard benchmark for the mode's performance on this oral history.   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "513621",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from jiwer import wer\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# Function to clean HTML tags\n",
                "def clean_html(text):\n",
                "    return re.sub(r'<.*?>', '', text)\n",
                "\n",
                "# Load the formatted transcripts\n",
                "original_file_path = './revised_original_transcript_formatted_2.txt'\n",
                "whisper_file_path = './revised_whisper_transcript_formatted_2.txt'\n",
                "\n",
                "# Read the contents of the original and whisper transcripts\n",
                "with open(original_file_path, 'r') as original_file:\n",
                "    original_transcript = original_file.read()\n",
                "\n",
                "with open(whisper_file_path, 'r') as whisper_file:\n",
                "    whisper_transcript = whisper_file.read()\n",
                "\n",
                "# Clean the transcripts for WER calculation\n",
                "cleaned_original_transcript = clean_html(original_transcript)\n",
                "cleaned_whisper_transcript = clean_html(whisper_transcript)\n",
                "\n",
                "# Calculate the Word Error Rate (WER)\n",
                "error_rate = wer(cleaned_original_transcript, cleaned_whisper_transcript)\n",
                "\n",
                "# Add <br> tags to preserve line breaks in the text\n",
                "original_transcript = original_transcript.replace('\\n', '<br>')\n",
                "whisper_transcript = whisper_transcript.replace('\\n', '<br>')\n",
                "\n",
                "# Ensure that color highlighting also includes bolding\n",
                "whisper_transcript = whisper_transcript.replace(\n",
                "    'style=\"background-color: #fbb;\"',\n",
                "    'style=\"background-color: #fbb; font-weight: bold;\"'\n",
                ")\n",
                "\n",
                "original_transcript = original_transcript.replace(\n",
                "    'style=\"background-color: #bfb;\"',\n",
                "    'style=\"background-color: #bfb; font-weight: bold;\"'\n",
                ")\n",
                "\n",
                "# Display the two transcripts side by side using HTML in Jupyter\n",
                "html_content = f'''\n",
                "<div style=\"display: flex;\">\n",
                "    <div style=\"width: 50%; padding-right: 20px; border-right: 1px solid black;\">\n",
                "        <h4>Original Transcript: (discrepancies in green)</h4>\n",
                "       {original_transcript}\n",
                "    </div>\n",
                "    <div style=\"width: 50%; padding-left: 20px;\">\n",
                "        <h4>Whisper Transcript: (discrepancies in green)</h4>\n",
                "        {whisper_transcript}\n",
                "    </div>\n",
                "</div>\n",
                "<br><br>\n",
                "<div style=\"text-align: center;\">\n",
                "    <h4>Word Error Rate (WER) for Whisper: {error_rate:.2%}</h4>\n",
                "</div>\n",
                "'''\n",
                "\n",
                "# Render the HTML content in Jupyter\n",
                "display(HTML(html_content))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac8a64",
            "metadata": {},
            "source": [
                "There are some suggestive observations we can take from these results. There are several notable errors, significant ommisions, and changes in syntax. But given the media and the source audio quality, the oral historian has a solid first draft to speed their initial review in a few seconds. Human review is still required. But even best human transcriptions contain errors. In the original transcript, Franklin identifies Harvard as the destination of E. Franklin Frazier in 1934; but the noted sociologist and pioneer actually joined the faculty of Howard University, something audible but easily confused in the original recording. Here Whisper actually corrected a human error. Whisper's performance on this example is in line with other larger studies on similar media. \n",
                "\n",
                "Applications like Whisper hold significant potential for oral historians, as they can dramatically improve the efficiency and cost-effectiveness of transcription workflows. Scholars are already using these techniques for multi-lingual transcription \n",
                "\n",
                "sources - German/Czech Paper - https://www.isca-archive.org/interspeech_2023/lehecka23_interspeech.pdf\n",
                "Holocaust oral history archive - j/https://aclanthology.org/2024.htres-1.6.pdf\n",
                "The Computational Social Science group of the Institut Polytechnique de Paris - https://www.css.cnrs.fr/using-whisper-to-transcribe-oral-interviews/\n",
                "Rochester Tech - https://www.rit.edu/news/artificial-intelligence-aids-cultural-heritage-researchers-documenting-and-teaching-oral\n",
                "ASR Leaderboard - https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\n",
                "Chinese NER with Whisper - https://arxiv.org/abs/2401.11382\n",
                "Whisper paper - https://cdn.openai.com/papers/whisper.pdf\n",
                "\n",
                "New Yorker article - https://www.newyorker.com/tech/annals-of-technology/whispers-of-ais-modular-future\n",
                "https://arxiv.org/abs/2407.17160\n",
                "\n",
                "\n",
                "\n",
                "As this technology continues to advance, it is likely that the accuracy of tools like Whisper will only improve, further enhancing their utility for oral historians and allowing them to focus more on the analysis and interpretation of these sources.\n",
                "\n",
                "Transition to prompt section"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "6adf01",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "f8918c",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "a2b78c",
            "metadata": {},
            "source": [
                "## Case Study: Error Correction of Optical Character Recognition Scans "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c72165",
            "metadata": {},
            "source": [
                "Another potential use case of AI models for digital historians is the error correction of optical character recognition (OCR) scans. Machine learning techniques, such as those pioneered by the research team at Transkribus, have greatly enhanced the quality, speed, and cost of OCR scans across a broad range of historical texts. (<cite id=\"cite2c-27937/FUEUWF5M\"><a href=\"#zotero%7C27937%2FFUEUWF5M\"></a></cite>) However, even high-fidelity OCRs possess error rates that have insidious impacts on the accessibility and searchability of texts collections. (<cite id=\"cite2c-27937/ZJW9AI49\"><a href=\"#zotero%7C27937%2FZJW9AI49\"></a></cite>) For instance, the image below comes from a newspaper published in a German prisoner-of-war camp in Mississippi during World War II and later microfilmed by the Library of Congress. Let's compare an OCR scan of this image via Google's Cloud Vision OCR service with a human transcription of the same text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "0b99a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Source: \"Nur ein Film?.\" Die Lotse (Camp McCain, Mississippi), 30 June 1945. In: Karl John Richard Arndt, editor. German P.O.W. camp papers. Washington, D.C.: Library of Congress Photoduplication Service, 1965. Reel 9.\n",
                "\n",
                "from PIL import Image\n",
                "image = Image.open('./die_lotse_6-30-45_1.png')\n",
                "\n",
                "new_width = 600\n",
                "new_height = int(image.height * (new_width / image.width))\n",
                "\n",
                "# Resize the image\n",
                "resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
                "\n",
                "# Display the resized image\n",
                "display(Markdown(\"\"\"\"Nur ein Film?.\" *Die Lotse* (Camp McCain, Mississippi), 30 June 1945. In: Karl John Richard Arndt, editor. German P.O.W. Camp Papers. Washington, D.C.: Library of Congress Photoduplication Service, 1965. Reel 9.\"\"\"))\n",
                "display(resized_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "253186",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This script compares an OCR output of the image above with human transcription. \n",
                "# Words in red are from the OCR corrections, words in green are from the human transcription.\n",
                "\n",
                "import difflib\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "ocr_output_1 = 'NUR EIN FILM? NUR EIN\\nUnverloeschbar tief haben sich uns die Bilder des Grauens einge- praegt, die jeder von uns dieser Tage in dem ersten amerikanischen Armeefilm aus Deutschland sah. Er schuetterung und Ent setzen haben Jeden Fuehlenden verstummen las sen, aber die Unmenschlichkeit, von \"Deutschen\" auf deutschem Bo- den begangen, lassst den Gesitte- ten nicht schweigend darueberhin- gehen.\\nDante setzt in seinem Werk: 1 \"Die goettliche Komoedie\" ueber den Eingang zur Hoelle die Worte: \"Lasst fahren alle Hoffnungen 1hr, die ihr hier eintritt.\"\\nDiese Worte koonnen ueber je- dem K.Z.-Lager Deutschlands ge standen haben; denn die Bilder des Schreckens und Grauens, wie sie Dante von der Hoelle entwirft, ver- blassen vor dieser schaurigen Wirklichkeit, die sich hier auf Er den unter lebenden Menschen im Herzen Europas abspielte. Was wir sahen, war dabei wohl nur ein kleiner Ausschnitt, wenn wir beden- ken, dass diese Tragoedie seit 1933 unzaehlige Opfer forderte.'\n",
                "human_corrected_output_1 = 'NUR EIN FILM?\\nUnverloeschbar tief haben sich uns die Bilder des Grauens eingepraegt, die jeder von uns dieser Tage in dem ersten amerikanischen Armeefilm aus Deutschland sah. Erschuetterung und Entsetzen haben jeden Fuehlenden verstummen lassen, aber die Unmenschlichkeit, von \"Deutschen\" auf deutschem Boden begangen, laesst den Gesitteten nicht schweigend darueberhingehen.\\nDante setzt in seinem Werk: \"Die goettliche Komoedie\" ueber den Eingang zur Hoelle die Worte: \"Lasst fahren alle Hoffnungen ihr, die ihr hier eintritt.\"\\nDiese Worte koennen ueber jedem K.Z.-Lager Deutschlands gestanden haben; denn die Bilder des Schreckens und Grauens, wie sie Dante von der Hoelle entwirft, verblassen vor dieser schaurigen Wirklichkeit, die sich hier auf Erden unter lebenden Menschen im Herzen Europas abspielte. Was wir sahen, war dabei wohl nur ein kleiner Ausschnitt, wenn wir bedenken, dass diese Tragoedie seit 1933 unzaehlige Opfer forderte.'\n",
                "\n",
                "differ = difflib.Differ()\n",
                "diff1 = list(differ.compare(ocr_output_1.split(), human_corrected_output_1.split()))\n",
                "\n",
                "def ocr1_vs_human_1(diff1):\n",
                "    result1 = []\n",
                "    for word in diff1:\n",
                "        if word.startswith('+'):\n",
                "            result1.append(f'<span style=\"color:green;background-color:#e6ffe6;\">{word[2:]}</span>')\n",
                "        elif word.startswith('-'):\n",
                "            result1.append(f'<span style=\"color:red;background-color:#ffe6e6;\">{word[2:]}</span>')\n",
                "        elif word.startswith(' '):\n",
                "            result1.append(word[2:])\n",
                "    return ' '.join(result1)\n",
                "\n",
                "colored_diff_1 = ocr1_vs_human_1(diff1)\n",
                "\n",
                "display(HTML(f'<p><strong>Differences between OCR Output (red) vs Human Transcription (green):</strong></p><p>{colored_diff_1}</p>'))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "21bb35",
            "metadata": {},
            "source": [
                "While the image quality is satisfactory and the text is printed using modern typefaces, the OCR still generates errors requiring human correction. Correcting such errors necessitates substantial review and intervention, representing a significant labor for processing a sizable text corpus. However, LLMs can expedite this correction process when guided by a carefully designed prompt. This practice, often referred to as \"prompt engineering,\" is a method used to direct LLMs in completing specific tasks. Details concerning each prompt and the methods behind it are described in the hermeneutical layer."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e00585",
            "metadata": {},
            "source": [
                "Let's now see how GPT-4 interprets this prompt, and compare the generated corrections with the initial OCR scan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "0ad1fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompt 1: OCR Correction\n",
                "\n",
                "ocr_prompt = \"\"\"You are an AI research assistant with a specialty in correcting errors in OCR scans of  newspaper images. In the following task you will be given a OCR'd text, and you will generate corrections using the following Task Format. Follow the instructions of the Format step-by-step.\\n\\nTask Format\\n1. Examine the Examples: Examine the two examples of sample OCR generations, Sample Text 1 and Sample Text 2. \\n2. Examine the Corrections: Examine the two examples of OCR corrections given, Corrected Transcription 1 and Corrected Transcription 2. Compare the changes between the Samples to the Corrected Transcriptions.\\n3. Note Formatting in the Corrected Transcriptions: Within each corrected transcription are symbols to represent uncertainty or substantial edits to the original OCR. These are inserted to communicate to the user which words may need additional human review. Words that you are very uncertain about are bracketed with a \\***\\ before and after the very uncertain word. \\n4. Examine New OCR Generation: You will then be given a new OCR generation.\\n5. Generate New Corrected Transcription: Based on the examples and the prompt instructions, compose a New Corrected Transcription based on the New OCR Generation. Do your best to make it as accurate as possible. Do not correct the grammar or wording of the text, only seek to correct errors in the OCR. Likewise do not add umlauts, eszetts, or other diacritics. \\n\\nLet's begin.\\n\\nSample Text 1\\n\\n#begin sample text 1\\n\\nEIN LLERRUNDGANG\\n\\nit diosom Boitreg wird der Boricht des\\nOborfoldwebels ltstaodt uobor seinen Rund-\\ngang durchs Lagor c.bieschlosson.\\nIm Juli bozogon dig orston PoWs ihra \"zwungsheimat\" im Camp I. wio sah es\\ndesnals in diesom Lagor aus ? Bcreckon vurde:: aufgebaut; \\n\\n#end sample text 1\\n\\nHere is Sample Correction 1. \\n\\n#begin sample correction 1\\n\\nEIN LAGERRUNDGANG\\n\\nMit diesem Beitrag wird der Bericht des Oberfeldwebels Altstaedt ueber seinen Rundgang durchs Lager abgeschlossen.\\nIm Juli bezogen die ersten PoWs ihre \"Zwangsheimat\" im Camp I. Wie sah es damals in diesem Lager aus? Barecken wurden aufgebaut\\n#end sample correction 1\\n\\nHere is Sample Text 2:\\n\\n#begin sample text 2\\n\\nDem crsten bericut, der den kunagang duro:\\ndas dritte bateillon schilcerte, lesson wir\\nheuto den ueber das zwcito bataillon folgen.\\nDie Schriftl.\\n\\nim bingang zum zweiten Lataillon ruht der blick auf dor Lagerstresse, did\\nscharf ansteigt Lirks und rechts zichon sich sauber geinauerte Graeben entlane\\n#end sample text 2\\n\\nHere is Sample Correction 2:\\n\\n#begin sample correction 2\\nDem ersten ***Bericht***, der den ***Aufmarsch***\\ndes dritten Bataillons schilderte, lessen wir  \\nheute der Ueber das zweite Bataillon. \\nDie Schriftl. \\n\\nAm Anfang des zweiten Bataillons ruht der Blick auf der Lagerstrasse, die \\nscharf ansteigt. Links und rechts sich sauber eingegrabene Graeben entlang.\\n\\n#end sample correction 2\\n\\nHere is the New OCR Generation:\\n\\n#begin new corrected transcription without umlauts, eszetts, or other diacritics\\n\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18e65e",
            "metadata": {},
            "source": [
                "Prompt engineering utilizes the emergent capabilities of LLMs to solve complex tasks. This prompt employs two common prompt methods:\n",
                "\n",
                "Few-shot learning: In this prompt, two sample OCR generations (Sample Text 1 and Sample Text 2) are provided, along with their corresponding corrected transcriptions (Corrected Transcription 1 and Corrected Transcription 2). These examples serve as a learning basis for the model to emulate the task and perform it on a new OCR generation. This technique allows the AI to generalize from the provided examples and apply its insights to new instances. (<cite id=\"cite2c-27937/KNEK45E4\"><a href=\"#zotero%7C27937%2FKNEK45E4\">(Brown et al., 2020)</a></cite>)\n",
                "\n",
                "Chain-of-thought reasoning: The prompt guides the model through a series of steps to complete the task. It starts with examining the sample texts and their corrections, noting the formatting in the corrected transcriptions, and then proceeds to work on a new OCR generation. It is theorized that this structured approach helps LLMs better determine the desired output, which in this case is a more accurate transcription. (<cite id=\"cite2c-27937/4QKVA4R7\"><a href=\"#zotero%7C27937%2F4QKVA4R7\"></a></cite>)\n",
                "\n",
                "The versatility of these prompting methods enables the completion of a broad range of tasks, as does the ability to prompt LLMs using natural language instructions. Understanding and developing prompt approaches is an active area of research and experimentation. An excellent resource for exploring prompt engineering has been published by DAIR.AI (Democratizing Artificial Intelligence Research, Education, and Technologies). (<cite id=\"cite2c-27937/3I2YH8D3\"><a href=\"#zotero%7C27937%2F3I2YH8D3\"></a></cite>)\n",
                "\n",
                "Prompt 1: OCR Correction\n",
                "\n",
                "You are an AI research assistant with a specialty in correcting errors in OCR scans of newspaper images. In the following task you will be given a OCR\u2019d text, and you will generate corrections using the following Task Format. Follow the instructions of the Format step-by-step.\n",
                "\n",
                "Task Format\n",
                "\n",
                "1. Examine the Examples: Examine the two examples of sample OCR generations, Sample Text 1 and Sample Text 2.\n",
                "\n",
                "2. Examine the Corrections: Examine the two examples of OCR corrections given, Corrected Transcription 1 and Corrected Transcription \n",
                "\n",
                "3. Compare the changes between the Samples to the Corrected Transcriptions. Note Formatting in the Corrected Transcriptions: Within each corrected transcription are symbols to represent uncertainty or substantial edits to the original OCR. These are inserted to communicate to the user which words may need additional human review. Words that you are very uncertain about are bracketed with a ***\\ before and after the very uncertain word.\n",
                "\n",
                "4. Examine New OCR Generation: You will then be given a new OCR generation.\n",
                "\n",
                "5. Generate New Corrected Transcription: Based on the examples and the prompt instructions, compose a New Corrected Transcription based on the New OCR Generation. Do your best to make it as accurate as possible. Do not correct the grammar or wording of the text, only seek to correct errors in the OCR. Likewise do not add umlauts, eszetts, or other diacritics.\n",
                "\n",
                "Let\u2019s begin.\n",
                "\n",
                "Sample Text 1\n",
                "\n",
                "#begin sample text 1\n",
                "\n",
                "EIN LLERRUNDGANG\n",
                "\n",
                "it diosom Boitreg wird der Boricht des Oborfoldwebels ltstaodt uobor seinen Rund- gang durchs Lagor c.bieschlosson. Im Juli bozogon dig orston PoWs ihra \u201czwungsheimat\u201d im Camp I. wio sah es desnals in diesom Lagor aus ? Bcreckon vurde:: aufgebaut;\n",
                "\n",
                "#end sample text 1\n",
                "\n",
                "Here is Sample Correction 1.\n",
                "\n",
                "#begin sample correction 1\n",
                "\n",
                "EIN LAGERRUNDGANG\n",
                "\n",
                "Mit diesem Beitrag wird der Bericht des Oberfeldwebels Altstaedt ueber seinen Rundgang durchs Lager abgeschlossen. Im Juli bezogen die ersten PoWs ihre \u201cZwangsheimat\u201d im Camp I. Wie sah es damals in diesem Lager aus? Barecken wurden aufgebaut \n",
                "\n",
                "#end sample corrrection 1\n",
                "\n",
                "Here is Sample Text 2:\n",
                "\n",
                "#begin sample text 2\n",
                "\n",
                "Dem crsten bericut, der den kunagang duro: das dritte bateillon schilcerte, lesson wir heuto den ueber das zwcito bataillon folgen. Die Schriftl.\n",
                "\n",
                "im bingang zum zweiten Lataillon ruht der blick auf dor Lagerstresse, did scharf ansteigt Lirks und rechts zichon sich sauber geinauerte Graeben entlane \n",
                "\n",
                "#end sample text 2\n",
                "\n",
                "Here is Sample Correction 2:\n",
                "\n",
                "#begin sample correction 2 \n",
                "\n",
                "Dem ersten Bericht, der den Aufmarsch des dritten Bataillons schilderte, lessen wir\n",
                "heute der Ueber das zweite Bataillon. Die Schriftl.\n",
                "\n",
                "Am Anfang des zweiten Bataillons ruht der Blick auf der Lagerstrasse, die scharf ansteigt. Links und rechts sich sauber eingegrabene Graeben entlang.\n",
                "\n",
                "#end sample correction 2\n",
                "\n",
                "Here is the New OCR Generation:\n",
                "\n",
                "#begin new corrected transcription without umlauts, eszetts, or other diacritics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "75f3d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OpenAI completion using the GPT-4 model with the OCR correction prompt.\n",
                "\n",
                "query = openai.ChatCompletion.create(\n",
                "                      model=\"gpt-4\",\n",
                "                      messages=[\n",
                "                            {\"role\": \"assistant\", \"content\": ocr_prompt},\n",
                "                            {\"role\": \"user\", \"content\": ocr_output_1}\n",
                "                        ]\n",
                "                    )\n",
                "                             \n",
                "gpt4_output_1 = query['choices'][0]['message']['content']\n",
                "\n",
                "# Comparing GPT-4's output with the initial OCR scan results.\n",
                "\n",
                "differ = difflib.Differ()\n",
                "diff = list(differ.compare(gpt4_output_1.split(), human_corrected_output_1.split()))\n",
                "\n",
                "def gpt4_vs_human_1(diff2):\n",
                "    result = []\n",
                "    for word in diff:\n",
                "        if word.startswith('+'):\n",
                "            result.append(f'<span style=\"color:green;background-color:#e6ffe6;\">{word[2:]}</span>')\n",
                "        elif word.startswith('-'):\n",
                "            result.append(f'<span style=\"color:red;background-color:#ffe6e6;\">{word[2:]}</span>')\n",
                "        elif word.startswith(' '):\n",
                "            result.append(word[2:])\n",
                "    return ' '.join(result)\n",
                "\n",
                "colored_diff_2 = gpt4_vs_human_1(diff)\n",
                "\n",
                "display(HTML(f'<p><strong>Differences between GPT-4 Output (red) vs Human Transcription (green):</strong></p><p>{colored_diff_2}</p>'))\n",
                "display(HTML(f'<p><strong>Differences between OCR Output (red) vs Human Transcription (green):</strong></p><p>{colored_diff_1}</p>'))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "634fec",
            "metadata": {},
            "source": [
                "In comparing the two outputs against the human transcription, GPT-4 demonstrates a remarkable ability to correct the errors in the OCR scan. GPT-4's ability to indicate its uncertanity in its error corrections also speeds human review of its output. However, the quality of the initial OCR scan remains crucial for a successful output. For example, here is an lower-quality image containing 'noise' that causes substantial errors in the OCR output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "a22139",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Source: \"Zum Geleit.\" Die Lotse (Camp McCain, Mississippi), 15 March 1945. In: Karl John Richard Arndt, editor. German P.O.W. camp papers. Washington, D.C.: Library of Congress Photoduplication Service, 1965. Reel 9.\n",
                "\n",
                "image = Image.open('./die_lotse_3-15-45_1.png')\n",
                "\n",
                "new_width = 600\n",
                "new_height = int(image.height * (new_width / image.width))\n",
                "\n",
                "# Resize the image\n",
                "resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
                "\n",
                "# Display the resized image\n",
                "display(Markdown(\"\"\"\"Zum Geleit.\" *Die Lotse* (Camp McCain, Mississippi), 15 March 1945. In: Karl John Richard Arndt, editor. German P.O.W. Camp Papers. Washington, D.C.: Library of Congress Photoduplication Service, 1965. Reel 9.\"\"\"))\n",
                "display(resized_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "343e11",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OpenAI completion using the GPT-4 model.\n",
                "\n",
                "ocr_output_2 = \"Zum Deleit:\\nDie neue Lagerzeitung ist nun erschienen. Ja eis ist nun ehr eine unengoare Totesche reworden und we ate anime der Prisoner in stilien Stunden und in froler Laune ersonnon, ier findet ihr es schwarz auf weiss.\\nUeber manches moschtet ihr nachdenken, ueber manches euch freuen, belaecheln koennt ihr aller, aber denkt iaren wie an es besser nachen koennte und seit mit Vorschlaegen nicht geizig und zurueckhaltend. Alles, or euch bewegt, arnstes und Heiteres, soll seinen Platz Tinden in dieren. Blaettern, nur Politik lasst ferne.\\nWenn euch diese Zeitung Errunterung Unterhaltung und Anregung Ceben, so ist das Cer rchoenste Loin fuer die Nuehe aller, die um das Zustandekommen dieser Laerzeitung benueht war'n.\\nwollen\\nNoolimals, Jeder arbeite mit an diesen schoenen Werk, nach der Parole Alles von Prisoner fuer Prisoner wir die Zeitung fuehren.\\nDas Erscheinen ist nonetlich zreimal vorgesehen. Einsendungen werden nach Hasnabe des verfuegberen Platzes aufgenommen, wobei kein besonders kritischer Kesesta oezue lich er kuenetlerischen Vollendun; an- Celest sird, inner in denkt daran sie viele Kameraden sure Geisteeprodukte lesen und wir doch eine Auerall treffen muessen.\\nDie Sohriftleitung.\"\n",
                "human_corrected_output_2 = \"Zum Geleit:\\nDie neue Lagerzeitung ist nun erschienen. Ja sie ist nunmehr eine unlengoare Tatsache geworden und was die Oshirne [?] der Prisoner in stillen Stunden und in froher Laune ersonnen, hier findet ihr es schwarz auf weiss.\\nUeber manches moechtet ihr nachdenken, ueber manches euch freuen, belaecheln koennt ihr aller, aber denkt daran wie man es besser machen koennte und seit mit Vorschlaegen nicht geizig und zurueckhaltend. Alles, was euch bewegt, Ernstes und Heiteres, soll seinen Platz finden in diesen Blaettern, nur Politik lasst ferne.\\nWenn euch diese Zeitung Ermunterung, Unterhaltung und Anregung geben, so ist das der schoenste Lohn fuer die Muehe aller, die um das Zustandekommen dieser Lagerzeitung bemueht war'n.\\nNochmals, jeder arbeite mit an diesem schoenen Werk, nach der Parole \u201cAlles von Prisoner fuer Prisoner\u201d wollen wir die Zeitung fuehren.\\nDas Erscheinen ist monatlich zweimal vorgesehen. Einsendungen werden nach Hasnabe des verfuegbaren Platzes aufgenommen, wobei kein besonders kritischer Massstab bezueglich der k\u00fcnstlerischen Vollendung angelegt wird, immerhin denkt daran sie viele Kameraden eure Geistesprodukte lesen und wir doch eine Auswahl treffen muessen.\\nDie Schriftleitung.\"\n",
                "\n",
                "query = openai.ChatCompletion.create(\n",
                "                      model=\"gpt-4\",\n",
                "                      messages=[\n",
                "                            {\"role\": \"assistant\", \"content\": ocr_prompt},\n",
                "                            {\"role\": \"user\", \"content\": ocr_output_2}\n",
                "                        ]\n",
                "                    )\n",
                "\n",
                "gpt4_output_2 = query['choices'][0]['message']['content']\n",
                "\n",
                "\n",
                "# Comparing GPT-4's output with the human transcription.\n",
                "\n",
                "differ = difflib.Differ()\n",
                "diff2 = list(differ.compare(ocr_output_2.split(), human_corrected_output_2.split()))\n",
                "\n",
                "def ocr2_vs_human_2(diff):\n",
                "    result2 = []\n",
                "    for word in diff2:\n",
                "        if word.startswith('+'):\n",
                "            result2.append(f'<span style=\"color:green;background-color:#e6ffe6;\">{word[2:]}</span>')\n",
                "        elif word.startswith('-'):\n",
                "            result2.append(f'<span style=\"color:red;background-color:#ffe6e6;\">{word[2:]}</span>')\n",
                "        elif word.startswith(' '):\n",
                "            result2.append(word[2:])\n",
                "    return ' '.join(result2)\n",
                "\n",
                "colored_diff_3 = ocr2_vs_human_2(diff2)\n",
                "                           \n",
                "\n",
                "differ = difflib.Differ()\n",
                "diff3 = list(differ.compare(gpt4_output_2.split(), human_corrected_output_2.split()))\n",
                "\n",
                "def gpt4_vs_human_2(diff3):\n",
                "    result_3 = []\n",
                "    for word in diff3:\n",
                "        if word.startswith('+'):\n",
                "            result_3.append(f'<span style=\"color:green;background-color:#e6ffe6;\">{word[2:]}</span>')\n",
                "        elif word.startswith('-'):\n",
                "            result_3.append(f'<span style=\"color:red;background-color:#ffe6e6;\">{word[2:]}</span>')\n",
                "        elif word.startswith(' '):\n",
                "            result_3.append(word[2:])\n",
                "    return ' '.join(result_3)\n",
                "\n",
                "colored_diff_4 = gpt4_vs_human_2(diff3)\n",
                "\n",
                "display(HTML(f'<p><strong>Differences between GPT-4 Output (red) vs Human Transcription (green):</strong></p><p>{colored_diff_4}</p>'))\n",
                "display(HTML(f'<p><strong>Differences between OCR Output (red) vs Human Transcription (green):</strong></p><p>{colored_diff_3}</p>'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aefe78",
            "metadata": {},
            "source": [
                "Here we see that GPT-4 achieved only modest improvements in the OCR output, perhaps an indication of the limits of this approach. However, GPT-4's multimodal nature may open new opportunities for the future. GPT-4's multi-modal interface will soon allow it to directly perform prompted OCR scans of images. (<cite id=\"cite2c-27937/U534FF7L\"><a href=\"#zotero%7C27937%2FU534FF7L\">(OpenAI, 2023)</a></cite>) Only time will tell if these abilities surpass existing OCR techniques. Yet there seems to be remarkable potential for further exploration.\n",
                "\n",
                "These two case studies demonstrate an LLMs capacity to assist in various forms of data cleanup and preparation. While human review remains essential, LLMs can make that review less time-consuming and labor-intensive. LLMs are already being employed for tasks as varied as text normalization, metadata generation, automated summarization, date extraction and standardization, sentiment analysis, relationship extraction, and named entity recognition. Further experimentation will undoubtedly reveal future use cases. Such approaches can improve the accuracy, lower the costs, and accelerate the pace of data preparation. LLMs can also expand accessibility to historical sources, enabling the use of programmatic techniques via natural language instructions."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "214552",
            "metadata": {},
            "source": [
                "## Case Study: Ask-A-Source - Retrieval Based Methods for LLMs"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c64077",
            "metadata": {},
            "source": [
                "While LLMs demonstrate a broad range of capabilities for data cleanup, their tendency towards 'hallucinations' represents a formidable obstacle towards their use in historical research and analysis. However, recent advances in retrieval-based methods offer potential to ground LLMs in greater factual accuracy. (<cite id=\"cite2c-27937/XKINSLH3\"><a href=\"#zotero%7C27937%2FXKINSLH3\"></a></cite>) Such techniques also enable the use of LLMs to analyze large text collections, search the Internet, and utilize external tools to solve problems in unfamiliar knowledge domains. The following case study demonstrates one such approach for historians: how an LLM can be used to answer questions about a historical source.\n",
                "\n",
                "One of the shortcomings of LLMs is their context length, or the hard limits on how much text they can interpret in a single query. Models like GPT-3 and ChatGPT can only process textual data for some two to three pages in length, while GPT-4 possesses a much larger context length. (<cite id=\"cite2c-27937/U534FF7L\"><a href=\"#zotero%7C27937%2FU534FF7L\">(OpenAI, 2023)</a></cite>) Yet even the most advanced models cannot directly interpret long-form texts or large text collections in a single query.\n",
                "\n",
                "Yet these limits can be circumvented through the use of tools like semantic search and prompt chaining. In this case study, two different AI models will work together to answer questions about a historical text, Thomas More's *History of Richard III* (<cite id=\"cite2c-27937/IDRV8C65\"><a href=\"#zotero%7C27937%2FIDRV8C65\"></a></cite>). At the end of the process, GPT-4 will deliver a series of responses supported by direct quotations from the text.\n",
                "\n",
                "Tne first step in this process is semantic search with OpenAI's ADA model, a computational technique for establishing text similarity. Let's pose the following question: \"Who killed the princes in the Tower?\"\n",
                "\n",
                "Here are the sections of the text identified by Ada as the most semantically similar:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "23c13c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Script for semantic search over an embedded text using OpenAI's Ada embedding model.\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
                "\n",
                "question = \"Who killed the princes in the Tower?\"\n",
                "\n",
                "# Computed embeddings of Thomas More's History of Richard III. Avaliable in article Github:\n",
                "\n",
                "datafile_path = \"./more_text_embedded.csv\"\n",
                "\n",
                "df = pd.read_csv(datafile_path)\n",
                "df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
                "\n",
                "def search_text(df, text, n=3, pprint=True):\n",
                "    text_embedding = get_embedding(\n",
                "        text,\n",
                "        engine=\"text-embedding-ada-002\"\n",
                "    )\n",
                "    df[\"similarities\"] = df.embedding.apply(lambda x: cosine_similarity(x, text_embedding))\n",
                "\n",
                "    # Select the first three rows of the sorted DataFrame\n",
                "    top_three = df.sort_values(\"similarities\", ascending=False).head(3)\n",
                "\n",
                "    # If `pprint` is True, display the output\n",
                "    if pprint:\n",
                "        for i, (_, row) in enumerate(top_three.iterrows(), 1):\n",
                "            display(Markdown(f\"**Result {i} (Similarity: {row['similarities']:.4f}):**\\n\\n{row['combined']}\\n\"))\n",
                "\n",
                "    # Return the DataFrame with the added similarity values\n",
                "    return top_three\n",
                "\n",
                "\n",
                "# Call the search_text() function and store the return value in a variable\n",
                "results_df = search_text(df, question, n=3)\n",
                "\n",
                "# Reset the index and create a new column \"index\"\n",
                "results_df = results_df.reset_index()\n",
                "\n",
                "# Access the values in the \"similarities\" and \"combined\" columns\n",
                "similarity1 = results_df.iloc[0][\"similarities\"]\n",
                "combined1 = str(results_df.iloc[0][\"combined\"])\n",
                "\n",
                "similarity2 = results_df.iloc[1][\"similarities\"]\n",
                "combined2 = str(results_df.iloc[1][\"combined\"])\n",
                "\n",
                "similarity3 = results_df.iloc[2][\"similarities\"]\n",
                "combined3 = str(results_df.iloc[2][\"combined\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "260567",
            "metadata": {},
            "source": [
                "To enable semantic search in More's text, it must be converted to text embeddings, an approach for transforming \"unstructured text data into a structured form.\" ((\"Text Embeddings Visually Explained\" 2022)). Preparing a text for semantic search depends on the intended use case and requires consideration of the model's context length. In this example, the text was broken down to the paragraph level and accompanied by a summary initially generated by ChatGPT (GPT-3.5) and edited by the author afterward. OpenAI's Ada model was then used to compute a set of searchable embeddings. Both the original data file and computed embeddings are included in the Github repo for this article. See the OpenAI Cookbook for step-by-step code examples of how to generate embeddings for texts. ((OpenAI Cookbook 2023)) A variety of other semantic search platforms are available, such as Pinecone, Weaviate, and Haystack.\n",
                "\n",
                "This student edition of More's *History of Richard III* is produced by the Thomas More Society. (<cite id=\"cite2c-27937/IDRV8C65\"><a href=\"#zotero%7C27937%2FIDRV8C65\"></a></cite>) My thanks to Dr. Ian Crowe, director of the Thomas More Program at Belmont Abbey College, for the opporunity to explore this text with his students in spring 2023 with this research approach."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45dbca",
            "metadata": {},
            "source": [
                "The results from the semantic search provide the three sections of the text with the highest semantic similarity score. However, high semantic similarity does not always indicate relevance, and such searches can return false positives. Filtering these false positives is essential if you wish to attempt large-scale analysis of an entire text. We can use an LLM to determine the relevance of each text section and then filter out irrelevant sections before answering the question.\n",
                "\n",
                "In making multiple queries to the LLM for a single task, we will use a technique known as prompt chaining. Prompt chains break down complex tasks into smaller, more manageable components by making a series of queries to the LLM. We'll use the langchain library for creating these chains. (<cite id=\"cite2c-27937/IJ5AI567\"><a href=\"#zotero%7C27937%2FIJ5AI567\"></a></cite>)\n",
                "\n",
                "The first link in the chain is a relevance check. In this sequence, GPT-4 will be prompted with a detailed set of instructions, along with three examples of how to determine relevance. Then each text section identified in the semantic search will be passed to the LLM. GPT-4 will use the prompt to generate an analysis of each section's relevance."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "009d73",
            "metadata": {},
            "source": [
                "langchain is a Python library for large language model programming. It is supported by a highly active open source community, and supported by extensive documentation. <cite id=\"cite2c-27937/YAX4VVR9\"><a href=\"#zotero%7C27937%2FYAX4VVR9\"></a></cite>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "ee3a03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code for Text Relevance Prompt using langchain\n",
                "\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.prompts import FewShotPromptTemplate\n",
                "\n",
                "# Few-shot examples to enable in-context learning for GPT-4.\n",
                "\n",
                "burial_question = \"2. Section: Summary: Section_160:  Sir James had the murderers bury King Edward V and Prince Richard's bodies deep in the ground under a heap of stones. Text: Section_160: Which after that the wretches perceived, first by the struggling with the pains of death, and after long lying still, to be thoroughly dead, they laid their bodies naked out upon the bed, and fetched Sir James to see them. Who, upon the sight of them, caused those murderers to bury them at the stair-foot, suitably deep in the ground, under a great heap of stones..\\n3. SSS: 0.896\\n4.Key Words: Edward V, Prince Richard, bodies, bury, Sir James\\n5. Background knowledge and context: Edward V was one of the sons of King Edward IV and Prince Richard was his brother. Sir James was involved in their deaths and had their bodies buried.\\n6.Relevance Determination: Medium\\n7. Relevance Explanation: The key words 'Edward V' and 'Prince Richard' are related to the question as they are mentioned in the same sentence as 'bury'. However, the question specifically asks about the burial of Edward IV, not Edward V and Prince Richard.\\n8Final Output: Section_160: Irrelevant.\\nExcellent. Let's try another.\",\n",
                "cecily_question = \"2. Section: Summary: Section_113:  In a sermon at Paul's Cross, it was revealed to the people that King Edward IV's marriage was not lawful, and that his children were bastards. Text: Section_113: Now then as I began to show you, it was by the Protector and his council concluded that this Doctor Shaa should in a sermon at Paul's Cross signify to the people that neither King Edward himself nor the Duke of Clarence were lawfully begotten, nor were the very children of the Duke of York, but gotten unlawfully by other persons by the adultery of the Duchess, their mother, and that also Dame Elizabeth Lucy was verily the wife of King Edward, and so the Prince and all his children were bastards that were gotten upon the Queen.\\n3. SSS: 0.869\\nBased on the provided information, it appears that the section is potentially relevant to the question. The semantic similarity score is relatively high, indicating that there may be some connection between the section and the question. However, it is important to carefully examine the section and the question to determine the specific relevance.\\n4.Key Words: The key words in the section that may be specifically and directly related to the question are 'King Edward,' 'Duke of Clarence,' 'Duke of York,' 'Elizabeth Lucy,' 'Prince,' and 'children.' These words refer to individuals or groups of people mentioned in the section.\\n5. Background knowledge and context: Knowing that the question is asking about a person named Cecily, we can use our background knowledge about the context of the text to further assess the relevance of the section. The section mentions several individuals and groups of people, including King Edward, the Duke of Clarence, the Duke of York, Elizabeth Lucy, the Prince, and the children. Cecily is not mentioned by name in the section.\\n6.Relevance Determination: Based on the key words identified in the section and our background knowledge of the context, it is unlikely that the section is relevant to the question. The section does not mention the name Cecily and does not provide any information about her. Therefore, I have a low degree of confidence in determining that the section is relevant to the question.\\n7.Relevance Explanation: The section is not relevant to the question because it does not mention the name Cecily and does not provide any information about her.\\n8.Final Output: Section_113: Irrelevant.\\nExcellent. Let's try another.\",\n",
                "edward_question = \"2. Section: Summary: Section_3:  King Edward IV was a good-looking and strong man who was wise in counsel and just in war. He was also known for his love of women and good food. However, he was also known to be a fair and merciful man, and he was greatly loved by his people. Text: Section_3: He was a goodly personage, and very princely to behold: of heart, courageous; politic in counsel; in adversity nothing abashed; in prosperity, rather joyful than proud; in peace, just and merciful; in war, sharp and fierce; in the field, bold and hardy, and nevertheless, no further than wisdom would, adventurous. Whose wars whosoever would well consider, he shall no less commend his wisdom when he withdrew than his manhood when he vanquished. He was of visage lovely, of body mighty, strong, and clean made; however, in his latter days with over-liberal diet , he became somewhat corpulent and burly, and nonetheless not uncomely; he was of youth greatly given to fleshly wantonness, from which health of body in great prosperity and fortune, without a special grace, hardly refrains. This fault not greatly grieved the people, for one man's pleasure could not stretch and extend to the displeasure of very many, and the fault was without violence, and besides that, in his latter days, it lessened and well left.\\n3. SSS: 0.928\\nTo determine whether this section is relevant to the question, let's follow the steps of the Method:1.Question: The user's question is \u2018What was King Edward IV's appearance?\u2019\\n2.Section: The given section is about King Edward IV's appearance, character, and behavior.\\n3. SSS: The semantic similarity score (SSS) is 0.928, which is above the threshold of .90 and indicates that there is some potential relevance between the section and the question.\\n4. Key Words: Key words in the section that are directly and specifically related to the question include \u2018goodly personage,\u2019 \u2018visage lovely,\u2019 \u2018body mighty, strong, and clean made,\u2019 and \u2018somewhat corpulent and burly.\u2019 These words directly describe King Edward IV's appearance.\\n5. Background Knowledge: Based on my background knowledge of the subject matter, I can confirm that this section is directly and specifically relevant to answering the question about King Edward IV's appearance.\\n6. Relevance Determination: The relevance determination is high, as the section is directly and specifically related to the question.\\n7. Relevance Explanation: The relevance explanation is that the section contains detailed descriptions of King Edward IV's appearance, including his physical appearance and any changes to it over time.\\n8. Final Output: Therefore, the final output is \u2018Section_3: Relevant.\u2019\\nExcellent. Let's try another.\"\n",
                "\n",
                "# Formatting the examples to pass on the LLM.\n",
                "examples = [\n",
                "    {\"question\": \"1. Question: Where was Edward IV buried?\", \"output\": burial_question},\n",
                "    {\"question\": \"1. Question: What was Edward IV's appearence?\", \"output\": edward_question},\n",
                "    {\"question\": \"1. Question: Who is Cecily?\", \"output\": cecily_question}\n",
                "],\n",
                "\n",
                "example_prompt = PromptTemplate(\n",
                "    input_variables=[\"question\"],\n",
                "    template=\"question: {question}\",\n",
                ")\n",
                "\n",
                "relevance_prompt = FewShotPromptTemplate(\n",
                "    # These are the examples we want to insert into the prompt.\n",
                "    examples=examples,\n",
                "    # This is how we want to format the examples when we insert them into the prompt.\n",
                "    example_prompt=example_prompt,\n",
                "    # The prefix is some text that goes before the examples in the prompt.\n",
                "    # Usually, this consists of intructions.\n",
                "    prefix=\"You are an AI expert on the 'History of Richard III' by Thomas More. In this exercise you are given a user supplied question, a Section of the Text, a Semantic Similarity Score, and a Method for determining the Section\u2019s relevance to the Question. Your objective is to determine whether that Section of the text is directly and specifically relevant to the user question. You will be the Method below to fulfill this objective, taking each step by step.\\n\\nHere is your Method.\\n\\nMethod: Go step by step in answering the question.\\n1. Question: You will be provided with a user question.\\n2. Section: You will be given a section of the text from Thomas More's 'The History of Richard III.' \\n3. Semantic Similarity Score: You are then given a semantic similarity score, which ranges from 1.0 (highest) to 0.0 (lowest). The higher the score, the more likely its potential relevance. Scores approaching .90 and above are generally considered to have some relevance. However, this score isn\u2019t fully determinative, as other semantically related words in the Section can generate false positives. Weigh the value of this score alongside a careful examination of the Question and the Section.\\n4. Key Words: Identify key words in the Section that are specifically and directly related to the Question. Such key words could include specific locations, events, or people mentioned in the Section.\\n5. Background knowledge and context: Use your background knowledge of the subject matter to further elaborate on whether the Section is directly and specifically relevant to answering the Question.\\n6. Relevance Determination: Based on your review of the earlier steps in the Method, determine whether the section is relevant, and gauge your confidence (high, medium, low, or none)  in this determination. High determination is specifically and directly related to the Question. If the section is relevant and ranked high, write \u2018'Section_x: Relevant'. Otherwise, if the section is not relevant and the determination is less than high, write 'Section_x: Irrelevant'.\\n7. Relevance Explanation: Based on your review in the earlier steps in the Method, explain why the Section\u2019s relevance to the Question.\\nLet\u2019s begin.\",\n",
                "    # The suffix is some text that goes after the examples in the prompt.\n",
                "    # Usually, this is where the user input will go\n",
                "    suffix=\"Question: {question}\\nKey Terms:\",\n",
                "    # The input variables are the variables that the overall prompt expects.\n",
                "    input_variables=[\"question\"],\n",
                "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
                "    example_separator=\"\\n\\n\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ebbb3e",
            "metadata": {},
            "source": [
                "This prompt design employs few-shot learning and chain-of-thought prompting to guide the language model in determining the relevance of a given text section to a specific question. Let's analyze each aspect of the prompt design and its impact on the model's response.\n",
                "\n",
                "Chain-of-thought prompting: The prompt design incorporates a set of instructions for step-by-step completion of the task. This method guides the model through the process of analyzing the text section and evaluating its relevance to the deaths of the Princes.\n",
                "\n",
                "Few-shot learning: As in the previous example, a set of examples are offered to help guide the model's response. Here various sections of the text are compared with a user question for relevance. A scripted sequence is then provided, following the chain-of-thought instructions. Both irrevelant and revelant examples are used. \n",
                "\n",
                "Prompt 2: Textual Relevance\n",
                "\n",
                "You are an AI expert on the \u2018History of Richard III\u2019 by Thomas More. In this exercise you are given a user supplied question, a Section of the Text, a Semantic Similarity Score, and a Method for determining the Section\u2019s relevance to the Question. Your objective is to determine whether that Section of the text is directly and specifically relevant to the user question. You will be the Method below to fulfill this objective, taking each step by step.\n",
                "\n",
                "Here is your Method.\n",
                "\n",
                "Method: Go step by step in answering the question.\n",
                "\n",
                "1. Question: You will be provided with a user question.\n",
                "\n",
                "2. Section: You will be given a section of the text from Thomas More\u2019s \u2018The History of Richard III.\u2019\n",
                "\n",
                "3. Semantic Similarity Score: You are then given a semantic similarity score, which ranges from 1.0 (highest) to 0.0 (lowest). The higher the score, the more likely its potential relevance. Scores approaching .90 and above are generally considered to have some relevance. However, this score isn\u2019t fully determinative, as other semantically related words in the Section can generate false positives. Weigh the value of this score alongside a careful examination of the Question and the Section.\n",
                "\n",
                "4. Key Words: Identify key words in the Section that are specifically and directly related to the Question. Such key words could include specific locations, events, or people mentioned in the Section.\n",
                "\n",
                "5. Background knowledge and context: Use your background knowledge of the subject matter to further elaborate on whether the Section is directly and specifically relevant to answering the Question.\n",
                "\n",
                "6. Relevance Determination: Based on your review of the earlier steps in the Method, determine whether the section is relevant, and gauge your confidence (high, medium, low, or none) in this determination. High determination is specifically and directly related to the Question. If the section is relevant and ranked high, write \u2018\u2018Section_x: Relevant\u2019. Otherwise, if the section is not relevant and the determination is less than high, write \u2018Section_x: Irrelevant\u2019.\n",
                "\n",
                "7. Relevance Explanation: Based on your review in the earlier steps in the Method, explain why the Section\u2019s relevance to the Question. Let\u2019s begin.\n",
                "\n",
                "Prompt 2: Examples for in-context learning.\n",
                "\n",
                "Example 1: \n",
                "\n",
                "1. Question: Who is Cecily?\n",
                "\n",
                "2. Section: \n",
                "\n",
                "Summary: Section_113: In a sermon at Paul's Cross, it was revealed to the people that King Edward IV's marriage was not lawful, and that his children were bastards. \n",
                "\n",
                "Text: Section_113: Now then as I began to show you, it was by the Protector and his council concluded that this Doctor Shaa should in a sermon at Paul's Cross signify to the people that neither King Edward himself nor the Duke of Clarence were lawfully begotten, nor were the very children of the Duke of York, but gotten unlawfully by other persons by the adultery of the Duchess, their mother, and that also Dame Elizabeth Lucy was verily the wife of King Edward, and so the Prince and all his children were bastards that were gotten upon the Queen. \n",
                "\n",
                "3. SSS: 0.869 Based on the provided information, it appears that the section is potentially relevant to the question. The semantic similarity score is relatively high, indicating that there may be some connection between the section and the question. However, it is important to carefully examine the section and the question to determine the specific relevance. \n",
                "\n",
                "4. Key Words: The key words in the section that may be specifically and directly related to the question are 'King Edward,' 'Duke of Clarence,' 'Duke of York,' 'Elizabeth Lucy,' 'Prince,' and 'children.' These words refer to individuals or groups of people mentioned in the section. \n",
                "\n",
                "5. Background knowledge and context: Knowing that the question is asking about a person named Cecily, we can use our background knowledge about the context of the text to further assess the relevance of the section. The section mentions several individuals and groups of people, including King Edward, the Duke of Clarence, the Duke of York, Elizabeth Lucy, the Prince, and the children. Cecily is not mentioned by name in the section. \n",
                "\n",
                "6. Relevance Determination: Based on the key words identified in the section and our background knowledge of the context, it is unlikely that the section is relevant to the question. The section does not mention the name Cecily and does not provide any information about her. Therefore, I have a low degree of confidence in determining that the section is relevant to the question. \n",
                "\n",
                "7. Relevance Explanation: The section is not relevant to the question because it does not mention the name Cecily and does not provide any information about her. \n",
                "\n",
                "8. Final Output: Section_113: Irrelevant. \n",
                "\n",
                "Excellent. Let's try another.\n",
                "\n",
                "\n",
                "Example 2:\n",
                "\n",
                "Edward Question: \"What was King Edward IV's appearance?\"\n",
                "\n",
                "Section 3: Summary\n",
                "\n",
                "King Edward IV was a good-looking and strong man who was wise in counsel and just in war. He was also known for his love of women and good food. However, he was also known to be a fair and merciful man, and he was greatly loved by his people.\n",
                "\n",
                "Section 3: Text\n",
                "\n",
                "King Edward IV was described as a goodly personage and very princely to behold. He was courageous at heart, politically astute, unshaken in adversity, and joyful in prosperity without being overly proud. In peace, he was just and merciful, while in war, he was sharp and fierce. He was bold and hardy in the field, but he didn't take unnecessary risks. Observers of his wars would commend his wisdom when he withdrew, just as they would praise his manhood when he emerged victorious.\n",
                "\n",
                "Edward had a lovely visage, and his body was mighty, strong, and well-built. However, in his later years, he became somewhat corpulent and burly due to overindulgence in food, but he still maintained an attractive appearance. In his youth, he was greatly given to fleshly wantonness, a fault that the people did not hold against him since it didn't involve violence and affected only a few. Furthermore, this fault diminished and was eventually abandoned in his later years.\n",
                "\n",
                "Semantic Similarity Score (SSS): 0.928\n",
                "\n",
                "To determine the relevance of this section to the question, we can follow the steps of the Method:\n",
                "\n",
                "1. Question: The user's question is \"What was King Edward IV's appearance?\"\n",
                "2. Section: The given section is about King Edward IV's appearance, character, and behavior.\n",
                "3. SSS: The semantic similarity score (SSS) is 0.928, which is above the threshold of 0.90, indicating potential relevance between the section and the question.\n",
                "4. Key Words: Key words in the section that are directly and specifically related to the question include \"goodly personage,\" \"visage lovely,\" \"body mighty, strong, and clean made,\" and \"somewhat corpulent and burly.\" These words directly describe King Edward IV's appearance.\n",
                "5. Background Knowledge: Based on background knowledge of the subject matter, it can be confirmed that this section is directly and specifically relevant to answering the question about King Edward IV's appearance.\n",
                "6. Relevance Determination: The relevance determination is high, as the section is directly and specifically related to the question.\n",
                "7. Relevance Explanation: The relevance explanation is that the section contains detailed descriptions of King Edward IV's appearance, including his physical appearance and any changes to it over time.\n",
                "\n",
                "\n",
                "Final Output: Therefore, the final output is \"Section_3: Relevant.\"\n",
                "\n",
                "\n",
                "Excellent. Let's try another."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "0298a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code for using Text Relevance Prompt with GPT-4 via the langchain library.\n",
                "\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "from langchain import PromptTemplate, LLMChain\n",
                "from langchain.prompts.chat import (\n",
                "    ChatPromptTemplate,\n",
                "    SystemMessagePromptTemplate,\n",
                "    AIMessagePromptTemplate,\n",
                "    HumanMessagePromptTemplate,\n",
                ")\n",
                "from langchain.schema import (\n",
                "    AIMessage,\n",
                "    HumanMessage,\n",
                "    SystemMessage\n",
                ")\n",
                "\n",
                "system_prompt = SystemMessagePromptTemplate(prompt=relevance_prompt)\n",
                "\n",
                "human_message_prompt_template = \"Question: {question}\\nKey Terms:\"\n",
                "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message_prompt_template)\n",
                "\n",
                "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_message_prompt])\n",
                "\n",
                "chat = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
                "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
                "\n",
                "r_check_1 = chain.run(question=str(question + \"\\n2. Section:\\n \" + combined1 + \"\\n3. SSS: \" + str(similarity1)))\n",
                "#print(\"Relevance Check 1: \\n\\n\" + \"4. Key Terms: \\n\" + r_check_1 + \"\\n\")\n",
                "\n",
                "r_check_2 = chain.run(question=str(question + \"\\n2. Section:\\n \" + combined2 + \"\\n3. SSS: \" + str(similarity2)))\n",
                "#print(\"Relevance Check 2: \\n\\n\" +  \"4. Key Terms: \\n\" + r_check_2 + \"\\n\")\n",
                "\n",
                "r_check_3 = chain.run(question=str(question + \"\\n2. Section:\\n \" + combined3 + \"\\n3. SSS: \" + str(similarity3)))\n",
                "#print(\"Relevance Check 3: \\n\\n\" +  \"4. Key Terms: \\n\" + r_check_3 + \"\\n\")\n",
                "\n",
                "display(Markdown(\"GPT-4's Determination of Relevance Starts at Step 4:\\n\\nRelevance Check 1: \\n\\n\" + combined1 + \"\\n\\n4. Key Terms: \\n\" + r_check_1 + \"\\n\\n\" + \"Relevance Check 2: \\n\\n\" +  combined2 + \"\\n\\n4. Key Terms: \\n\" + r_check_2 + \"\\n\\n\" + \"Relevance Check 3: \\n\\n\" +  combined3 + \"\\n\\n4. Key Terms: \\n\" + r_check_3 + \"\\n\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2c7bd",
            "metadata": {},
            "source": [
                "GPT-4's analysis of the three text sections provides determinations of each section's relevance to the original question of \"who killed the Princes in the Tower.\" The model also provides the basis behind its determination, facilitating not just the ability to examine its analysis, but offering further data for the next link in the prompt chain.\n",
                "\n",
                "In our next step, we'll filter out irrelevant sections and pass on the remaining texts to the model to answer the initial question. We'll then prompt GPT-4 to identify a supporting quotation from each text to support its answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "c7dba5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is example code in this notebook demonstrating a script for using regular expressions to filter out \n",
                "# irrevelant text sections for the next part of the prompt chain. For this particular example, all three texts are relevant.\n",
                "# Code designed with the assistiance of GPT-3.\n",
                "\n",
                "import pandas as pd\n",
                "import re\n",
                "\n",
                "# combined function for combining sections + outputs, and then filtering via regex for relevant sections\n",
                "\n",
                "combined_df = pd.DataFrame(columns=['output', 'r_check'])\n",
                "combined_df['output'] = [combined1, combined2, combined3]\n",
                "combined_df['r_check'] = [r_check_1, r_check_2, r_check_3]\n",
                "\n",
                "# Use the re.IGNORECASE flag to make the regular expression case-insensitive\n",
                "regex = re.compile(r'(section_\\d+:\\srelevant)', re.IGNORECASE)\n",
                "\n",
                "# Apply the regex pattern to the 'r_check' column and store the results in a new 'mask' column\n",
                "combined_df['mask'] = combined_df['r_check'].str.extract(regex).get(0).notnull()\n",
                "\n",
                "# Create a second mask to capture \"this is relevant\"\n",
                "combined_df['second_mask'] = combined_df['r_check'].str.contains(r'this section is relevant', flags=re.IGNORECASE)\n",
                "\n",
                "# Combine the two masks using the bitwise OR operator (|) and store the result in the 'mask' column\n",
                "combined_df['mask'] = combined_df['mask'] | combined_df['second_mask']\n",
                "\n",
                "# Filter the combined dataframe to include only rows where the 'mask' column is True\n",
                "relevant_df = combined_df.loc[combined_df['mask']].copy()\n",
                "\n",
                "# Check if there are any rows in the relevant_df dataframe\n",
                "if relevant_df.empty:\n",
                "    # If there are no rows, print the desired message\n",
                "    print(\"No relevant sections identified.\")\n",
                "else:\n",
                "    # Otherwise, continue with the rest of the script\n",
                "\n",
                "    def combine_strings(row):\n",
                "        return row['output'] + '\\nKey Terms\\n' + row['r_check']\n",
                "\n",
                "    # Use the apply function to apply the combine_strings function to each row of the relevant_df dataframe\n",
                "    # and assign the result to the 'combined_string' column\n",
                "    relevant_df['combined_string'] = relevant_df.apply(combine_strings, axis=1)\n",
                "\n",
                "    final_sections = relevant_df['combined_string']\n",
                "    #final_sections.to_csv('final_sections.csv')\n",
                "\n",
                "    evidence_df = pd.DataFrame(final_sections)\n",
                "\n",
                "    evidence = '\\n\\n'.join(evidence_df['combined_string'])      \n",
                "    \n",
                "    # Filter the relevant_df dataframe to include only the 'output' column\n",
                "    output_df = relevant_df[['output']]\n",
                "\n",
                "    # Convert the dataframe to a dictionary\n",
                "    output_dict = output_df.to_dict('records')\n",
                "\n",
                "    # Extract the values from the dictionary using a list comprehension\n",
                "    output_values = [d['output'] for d in output_dict]\n",
                "\n",
                "    # Print the output values to see the results\n",
                "    #print(output_values)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "fa1d8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompt for GPT-4 to identify quotes from the texts to support its answer. \n",
                "\n",
                "windsor_analysis = \"2. Summary: Section_1:  King Edward IV was a beloved king who was interred at Windsor with great honor. He was especially beloved by the people at the time of his death. Text: Section_1: This noble prince died at his palace of Westminster and, with great funeral honor and heaviness of his people from thence conveyed, was interred at Windsor. He was a king of such governance and behavior in time of peace (for in war each part must needs be another's enemy) that there was never any prince of this land attaining the crown by battle so heartily beloved by the substance of the people, nor he himself so specially in any part of his life as at the time of his death.\\n3.Initial Answer: King Edward IV was buried at Windsor with great honor and mourning from his people.\\n4.Supporting Quote: \u2018This noble prince died at his palace of Westminster and, with great funeral honor and heaviness of his people from thence conveyed, was interred at Windsor.\u2019 (S.1)\\n5. Combined Answer: King Edward IV was interred at Windsor with great honor and mourned by his people: \u2018This noble prince...was interred at Windsor...and at the time of his death there was never any prince of this land attaining the crown by battle so heartily beloved by the substance of the people.\u2019 (S.1)\\nExcellent. Let\u2019s try another.\",\n",
                "wales_analysis = \"2. Summary: Section_17:  After King Edward IV's death, his son Prince Edward moved towards London. He was accompanied by Sir Anthony Woodville, Lord Rivers, and other members of the queen's family. Text: Section_17: As soon as the King was departed, that noble Prince his son drew toward London, who at the time of his father's death kept household at Ludlow in Wales.  Such country, being far off from the law and recourse to justice, was begun to be far out of good will and had grown up wild with robbers and thieves walking at liberty uncorrected. And for this reason the Prince was, in the life of his father, sent thither, to the end that the authority of his presence should restrain evilly disposed persons from the boldness of their former outrages.  To the governance and ordering of this young Prince, at his sending thither, was there appointed Sir Anthony Woodville, Lord Rivers and brother unto the Queen, a right honorable man, as valiant of hand as politic in counsel. Adjoined were there unto him others of the same party, and, in effect, every one as he was nearest of kin unto the Queen was so planted next about the Prince.\\n3. Initial Answer: Wales is mentioned in the text as the place where Prince Edward kept household at the time of his father's death and where he was sent to maintain order and restrain criminal activity.\\n4. Supporting Quote: 'That noble Prince his son drew toward London, who at the time of his father's death kept household at Ludlow in Wales\u2026That the authority of his presence should restrain evilly disposed persons from the boldness of their former outrages.' (S.17)\\n5. Combined Answer: Wales is mentioned in the text as the place where Prince Edward kept household and was sent to maintain order and prevent crime: 'That noble Prince his son drew toward London, who at the time of his father's death kept household at Ludlow in Wales...That the authority of his presence should restrain evilly disposed persons from the boldness of their former outrages.' (S.17)\",\n",
                "edward_analysis = \"2. Summary: Section_2:  The people's love for King Edward IV increased after his death, as many of those who bore him grudge for deposing King Henry VI were either dead or had grown into his favor. Text: Section_2: Even after his death, this favor and affection toward him because of the cruelty, mischief, and trouble of the tempestuous world that followed afterwards increased more highly. At such time as he died, the displeasure of those that bore him grudge for King Henry's sake, the Sixth, whom he deposed, was well assuaged, and in effect quenched, in that many of them were dead in the more than twenty years of his reign a great part of a long life. And many of them in the meantime had grown into his favor, of which he was never sparing.\\nInitial Answer: The public regarded Edward IV highly, with their love for him increasing after his death as many of those who bore him grudge for deposing Henry VI either died or grew into his favor.\\nSupporting Quote: 'Even after his death, this favor and affection toward him because of the cruelty, mischief, and trouble of the tempestuous world that followed afterwards increased more highly...At such time as he died, the displeasure of those that bore him grudge for King Henry's sake, the Sixth, whom he deposed, was well assuaged, and in effect quenched, in that many of them were dead in the more than twenty years of his reign a great part of a long life. And many of them in the meantime had grown into his favor, of which he was never sparing.' (S.2)\\nCombined Answer: The public regarded Edward IV highly at the time of his death, with their love for him increasing over time. 'Even after his death, this favor and affection toward him because of the cruelty, mischief, and trouble of the tempestuous world that followed afterwards increased more highly.' (S.2)\\n. Excellent. Let\u2019s try another.\"\n",
                "\n",
                "examples = [\n",
                "    {\"question\": \"Question: Where was Edward IV buried?\", \"output\": windsor_analysis},\n",
                "    {\"question\": \"Question: Is Wales mentioned in the text?\", \"output\": wales_analysis},\n",
                "    {\"question\": \"Question: How did the public regard Edward IV?\", \"output\": edward_analysis}\n",
                "],\n",
                "\n",
                "# This how we specify how the example should be formatted.\n",
                "example_prompt = PromptTemplate(\n",
                "    input_variables=[\"question\"],\n",
                "    template=\"question: {question}\",\n",
                ")\n",
                "\n",
                "quotation_extraction_prompt = \"You are an AI question-answerer and quotation-selector. The focus of your expertise is interpreting \u201cThe History of Richard III\u201d by Thomas More. In this exercise you will first be given a user question, a Section of More\u2019s text, and a Method for answering the question and supporting it with an appropriate quotation from the Section. In following this Method you will complete each step by step until finished.\\n\\nHere is your Method.\\nMethod: Go step by step in the question.\\n1. Question: You will be provided with a user question.\\n2. Section: You will be given a section from Thomas More's 'The History of Richard III.'\\n3. Compose Initial Answer: Based on the Question and information provided in the Section, compose a historically accurate Initial Answer to that Question. The Initial Answer should be incisive, brief, and well-written.\\n4. Identify Supporting Quote: Based on the Answer, select a Quote from the Section that supports that Answer. Be sure to only select Quotes from the \u201cText:Section_number\u201d part of the Section. Select the briefest and most relevant Quote possible. You can also use paraphrasing to further shorten the Quote. Cite the Section the Quote came from, in the following manner: (S.1) for quotes from Section_1.\\n5. Combined Answer with Supporting Quote: Rewrite the Initial Answer to incorporate the Quote you\u2019ve identified from the \u201cText:Section_number\u201d part of the Section. This Combined Answer should be historically accurate, and be incisive, brief, and well-written. All Quotes used should be cited using the method above.\\nLet\u2019s begin.\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5640d",
            "metadata": {},
            "source": [
                "This prompt design employs the same structure of few-shot learning and chain-of-thought prompting as in the last example.\n",
                "\n",
                "Prompt 3: Quotation Extraction\n",
                "\n",
                "You are an AI question-answerer and quotation-selector. The focus of your expertise is interpreting \u201cThe History of Richard III\u201d by Thomas More. In this exercise you will first be given a user question, a Section of More\u2019s text, and a Method for answering the question and supporting it with an appropriate quotation from the Section. In following this Method you will complete each step by step until finished.\n",
                "\n",
                "Here is your Method. \n",
                "\n",
                "Method: Go step by step in the question.\n",
                "\n",
                "1. Question: You will be provided with a user question.\n",
                "2. Section: You will be given a section from Thomas More\u2019s \u2018The History of Richard III.\u2019\n",
                "3. Compose Initial Answer: Based on the Question and information provided in the Section, compose a historically accurate 4. Initial Answer to that Question. The Initial Answer should be incisive, brief, and well-written.\n",
                "5. Identify Supporting Quote: Based on the Answer, select a Quote from the Section that supports that Answer. Be sure to only select Quotes from the \u201cText:Section_number\u201d part of the Section. Select the briefest and most relevant Quote possible. You can also use paraphrasing to further shorten the Quote. Cite the Section the Quote came from, in the following manner: (S.1) for quotes from Section_1.\n",
                "6. Combined Answer with Supporting Quote: Rewrite the Initial Answer to incorporate the Quote you\u2019ve identified from the \u201cText:Section_number\u201d part of the Section. This Combined Answer should be historically accurate, and be incisive, brief, and well-written. All Quotes used should be cited using the method above. Let\u2019s begin.\n",
                "\n",
                "Example Prompt for in-context learning:\n",
                "\n",
                "Question: Where was Edward IV buried?\n",
                "\n",
                "Summary: Section_1: King Edward IV was a beloved king who was interred at Windsor with great honor. He was especially beloved by the people at the time of his death.\n",
                "\n",
                "Text: Section_1: This noble prince died at his palace of Westminster and, with great funeral honor and heaviness of his people from thence conveyed, was interred at Windsor. He was a king of such governance and behavior in time of peace (for in war each part must needs be another\u2019s enemy) that there was never any prince of this land attaining the crown by battle so heartily beloved by the substance of the people, nor he himself so specially in any part of his life as at the time of his death.\n",
                "\n",
                "Initial Answer: King Edward IV was buried at Windsor with great honor and mourning from his people.\n",
                "\n",
                "Supporting Quote: \u2018This noble prince died at his palace of Westminster and, with great funeral honor and heaviness of his people from thence conveyed, was interred at Windsor.\u2019 (S.1)\n",
                "\n",
                "Combined Answer: King Edward IV was interred at Windsor with great honor and mourned by his people: \u2018This noble prince\u2026was interred at Windsor\u2026and at the time of his death there was never any prince of this land attaining the crown by battle so heartily beloved by the substance of the people.\u2019 (S.1)]\n",
                "\n",
                "Excellent. Let\u2019s try another."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "3456ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code for calling GPT-4 with the Quote Extraction prompt for the relevant text sections.\n",
                "\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "example_prompt = SystemMessagePromptTemplate.from_template(quotation_extraction_prompt)\n",
                "\n",
                "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Question: {question}\\nKey Terms:\")\n",
                "\n",
                "chat_prompt = ChatPromptTemplate.from_messages([example_prompt, human_message_prompt])\n",
                "\n",
                "chat = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
                "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
                "\n",
                "# Create an empty list to store the final_analysis results\n",
                "final_analysis_results = []\n",
                "\n",
                "# Iterate over the output_values list\n",
                "for output_value in output_values:\n",
                "    # Run the final_analysis step and store the result in a variable\n",
                "    final_analysis = chain.run(question+output_value)\n",
                "    # Add the final_analysis result to the list\n",
                "    final_analysis_results.append(final_analysis)\n",
                "\n",
                "# Create a Pandas dataframe from the output_values list\n",
                "final_analysis_df = pd.DataFrame({'output_values': output_values, 'final_analysis': final_analysis_results})\n",
                "\n",
                "\n",
                "display(Markdown(f\"**Analysis 1:**\\n\\n\" + final_analysis_df['final_analysis'][0] + \"\\n\\n\"))\n",
                "display(Markdown(f\"**Analysis 2:**\\n\\n\" + final_analysis_df['final_analysis'][1] + \"\\n\\n\"))\n",
                "display(Markdown(f\"**Analysis 3:**\\n\\n\" + final_analysis_df['final_analysis'][2] + \"\\n\\n\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ec6e06",
            "metadata": {},
            "source": [
                "Using semantic search and prompt chaining this case demonstrates how to answer the user's questions about a longform text with direct quotations and a direct citation. There is also a log of the model's \"reasoning\" to better help human review of LLM hallucinations.  \n",
                "\n",
                "From here, additional links of the prompt chain could be added for customized analytical purposes. Each text section could be combined into a single analysis, providing the reader with a narrative response grounded in multiple sections of the text. GPT-4 could also be tasked with a range of other inquires: contextualizing these events within the broader events of the War of the Roses, extracting character interactions for graphing network analysis, or extracting geocoding data for digital mapping. Indeed, given GPT-4's remarkable capacities perhaps the major limiting factor in using this technology is the researcher's imagination and research budget.\n",
                "\n",
                "The possibilities for querying a historical source with customized analytical approaches are compelling. So too is the potential to scale this approach. Scholars have employed similar techniques that enable natural language queries over their Zotero research collections. (<cite id=\"cite2c-27937/UJDUIP4T\"><a href=\"#zotero%7C27937%2FUJDUIP4T\"></a></cite>) (<cite id=\"cite2c-27937/G7XTPF45\"><a href=\"#zotero%7C27937%2FG7XTPF45\"></a></cite>) Archival collections and other digitized text corpora could be searched in a similar manner. This capacity to \"ask a source\" could expand accessibility, accelerate research, and enable new forms of interpretation of the past."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83c6f8",
            "metadata": {},
            "source": [
                "## What Do AIs \u201cKnow\u201d About History? Assessing GPT-3\u2019s Historical Capacities"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a01bbd",
            "metadata": {},
            "source": [
                "The above case studies demonstrate the general versatility of LLMs on a range of technical and analytical tasks. Of particular interest to historians are empirical studies documenting generative AI's capacities for historical interpretation. \n",
                "\n",
                "Machine learning researchers have devised a series of benchmarks for measuring the capacities of LLMs on various forms of academic knowledge. One recently established benchmark introduces a standard for measuring LLM's performance on the Advanced Placement (A.P.) curricula for U.S., European, and World history. Hundreds of thousands of secondary students across the globe annually enroll in these curricula, which are designed to replicate the rigors of an introductory university-level history course.\n",
                "\n",
                "In January 2021, a team of ML researchers led by Dan Hendryks tested GPT-3 on hundreds of multiple-choice questions from the A.P. History curricula, along with fifty-seven other academic disciplines. Twenty-five percent accuracy represented random chance; eighty percent reflected expert-level accuracy. GPT-3 initially achieved over 50% accuracy on all three A.P. curricula. GPT-3's performance in these subfields numbered among the top third of all the academic disciplines included in the study, although in no field did GPT-3 achieve expert-level accuracy. While demonstrating strengths in some areas, GPT-3 nonetheless possessed worrying blind spots, such as particularly poor performance in the fields of \"Moral Questions\" and \"Professional Law.\" As the authors note, this \"weakness is particularly concerning because it will be important for future models to have a strong understanding of what is legal and what is ethical.\" (<cite id=\"cite2c-27937/ZS9JDNGD\"><a href=\"#zotero%7C27937%2FZS9JDNGD\"></a></cite>)\n",
                "\n",
                "Understanding the format of the benchmarks is important in evaluating the performance of LLMs. Below are two examples questions drawn from the U.S. History curriculum, both using the same historical source. The code below display's GPT-3's responses:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc1e2e",
            "metadata": {},
            "source": [
                "Here are the accuracy rates for GPT-3 for the initial Hendryks study: US History, 52.9%; European History, 53.9%; and World History, 56.1%. Full data for questions for history and other disciplines can be found at: (<cite id=\"cite2c-27937/A834FRJL\"><a href=\"#zotero%7C27937%2FA834FRJL\"></a></cite>) Many thanks to Dan Hendrycks for sharing the discipline-specific accuracy rates for these fields.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "af497f",
            "metadata": {},
            "outputs": [],
            "source": [
                "us_history_benchmark_q5 = \"\"\"This question refers to the following information.\\n\\n\\\"I was once a tool of oppression\\n\\nAnd as green as a sucker could be\\n\\nAnd monopolies banded together\\n\\nTo beat a poor hayseed like me.\\n\\n\"The railroads and old party bosses\\n\\nTogether did sweetly agree;\\n\\nAnd they thought there would be little trouble\\n\\nIn working a hayseed like me. . . .\"\\n\\n\u2014\"The Hayseed\"\\n\\nThe song, and the movement that it was connected to, highlight which of the following developments in the broader society in the late 1800s?\\n\\nA: Corruption in government, especially as it related to big business, energized the public to demand increased popular control and reform of local, state, and national governments.\\n\\nB: A large-scale movement of struggling African American and white farmers, as well as urban factory workers, was able to exert a great deal of leverage over federal legislation.\\n\\nC: The two-party system of the era broke down and led to the emergence of an additional major party that was able to win control of Congress within ten years of its founding.\\n\\nD: Continued skirmishes on the frontier in the 1890s with American Indians created a sense of fear and bitterness among western farmers.\"\"\"\n",
                "us_history_benchmark_q22 = \"\"\"This question refers to the following information.\\n\\n\\\"I was once a tool of oppression\\n\\nAnd as green as a sucker could be\\n\\nAnd monopolies banded together\\n\\nTo beat a poor hayseed like me.\\n\\n\"The railroads and old party bosses\\n\\nTogether did sweetly agree;\\n\\nAnd they thought there would be little trouble\\n\\nIn working a hayseed like me. . . .\"\\n\\n\u2014\"The Hayseed\"\\n\\nWhich of the following is an accomplishment of the political movement that was organized around sentiments similar to the one in the song lyrics above?\\n\\nA: Establishment of the minimum wage law.\\n\\nB: Enactment of laws regulating railroads.\\n\\nC: Shift in U.S. currency from the gold standard to the silver standard.\\n\\nD: Creation of a price-support system for small-scale farmers.\"\"\"\n",
                "display(Markdown(\"**U.S. History Benchmarks - Question 5:** \\n\\n\" + us_history_benchmark_q5 + \"\\n\\n\\n**U.S History Benchmarks - Question 22:** \\n\\n\" + us_history_benchmark_q22))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "997697",
            "metadata": {},
            "outputs": [],
            "source": [
                "import openai\n",
                "\n",
                "question_5 = openai.Completion.create(\n",
                "                    model='text-davinci-002',\n",
                "                    prompt=us_history_benchmark_q5,\n",
                "                    temperature=0,\n",
                "                    max_tokens=50)\n",
                "\n",
                "question_22 = openai.Completion.create(\n",
                "                    model='text-davinci-002',\n",
                "                    prompt=us_history_benchmark_q22,\n",
                "                    temperature=0,\n",
                "                    max_tokens=50)\n",
                "\n",
                "display(Markdown(\"**GPT-3's Answer for Question 5:** \" + (question_5.choices[0].text) + \"\\n\\n**Correct Answer**\\n\\n A: Corruption in government, especially as it related to big business, energized the public to demand increased popular control and reform of local, state, and national governments.\\n\\n\" + \"\\n\\n**GPT-3's Answer for Question 22:** \\n\\n\" + (question_22.choices[0].text) + \"\\n\\n**Correct Answer**\\n\\n B: Enactment of laws regulating railroads.\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "22b6c7",
            "metadata": {},
            "source": [
                "Since 2021, the release of new GPT models trained using \"reinforcement learning from human feedback\" (RLHF) has dramatically improved the performance of LLMs on these historical benchmarks, as well as numerous others. (<cite id=\"cite2c-27937/36TWI3H2\"><a href=\"#zotero%7C27937%2F36TWI3H2\"></a></cite>) Below are the results from my replication of the Hendryks study using later models in the GPT series: the GPT-3 Instruct model, ChatGPT (GPT 3.5), and GPT-4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "86af0d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Designed with the help of GPT-4\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "csv_files = [\n",
                "    \"euro_history_benchmark_tests_chatgpt.csv\",\n",
                "    \"euro_history_benchmark_tests_gpt3.csv\",\n",
                "    \"euro_history_benchmark_tests_gpt4.csv\",\n",
                "    \"us_history_benchmark_tests_chatgpt.csv\",\n",
                "    \"us_history_benchmark_tests_gpt3.csv\",\n",
                "    \"us_history_benchmark_tests_gpt4.csv\",\n",
                "    \"world_history_benchmark_tests_chatgpt.csv\",\n",
                "    \"world_history_benchmark_tests_gpt3.csv\",\n",
                "    \"world_history_benchmark_tests_gpt4.csv\",\n",
                "]\n",
                "\n",
                "# Function to calculate accuracy from a CSV file\n",
                "def calculate_accuracy(file_path):\n",
                "    df = pd.read_csv(file_path)\n",
                "    correct_count = df['correct_status'].value_counts().get('correct', 0)\n",
                "    total_count = len(df)\n",
                "    return correct_count / total_count * 100\n",
                "\n",
                "# Calculate accuracies for each file\n",
                "accuracies = {file: calculate_accuracy(file) for file in csv_files}\n",
                "\n",
                "# Function to extract the model and history type from the file path\n",
                "def extract_info(file_path):\n",
                "    file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
                "    history_type, model = file_name.split(\"_benchmark_tests_\")\n",
                "    return history_type, model\n",
                "\n",
                "# Convert the accuracy dictionary to a DataFrame\n",
                "data = []\n",
                "for file, accuracy in accuracies.items():\n",
                "    history_type, model = extract_info(file)\n",
                "    if model == 'chatgpt':\n",
                "        model = 'ChatGPT (GPT-3.5)'\n",
                "    elif model == 'gpt3':\n",
                "        model = 'GPT-3 (Instruct model)'\n",
                "    elif model == 'gpt4':\n",
                "        model = 'GPT-4'\n",
                "    data.append([history_type, model, accuracy])\n",
                "\n",
                "# Add the new data values for \"GPT-3 (Hendryks test)\"\n",
                "hendryks_test_data = [\n",
                "    [\"us_history\", \"GPT-3 (Hendryks test)\", 52.9],\n",
                "    [\"euro_history\", \"GPT-3 (Hendryks test)\", 53.9],\n",
                "    [\"world_history\", \"GPT-3 (Hendryks test)\", 56.1],\n",
                "]\n",
                "for item in hendryks_test_data:\n",
                "    history_type, model, accuracy = item\n",
                "    data.append([history_type, model, accuracy])\n",
                "\n",
                "df_accuracy = pd.DataFrame(data, columns=['History Type', 'Model', 'Accuracy'])\n",
                "\n",
                "# Reorder the models in the desired sequence\n",
                "model_order = [\"GPT-3 (Hendryks test)\", \"GPT-3 (Instruct model)\", \"ChatGPT (GPT-3.5)\", \"GPT-4\"]\n",
                "\n",
                "# Create a bar chart with Seaborn\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.set(style=\"whitegrid\")\n",
                "ax = sns.barplot(x=\"History Type\", y=\"Accuracy\", hue=\"Model\", data=df_accuracy, palette=\"muted\", hue_order=model_order)\n",
                "ax.set(title=\"Accuracy of the GPT Series on A.P. History Questions in the MMLU Benchmarks\", ylabel=\"Accuracy (%)\", xlabel=\"History Category\")\n",
                "\n",
                "# Customize the chart\n",
                "for p in ax.patches:\n",
                "    ax.annotate(\n",
                "        f\"{p.get_height():.2f}%\",\n",
                "        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "        ha=\"center\",\n",
                "        va=\"bottom\",\n",
                "        fontsize=9,\n",
                "        xytext=(0, 3),\n",
                "        textcoords=\"offset points\",\n",
                "    )\n",
                "\n",
                "# Move the legend to the bottom of the chart\n",
                "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(model_order), fancybox=True, shadow=True)\n",
                "\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "debff1",
            "metadata": {},
            "source": [
                "The trajectory of the GPT series on this form of historical knowledge offers a striking demonstration of the rapid gains made in just a few years. GPT-4 now meets expert-level accuracy on all three of the subject exams. These findings mirror GPT-4's performance in other knowledge domains such as medical tests (<cite id=\"cite2c-27937/VEDFUUBA\"><a href=\"#zotero%7C27937%2FVEDFUUBA\"></a></cite>), American bar exams (<cite id=\"cite2c-27937/A39GT7B4\"><a href=\"#zotero%7C27937%2FA39GT7B4\"></a></cite>), and a host of other standardized assessments. (<cite id=\"cite2c-27937/U534FF7L\"><a href=\"#zotero%7C27937%2FU534FF7L\">(OpenAI, 2023)</a></cite>)\n",
                "\n",
                "Yet, why do GPT-4 and other LLMs peform better in some knowledge domains than others? How can it get one question right, and other questions generate errors? There is a temptation to parse the model's performance in ways relatable to our human perspective. The human test taker might approach the question by assessing what types of historical thinking each question requires, what sort of knowledge is offered by the options, and how the historical source relates to the question. But, of course, GPT-4 isn't human - and unlike the human test taker, it has likely already seen the question in advance. In 2021, nearly 400,000 students took the A.P. U.S. History exam. (<cite id=\"cite2c-27937/HAZWJTY7\"><a href=\"#zotero%7C27937%2FHAZWJTY7\"></a></cite>) A vast web presence has emerged to serve the sizable population of students and instructors participating in this international curriculum. Hundreds of exam questions have migrated online via the collective efforts of the test prep publishing industry, various study apps, and uploaded example tests. Given the scale of the dataset used to create it, many of these questions have likely ended up in GPT-4's training data. If those who critique LLMs as \"stochastic parrots\" are correct, then GPT-4's likely comes from sheer memorization, and not through any analytical process. (<cite id=\"cite2c-27937/MVDFMR8K\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">(Bender et al., n.d.)</a></cite>, 618.) GPT-4's varying performance in A.P.'s different historical fields supports this hypothesis. GPT-4 achieves over 90% accuracy in the most popular A.P. courses: U.S. History (second most popular overall) and World History (fifth). In contrast, the GPT series lags in accuracy on European History, the seventeenth most popular A.P. course. (\u201cStudent Score Distributions: AP Exams - May 2019.\u201d) This less popular exam would presumably have a smaller presence both online and in GPT-4's training data. However, this argument is admittedly speculative. GPT-4's training data is not available for public inspection, and the specific mechanisms of how LLMs process information remain a fluid field of inquiry.  \n",
                "\n",
                "Yet even if GPT-4's remarkable performance on standardized tests is the product of memorization, this knowledge has long been a springboard for more advanced forms of historical inquiry. And A.P. study guides are not the only historical texts the GPT series is trained on. Primary source collections, academic monographs, scholarly journals - these too form GPT-4's training data. The influence of these sources can be found when GPT-4 is posed more complex questions in a structured prompt. Let's return to the earlier A.P. questions above featuring the Populist-era campaign song \"The Hayseed.\" In the following prompt, GPT-4 is given the lyrics and publication history of the song. (<cite id=\"cite2c-27937/R5P23ZWU\"><a href=\"#zotero%7C27937%2FR5P23ZWU\"></a></cite>) It is then instructed to identify the larger historical context of the source, the song's intended purpose and audience, and how the source might be interpreted via different historiographical approaches."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7e17c8",
            "metadata": {},
            "source": [
                "While one can debate aspects of GPT-4\u2019s interpretations, it nonetheless accurately captures much of the context and intent of the source. With the right design (and sufficient budget), GPT-4 could be automated to annotate an entire corpus of primary sources, becoming a tool of the digital historian overwhelmed by an abundance of historical data, as envisioned by Roy Rozenweig twenty years ago. Further experimentation will be needed to more fully assess GPT-4\u2019s capabilities for historical interpretation. But progress moves quickly in the ML world, and there is intense competition to build new models that advance the existing capabilities of LLMs and shed their shortcomings. Yet progress remains uneven. Of significant concern are LLM's performance on benchmarks on ethics and morality, which continue to demonstrate troubling areas of weakness. (<cite id=\"cite2c-27937/USPCB8UK\"><a href=\"#zotero%7C27937%2FG5ESJ8NI\">(Hoffmann et al., 2022)</a></cite>, 31, table A6)\n",
                "\n",
                "It at this juncture where historians should contribute their distinctive expertise in the collective efforts to establish ethical guidelines to inform future AI research. (<cite id=\"cite2c-27937/Q4U4YZPD\"><a href=\"#zotero%7C27937%2FQ4U4YZPD\"></a></cite>) We must especially confront the difficult challenge raised by AI researcher Janelle Shane: \u201cSometimes, to reckon with the effects of biased training data is to realize that the app shouldn\u2019t be built.\u201d<cite id=\"cite2c-27937/9SCM73IS\"><a href=\"#zotero%7C27937%2F9SCM73IS\"></a></cite>   "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd8a82",
            "metadata": {},
            "source": [
                "## Chatting with Representations of the Past: Why Historians Should Care About AI"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa0e21",
            "metadata": {},
            "source": [
                "While AIs might master multiple choice questions, most historians would consider this an insufficient proxy for true historical fluency. We need more creative forms of assessment and accessible tools that permit experimentation. Historians also need to engage with the ethical ramifications of these experiments, and devise socially responsible frameworks for implementing these technologies. But we'll need to think quickly - this technology has already enabled a compelling idea nonetheless fraught with unintended consequences.\n",
                "\n",
                "Among their many talents, GPT-4 and other LLMs are adept at generating responses when guided by a specific point of reference, such as the perspective of a well-known historical figure. This surprising capacity enables a simulation of the worldview of a historical personality. This ability may unlock new forms of interaction with historical sources. It could also reproduce ELIZA effects with significant ramifications for the public's engagement with the past.  \n",
                "\n",
                "Such was the context of my first experimentation with an LLM: a simulated conversation with \"Martin Luther\". I selected Luther because of his historical significance and because his conversational style is arguably captured in works like the Table Talk, which reflected his views on a wide range of subjects. (<cite id=\"cite2c-27937/LA73IHHD\"><a href=\"#zotero%7C27937%2FLA73IHHD\"></a></cite>) Using the OpenAI's Playground, I directed GPT-3 to adopt this perspective with the following prompt: \u201cI am an AI representation of Martin Luther, a key figure in the Protestant Reformation. You can ask me questions about faith and theology, and I will answer at great length and in the style of Luther's Table Talk.\u201d\n",
                "\n",
                "And so \u201che\u201d did. Our chat ranged on the key moments in Luther\u2019s life, religious teachings, and even contemporary events. (<cite id=\"cite2c-27937/W9A3PKIN\"><a href=\"#zotero%7C27937%2FW9A3PKIN\"></a></cite>) To be sure, GPT-3 generated for \u201cLuther\u201d some serious hallucinations, such as Emperor Charles V\u2019s conversion to Lutheranism and the Catholic Church\u2019s admission of error at the Diet of Worms. Yet GPT-3 offered accurate and evocative responses in other areas. GPT-3 correctly identified Luther\u2019s views on scriptural authority, the basis of human salvation, and the doctrine of predestination. In engaging with Luther\u2019s views on Copernicus, GPT-3 correctly interpreted Luther\u2019s opposition to heliocentrism and cited appropriate biblical passages supporting that view. Luther even complained of his depiction in contemporary historiography, citing the preeminent scholar in the field. I attempted to further enhance the verisimilitude of Luther\u2019s responses by creating a fine-tuned model of GPT-3 trained on the actual text of Luther\u2019s Table Talk. (<cite id=\"cite2c-27937/5L8CK3F6\"><a href=\"#zotero%7C27937%2F5L8CK3F6\"></a></cite>) This worked to a degree, and GPT-3 soon generated responses that more accurately matched Luther\u2019s famous pugnacity. But I soon questioned the wisdom of creating an application that accurately mimics Luther. His language inspired profound religious and cultural transformation whose power continues to reverberate centuries later. Luther\u2019s language also inspired violence, in his time and in recent memory. (<cite id=\"cite2c-27937/ZMXKDKE8\"><a href=\"#zotero%7C27937%2FZMXKDKE8\"></a></cite>)\n",
                "\n",
                "Following the release and surging popularity of ChatGPT in November 2022, dialogues with simulated historic figures proliferated over social media. App developers quickly developed interfaces to use ChatGPT for such simulated interactions, drawn by the pedagogical potential of new forms of historical interaction.  However, these apps did little to address the problems of LLM \"hallucinations,\" nor the potent ethical ramifications such approaches raise. Users soon reported their conversations with both humanity's greatest luminaries and its greatest villains. (<cite id=\"cite2c-27937/KPPP2BAQ\"><a href=\"#zotero%7C27937%2FKPPP2BAQ\"></a></cite>) The ability of these applications to \u201cbring history to life\u201d soon gave way to an appreciation that perhaps some parts of the past are better off dead. \n",
                "\n",
                "LLMs are quickly entering the public domain. These technologies have the potential to inspire new forms of human discovery and creativity. Yet if we do not take care, AIs will also advance the inequalities, injustices, and misinformation that form the record of human history on which they are trained.\n",
                "\n",
                "Historians have a stake in this future. The informed and ethical integration of AI in historical research and pedagogy has the potential to democratize access to the past, fostering greater inclusivity in a time of educational austerity.  This technology can enhance the learning experience by connecting historical information in novel ways to students and researchers alike, allowing for innovative explorations of historical data and primary sources. In turn, this can foster new scholarly conversations, enrich classroom discussions, and inspire a deeper appreciation for the complexities of the past. But we first have to understand its strengths and limitations, like with any historical source.\n",
                "\n",
                "GPT-4 is anchored within a specific time, with definitive (if vast) contours that historians can interrogate - except this source can respond to your questions. And yes, GPT-4 invents facts, confuses dates, and distorts the past. But don\u2019t our existing sources already require careful examination? Digital historians have demonstrated historiographical innovation in utilizing emerging technologies to create new forms of scholarship. There is similar potential for historical explorations of generative AI. The effort is worthwhile, as few historical sources possess GPT-4\u2019s scope. However flawed, generative AI represents a powerful tool for addressing Roy Rosenzweig\u2019s call to grapple with the \u201cunheard-of historical abundance\u201d of the digital age."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "331ad1",
            "metadata": {},
            "source": [
                "I am grateful to Abraham Gibson for extending an invitation to present the preliminary research findings of this article with the Digital History Working Group in May 2022, organized by the Consortium For History of Science, Technology, and Medicine. I would also like to express my appreciation to my colleagues William Mattingly and Patrick Wadden for their insightful commentary on the article, as well as to Ian Crowe for his valuable feedback and the opportunity to use the \"Ask-A-Source\" approach with students in the Thomas More Seminar at Belmont Abbey College.\n",
                "\n",
                "This article was facilitated by a sabbatical semester generously granted by the Office of Academic Affairs at Belmont Abbey College. My thanks to Provost Travis Feezell and Vice Provost David Williams for their support of this project. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6fdc34",
            "metadata": {},
            "source": [
                "## Bibliography"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7fcfa0",
            "metadata": {},
            "source": [
                "<!-- BIBLIOGRAPHY START --><!-- BIBLIOGRAPHY END -->"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "9b509f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "citation-manager": {
            "items": {
                "zotero": {
                    "27937/36TWI3H2": {
                        "URL": "http://arxiv.org/abs/2203.02155",
                        "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Ouyang",
                                "given": "Long"
                            },
                            {
                                "family": "Wu",
                                "given": "Jeff"
                            },
                            {
                                "family": "Jiang",
                                "given": "Xu"
                            },
                            {
                                "family": "Almeida",
                                "given": "Diogo"
                            },
                            {
                                "family": "Wainwright",
                                "given": "Carroll L."
                            },
                            {
                                "family": "Mishkin",
                                "given": "Pamela"
                            },
                            {
                                "family": "Zhang",
                                "given": "Chong"
                            },
                            {
                                "family": "Agarwal",
                                "given": "Sandhini"
                            },
                            {
                                "family": "Slama",
                                "given": "Katarina"
                            },
                            {
                                "family": "Ray",
                                "given": "Alex"
                            },
                            {
                                "family": "Schulman",
                                "given": "John"
                            },
                            {
                                "family": "Hilton",
                                "given": "Jacob"
                            },
                            {
                                "family": "Kelton",
                                "given": "Fraser"
                            },
                            {
                                "family": "Miller",
                                "given": "Luke"
                            },
                            {
                                "family": "Simens",
                                "given": "Maddie"
                            },
                            {
                                "family": "Askell",
                                "given": "Amanda"
                            },
                            {
                                "family": "Welinder",
                                "given": "Peter"
                            },
                            {
                                "family": "Christiano",
                                "given": "Paul"
                            },
                            {
                                "family": "Leike",
                                "given": "Jan"
                            },
                            {
                                "family": "Lowe",
                                "given": "Ryan"
                            }
                        ],
                        "id": "27937/36TWI3H2",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    3,
                                    4
                                ]
                            ]
                        },
                        "note": "arXiv:2203.02155 [cs]",
                        "number": "arXiv:2203.02155",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/36TWI3H2",
                        "title": "Training language models to follow instructions with human feedback",
                        "type": "article"
                    },
                    "27937/3I2YH8D3": {
                        "URL": "https://www.promptingguide.ai/",
                        "abstract": "A Comprehensive Overview of Prompt Engineering",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "id": "27937/3I2YH8D3",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "language": "en",
                        "system_id": "zotero|27937/3I2YH8D3",
                        "title": "Prompt Engineering Guide",
                        "type": "webpage"
                    },
                    "27937/4QKVA4R7": {
                        "URL": "http://arxiv.org/abs/2201.11903",
                        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Wei",
                                "given": "Jason"
                            },
                            {
                                "family": "Wang",
                                "given": "Xuezhi"
                            },
                            {
                                "family": "Schuurmans",
                                "given": "Dale"
                            },
                            {
                                "family": "Bosma",
                                "given": "Maarten"
                            },
                            {
                                "family": "Ichter",
                                "given": "Brian"
                            },
                            {
                                "family": "Xia",
                                "given": "Fei"
                            },
                            {
                                "family": "Chi",
                                "given": "Ed"
                            },
                            {
                                "family": "Le",
                                "given": "Quoc"
                            },
                            {
                                "family": "Zhou",
                                "given": "Denny"
                            }
                        ],
                        "id": "27937/4QKVA4R7",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    1,
                                    10
                                ]
                            ]
                        },
                        "note": "arXiv:2201.11903 [cs]",
                        "number": "arXiv:2201.11903",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/4QKVA4R7",
                        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "type": "article"
                    },
                    "27937/56EE9N63": {
                        "URL": "http://arxiv.org/abs/2206.07682",
                        "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Wei",
                                "given": "Jason"
                            },
                            {
                                "family": "Tay",
                                "given": "Yi"
                            },
                            {
                                "family": "Bommasani",
                                "given": "Rishi"
                            },
                            {
                                "family": "Raffel",
                                "given": "Colin"
                            },
                            {
                                "family": "Zoph",
                                "given": "Barret"
                            },
                            {
                                "family": "Borgeaud",
                                "given": "Sebastian"
                            },
                            {
                                "family": "Yogatama",
                                "given": "Dani"
                            },
                            {
                                "family": "Bosma",
                                "given": "Maarten"
                            },
                            {
                                "family": "Zhou",
                                "given": "Denny"
                            },
                            {
                                "family": "Metzler",
                                "given": "Donald"
                            },
                            {
                                "family": "Chi",
                                "given": "Ed H."
                            },
                            {
                                "family": "Hashimoto",
                                "given": "Tatsunori"
                            },
                            {
                                "family": "Vinyals",
                                "given": "Oriol"
                            },
                            {
                                "family": "Liang",
                                "given": "Percy"
                            },
                            {
                                "family": "Dean",
                                "given": "Jeff"
                            },
                            {
                                "family": "Fedus",
                                "given": "William"
                            }
                        ],
                        "id": "27937/56EE9N63",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    10,
                                    26
                                ]
                            ]
                        },
                        "note": "arXiv:2206.07682 [cs]",
                        "number": "arXiv:2206.07682",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/56EE9N63",
                        "title": "Emergent Abilities of Large Language Models",
                        "type": "article"
                    },
                    "27937/5L8CK3F6": {
                        "URL": "https://www.gutenberg.org/files/9841/9841-h/9841-h.htm",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "id": "27937/5L8CK3F6",
                        "system_id": "zotero|27937/5L8CK3F6",
                        "title": "Selections from the Table Talk of Martin Luther, by Martin Luther",
                        "type": "webpage"
                    },
                    "27937/6DE3XGUT": {
                        "DOI": "10.1162/99608f92.e32f6dec",
                        "URL": "https://hdsr.mitpress.mit.edu/pub/kyzf7fjv/release/5",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Crane",
                                "given": "Gregory"
                            }
                        ],
                        "container-title": "Harvard Data Science Review",
                        "id": "27937/6DE3XGUT",
                        "issue": "1",
                        "issued": {
                            "date-parts": [
                                [
                                    2019,
                                    7,
                                    3
                                ]
                            ]
                        },
                        "language": "en",
                        "system_id": "zotero|27937/6DE3XGUT",
                        "title": "AI, Language, and the Humanities",
                        "type": "article-journal",
                        "volume": "1"
                    },
                    "27937/7VHKCH3M": {
                        "URL": "http://arxiv.org/abs/2212.04356",
                        "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    29
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Radford",
                                "given": "Alec"
                            },
                            {
                                "family": "Kim",
                                "given": "Jong Wook"
                            },
                            {
                                "family": "Xu",
                                "given": "Tao"
                            },
                            {
                                "family": "Brockman",
                                "given": "Greg"
                            },
                            {
                                "family": "McLeavey",
                                "given": "Christine"
                            },
                            {
                                "family": "Sutskever",
                                "given": "Ilya"
                            }
                        ],
                        "id": "27937/7VHKCH3M",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    12,
                                    6
                                ]
                            ]
                        },
                        "note": "arXiv:2212.04356 [cs, eess]",
                        "number": "arXiv:2212.04356",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/7VHKCH3M",
                        "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
                        "type": "article"
                    },
                    "27937/9SCM73IS": {
                        "URL": "https://twitter.com/JanelleCShane/status/1309150806000664576",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    3
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Janelle Shane [@JanelleCShane]",
                                "given": ""
                            }
                        ],
                        "container-title": "Twitter",
                        "genre": "Tweet",
                        "id": "27937/9SCM73IS",
                        "issued": {
                            "date-parts": [
                                [
                                    2020,
                                    9,
                                    24
                                ]
                            ]
                        },
                        "language": "en",
                        "system_id": "zotero|27937/9SCM73IS",
                        "title": "Never forget that GPT-3 was trained on internet text, and internet text contains some awful stuff. Anyone planning to build things with GPT-3 will have to reckon with this. (essay text and interactive tool here: https://philosopherai.com/philosopher/what-ails-ethiopia-042cc6)",
                        "type": "post"
                    },
                    "27937/A39GT7B4": {
                        "URL": "http://arxiv.org/abs/2212.14402",
                        "abstract": "Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as \"the Bar Exam,\" as a precondition for law practice. To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school. In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try. In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in \"AI?\" In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam. While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance. For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Bommarito II",
                                "given": "Michael"
                            },
                            {
                                "family": "Katz",
                                "given": "Daniel Martin"
                            }
                        ],
                        "id": "27937/A39GT7B4",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    12,
                                    29
                                ]
                            ]
                        },
                        "note": "arXiv:2212.14402 [cs]",
                        "number": "arXiv:2212.14402",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/A39GT7B4",
                        "title": "GPT Takes the Bar Exam",
                        "type": "article"
                    },
                    "27937/A834FRJL": {
                        "URL": "https://github.com/hendrycks/test",
                        "abstract": "Measuring Massive Multitask Language Understanding | ICLR 2021",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Hendrycks",
                                "given": "Dan"
                            }
                        ],
                        "id": "27937/A834FRJL",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "note": "original-date: 2020-09-07T23:02:57Z",
                        "system_id": "zotero|27937/A834FRJL",
                        "title": "Measuring Massive Multitask Language Understanding",
                        "type": "book"
                    },
                    "27937/BXZEP65G": {
                        "URL": "https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "id": "27937/BXZEP65G",
                        "system_id": "zotero|27937/BXZEP65G",
                        "title": "What is LaMDA and What Does it Want? | by Blake Lemoine | Medium",
                        "type": "webpage"
                    },
                    "27937/CJYNFHVI": {
                        "DOI": "10.1086/ahr/108.3.735",
                        "URL": "https://doi.org/10.1086/ahr/108.3.735",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Rosenzweig",
                                "given": "Roy"
                            }
                        ],
                        "container-title": "The American Historical Review",
                        "id": "27937/CJYNFHVI",
                        "issue": "3",
                        "issued": {
                            "date-parts": [
                                [
                                    2003,
                                    6,
                                    1
                                ]
                            ]
                        },
                        "journalAbbreviation": "The American Historical Review",
                        "page": "735-762",
                        "shortTitle": "Scarcity or Abundance?",
                        "system_id": "zotero|27937/CJYNFHVI",
                        "title": "Scarcity or Abundance? Preserving the Past in a Digital Era",
                        "type": "article-journal",
                        "volume": "108"
                    },
                    "27937/F3XT4XAQ": {
                        "URL": "http://arxiv.org/abs/2108.07258",
                        "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Bommasani",
                                "given": "Rishi"
                            },
                            {
                                "family": "Hudson",
                                "given": "Drew A."
                            },
                            {
                                "family": "Adeli",
                                "given": "Ehsan"
                            },
                            {
                                "family": "Altman",
                                "given": "Russ"
                            },
                            {
                                "family": "Arora",
                                "given": "Simran"
                            },
                            {
                                "family": "von Arx",
                                "given": "Sydney"
                            },
                            {
                                "family": "Bernstein",
                                "given": "Michael S."
                            },
                            {
                                "family": "Bohg",
                                "given": "Jeannette"
                            },
                            {
                                "family": "Bosselut",
                                "given": "Antoine"
                            },
                            {
                                "family": "Brunskill",
                                "given": "Emma"
                            },
                            {
                                "family": "Brynjolfsson",
                                "given": "Erik"
                            },
                            {
                                "family": "Buch",
                                "given": "Shyamal"
                            },
                            {
                                "family": "Card",
                                "given": "Dallas"
                            },
                            {
                                "family": "Castellon",
                                "given": "Rodrigo"
                            },
                            {
                                "family": "Chatterji",
                                "given": "Niladri"
                            },
                            {
                                "family": "Chen",
                                "given": "Annie"
                            },
                            {
                                "family": "Creel",
                                "given": "Kathleen"
                            },
                            {
                                "family": "Davis",
                                "given": "Jared Quincy"
                            },
                            {
                                "family": "Demszky",
                                "given": "Dora"
                            },
                            {
                                "family": "Donahue",
                                "given": "Chris"
                            },
                            {
                                "family": "Doumbouya",
                                "given": "Moussa"
                            },
                            {
                                "family": "Durmus",
                                "given": "Esin"
                            },
                            {
                                "family": "Ermon",
                                "given": "Stefano"
                            },
                            {
                                "family": "Etchemendy",
                                "given": "John"
                            },
                            {
                                "family": "Ethayarajh",
                                "given": "Kawin"
                            },
                            {
                                "family": "Fei-Fei",
                                "given": "Li"
                            },
                            {
                                "family": "Finn",
                                "given": "Chelsea"
                            },
                            {
                                "family": "Gale",
                                "given": "Trevor"
                            },
                            {
                                "family": "Gillespie",
                                "given": "Lauren"
                            },
                            {
                                "family": "Goel",
                                "given": "Karan"
                            },
                            {
                                "family": "Goodman",
                                "given": "Noah"
                            },
                            {
                                "family": "Grossman",
                                "given": "Shelby"
                            },
                            {
                                "family": "Guha",
                                "given": "Neel"
                            },
                            {
                                "family": "Hashimoto",
                                "given": "Tatsunori"
                            },
                            {
                                "family": "Henderson",
                                "given": "Peter"
                            },
                            {
                                "family": "Hewitt",
                                "given": "John"
                            },
                            {
                                "family": "Ho",
                                "given": "Daniel E."
                            },
                            {
                                "family": "Hong",
                                "given": "Jenny"
                            },
                            {
                                "family": "Hsu",
                                "given": "Kyle"
                            },
                            {
                                "family": "Huang",
                                "given": "Jing"
                            },
                            {
                                "family": "Icard",
                                "given": "Thomas"
                            },
                            {
                                "family": "Jain",
                                "given": "Saahil"
                            },
                            {
                                "family": "Jurafsky",
                                "given": "Dan"
                            },
                            {
                                "family": "Kalluri",
                                "given": "Pratyusha"
                            },
                            {
                                "family": "Karamcheti",
                                "given": "Siddharth"
                            },
                            {
                                "family": "Keeling",
                                "given": "Geoff"
                            },
                            {
                                "family": "Khani",
                                "given": "Fereshte"
                            },
                            {
                                "family": "Khattab",
                                "given": "Omar"
                            },
                            {
                                "family": "Koh",
                                "given": "Pang Wei"
                            },
                            {
                                "family": "Krass",
                                "given": "Mark"
                            },
                            {
                                "family": "Krishna",
                                "given": "Ranjay"
                            },
                            {
                                "family": "Kuditipudi",
                                "given": "Rohith"
                            },
                            {
                                "family": "Kumar",
                                "given": "Ananya"
                            },
                            {
                                "family": "Ladhak",
                                "given": "Faisal"
                            },
                            {
                                "family": "Lee",
                                "given": "Mina"
                            },
                            {
                                "family": "Lee",
                                "given": "Tony"
                            },
                            {
                                "family": "Leskovec",
                                "given": "Jure"
                            },
                            {
                                "family": "Levent",
                                "given": "Isabelle"
                            },
                            {
                                "family": "Li",
                                "given": "Xiang Lisa"
                            },
                            {
                                "family": "Li",
                                "given": "Xuechen"
                            },
                            {
                                "family": "Ma",
                                "given": "Tengyu"
                            },
                            {
                                "family": "Malik",
                                "given": "Ali"
                            },
                            {
                                "family": "Manning",
                                "given": "Christopher D."
                            },
                            {
                                "family": "Mirchandani",
                                "given": "Suvir"
                            },
                            {
                                "family": "Mitchell",
                                "given": "Eric"
                            },
                            {
                                "family": "Munyikwa",
                                "given": "Zanele"
                            },
                            {
                                "family": "Nair",
                                "given": "Suraj"
                            },
                            {
                                "family": "Narayan",
                                "given": "Avanika"
                            },
                            {
                                "family": "Narayanan",
                                "given": "Deepak"
                            },
                            {
                                "family": "Newman",
                                "given": "Ben"
                            },
                            {
                                "family": "Nie",
                                "given": "Allen"
                            },
                            {
                                "family": "Niebles",
                                "given": "Juan Carlos"
                            },
                            {
                                "family": "Nilforoshan",
                                "given": "Hamed"
                            },
                            {
                                "family": "Nyarko",
                                "given": "Julian"
                            },
                            {
                                "family": "Ogut",
                                "given": "Giray"
                            },
                            {
                                "family": "Orr",
                                "given": "Laurel"
                            },
                            {
                                "family": "Papadimitriou",
                                "given": "Isabel"
                            },
                            {
                                "family": "Park",
                                "given": "Joon Sung"
                            },
                            {
                                "family": "Piech",
                                "given": "Chris"
                            },
                            {
                                "family": "Portelance",
                                "given": "Eva"
                            },
                            {
                                "family": "Potts",
                                "given": "Christopher"
                            },
                            {
                                "family": "Raghunathan",
                                "given": "Aditi"
                            },
                            {
                                "family": "Reich",
                                "given": "Rob"
                            },
                            {
                                "family": "Ren",
                                "given": "Hongyu"
                            },
                            {
                                "family": "Rong",
                                "given": "Frieda"
                            },
                            {
                                "family": "Roohani",
                                "given": "Yusuf"
                            },
                            {
                                "family": "Ruiz",
                                "given": "Camilo"
                            },
                            {
                                "family": "Ryan",
                                "given": "Jack"
                            },
                            {
                                "family": "R\u00e9",
                                "given": "Christopher"
                            },
                            {
                                "family": "Sadigh",
                                "given": "Dorsa"
                            },
                            {
                                "family": "Sagawa",
                                "given": "Shiori"
                            },
                            {
                                "family": "Santhanam",
                                "given": "Keshav"
                            },
                            {
                                "family": "Shih",
                                "given": "Andy"
                            },
                            {
                                "family": "Srinivasan",
                                "given": "Krishnan"
                            },
                            {
                                "family": "Tamkin",
                                "given": "Alex"
                            },
                            {
                                "family": "Taori",
                                "given": "Rohan"
                            },
                            {
                                "family": "Thomas",
                                "given": "Armin W."
                            },
                            {
                                "family": "Tram\u00e8r",
                                "given": "Florian"
                            },
                            {
                                "family": "Wang",
                                "given": "Rose E."
                            },
                            {
                                "family": "Wang",
                                "given": "William"
                            },
                            {
                                "family": "Wu",
                                "given": "Bohan"
                            },
                            {
                                "family": "Wu",
                                "given": "Jiajun"
                            },
                            {
                                "family": "Wu",
                                "given": "Yuhuai"
                            },
                            {
                                "family": "Xie",
                                "given": "Sang Michael"
                            },
                            {
                                "family": "Yasunaga",
                                "given": "Michihiro"
                            },
                            {
                                "family": "You",
                                "given": "Jiaxuan"
                            },
                            {
                                "family": "Zaharia",
                                "given": "Matei"
                            },
                            {
                                "family": "Zhang",
                                "given": "Michael"
                            },
                            {
                                "family": "Zhang",
                                "given": "Tianyi"
                            },
                            {
                                "family": "Zhang",
                                "given": "Xikun"
                            },
                            {
                                "family": "Zhang",
                                "given": "Yuhui"
                            },
                            {
                                "family": "Zheng",
                                "given": "Lucia"
                            },
                            {
                                "family": "Zhou",
                                "given": "Kaitlyn"
                            },
                            {
                                "family": "Liang",
                                "given": "Percy"
                            }
                        ],
                        "id": "27937/F3XT4XAQ",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    7,
                                    12
                                ]
                            ]
                        },
                        "note": "arXiv:2108.07258 [cs]",
                        "number": "arXiv:2108.07258",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/F3XT4XAQ",
                        "title": "On the Opportunities and Risks of Foundation Models",
                        "type": "article"
                    },
                    "27937/FMW5DCWM": {
                        "URL": "http://arxiv.org/abs/2204.02311",
                        "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    28
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Chowdhery",
                                "given": "Aakanksha"
                            },
                            {
                                "family": "Narang",
                                "given": "Sharan"
                            },
                            {
                                "family": "Devlin",
                                "given": "Jacob"
                            },
                            {
                                "family": "Bosma",
                                "given": "Maarten"
                            },
                            {
                                "family": "Mishra",
                                "given": "Gaurav"
                            },
                            {
                                "family": "Roberts",
                                "given": "Adam"
                            },
                            {
                                "family": "Barham",
                                "given": "Paul"
                            },
                            {
                                "family": "Chung",
                                "given": "Hyung Won"
                            },
                            {
                                "family": "Sutton",
                                "given": "Charles"
                            },
                            {
                                "family": "Gehrmann",
                                "given": "Sebastian"
                            },
                            {
                                "family": "Schuh",
                                "given": "Parker"
                            },
                            {
                                "family": "Shi",
                                "given": "Kensen"
                            },
                            {
                                "family": "Tsvyashchenko",
                                "given": "Sasha"
                            },
                            {
                                "family": "Maynez",
                                "given": "Joshua"
                            },
                            {
                                "family": "Rao",
                                "given": "Abhishek"
                            },
                            {
                                "family": "Barnes",
                                "given": "Parker"
                            },
                            {
                                "family": "Tay",
                                "given": "Yi"
                            },
                            {
                                "family": "Shazeer",
                                "given": "Noam"
                            },
                            {
                                "family": "Prabhakaran",
                                "given": "Vinodkumar"
                            },
                            {
                                "family": "Reif",
                                "given": "Emily"
                            },
                            {
                                "family": "Du",
                                "given": "Nan"
                            },
                            {
                                "family": "Hutchinson",
                                "given": "Ben"
                            },
                            {
                                "family": "Pope",
                                "given": "Reiner"
                            },
                            {
                                "family": "Bradbury",
                                "given": "James"
                            },
                            {
                                "family": "Austin",
                                "given": "Jacob"
                            },
                            {
                                "family": "Isard",
                                "given": "Michael"
                            },
                            {
                                "family": "Gur-Ari",
                                "given": "Guy"
                            },
                            {
                                "family": "Yin",
                                "given": "Pengcheng"
                            },
                            {
                                "family": "Duke",
                                "given": "Toju"
                            },
                            {
                                "family": "Levskaya",
                                "given": "Anselm"
                            },
                            {
                                "family": "Ghemawat",
                                "given": "Sanjay"
                            },
                            {
                                "family": "Dev",
                                "given": "Sunipa"
                            },
                            {
                                "family": "Michalewski",
                                "given": "Henryk"
                            },
                            {
                                "family": "Garcia",
                                "given": "Xavier"
                            },
                            {
                                "family": "Misra",
                                "given": "Vedant"
                            },
                            {
                                "family": "Robinson",
                                "given": "Kevin"
                            },
                            {
                                "family": "Fedus",
                                "given": "Liam"
                            },
                            {
                                "family": "Zhou",
                                "given": "Denny"
                            },
                            {
                                "family": "Ippolito",
                                "given": "Daphne"
                            },
                            {
                                "family": "Luan",
                                "given": "David"
                            },
                            {
                                "family": "Lim",
                                "given": "Hyeontaek"
                            },
                            {
                                "family": "Zoph",
                                "given": "Barret"
                            },
                            {
                                "family": "Spiridonov",
                                "given": "Alexander"
                            },
                            {
                                "family": "Sepassi",
                                "given": "Ryan"
                            },
                            {
                                "family": "Dohan",
                                "given": "David"
                            },
                            {
                                "family": "Agrawal",
                                "given": "Shivani"
                            },
                            {
                                "family": "Omernick",
                                "given": "Mark"
                            },
                            {
                                "family": "Dai",
                                "given": "Andrew M."
                            },
                            {
                                "family": "Pillai",
                                "given": "Thanumalayan Sankaranarayana"
                            },
                            {
                                "family": "Pellat",
                                "given": "Marie"
                            },
                            {
                                "family": "Lewkowycz",
                                "given": "Aitor"
                            },
                            {
                                "family": "Moreira",
                                "given": "Erica"
                            },
                            {
                                "family": "Child",
                                "given": "Rewon"
                            },
                            {
                                "family": "Polozov",
                                "given": "Oleksandr"
                            },
                            {
                                "family": "Lee",
                                "given": "Katherine"
                            },
                            {
                                "family": "Zhou",
                                "given": "Zongwei"
                            },
                            {
                                "family": "Wang",
                                "given": "Xuezhi"
                            },
                            {
                                "family": "Saeta",
                                "given": "Brennan"
                            },
                            {
                                "family": "Diaz",
                                "given": "Mark"
                            },
                            {
                                "family": "Firat",
                                "given": "Orhan"
                            },
                            {
                                "family": "Catasta",
                                "given": "Michele"
                            },
                            {
                                "family": "Wei",
                                "given": "Jason"
                            },
                            {
                                "family": "Meier-Hellstern",
                                "given": "Kathy"
                            },
                            {
                                "family": "Eck",
                                "given": "Douglas"
                            },
                            {
                                "family": "Dean",
                                "given": "Jeff"
                            },
                            {
                                "family": "Petrov",
                                "given": "Slav"
                            },
                            {
                                "family": "Fiedel",
                                "given": "Noah"
                            }
                        ],
                        "id": "27937/FMW5DCWM",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    10,
                                    5
                                ]
                            ]
                        },
                        "note": "arXiv:2204.02311 [cs]",
                        "number": "arXiv:2204.02311",
                        "publisher": "arXiv",
                        "shortTitle": "PaLM",
                        "system_id": "zotero|27937/FMW5DCWM",
                        "title": "PaLM: Scaling Language Modeling with Pathways",
                        "type": "article"
                    },
                    "27937/FUEUWF5M": {
                        "DOI": "10.1007/s10502-022-09397-0",
                        "URL": "https://doi.org/10.1007/s10502-022-09397-0",
                        "abstract": "Handwritten Text Recognition (HTR) technology is now a mature machine learning tool, becoming integrated in the digitisation processes of libraries and archives, speeding up the transcription of primary sources and facilitating full text searching and analysis of historic texts at scale. However, research into how HTR is changing our information environment is scant. This paper presents a systematic literature review regarding how researchers are using one particular HTR platform, Transkribus, to indicate the domains where HTR is applied, the approach taken, and how the technology is understood. 381 papers from 2015 to 2020 were gathered from Google Scholar, Scopus, and Web of Science, then grouped and coded into categories using quantitative and qualitative approaches. Published research that mentions Transkribus is international and rapidly growing. Transkribus features primarily in archival and library science publications, while a long tail of broad and eclectic disciplines, including history, computer science, citizen science, law and education, demonstrate the wider applicability of the tool. The most common paper categories were humanities applications (67%), technological (25%), users (5%) and tutorials (3%). This paper presents the first overarching review of HTR as featured in published research, while also elucidating how HTR is affecting the information environment.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Nockels",
                                "given": "Joe"
                            },
                            {
                                "family": "Gooding",
                                "given": "Paul"
                            },
                            {
                                "family": "Ames",
                                "given": "Sarah"
                            },
                            {
                                "family": "Terras",
                                "given": "Melissa"
                            }
                        ],
                        "container-title": "Archival Science",
                        "id": "27937/FUEUWF5M",
                        "issue": "3",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    9,
                                    1
                                ]
                            ]
                        },
                        "journalAbbreviation": "Arch Sci",
                        "language": "en",
                        "page": "367-392",
                        "shortTitle": "Understanding the application of handwritten text recognition technology in heritage contexts",
                        "system_id": "zotero|27937/FUEUWF5M",
                        "title": "Understanding the application of handwritten text recognition technology in heritage contexts: a systematic review of Transkribus in published research",
                        "type": "article-journal",
                        "volume": "22"
                    },
                    "27937/G5ESJ8NI": {
                        "URL": "http://arxiv.org/abs/2203.15556",
                        "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Hoffmann",
                                "given": "Jordan"
                            },
                            {
                                "family": "Borgeaud",
                                "given": "Sebastian"
                            },
                            {
                                "family": "Mensch",
                                "given": "Arthur"
                            },
                            {
                                "family": "Buchatskaya",
                                "given": "Elena"
                            },
                            {
                                "family": "Cai",
                                "given": "Trevor"
                            },
                            {
                                "family": "Rutherford",
                                "given": "Eliza"
                            },
                            {
                                "family": "Casas",
                                "given": "Diego de Las"
                            },
                            {
                                "family": "Hendricks",
                                "given": "Lisa Anne"
                            },
                            {
                                "family": "Welbl",
                                "given": "Johannes"
                            },
                            {
                                "family": "Clark",
                                "given": "Aidan"
                            },
                            {
                                "family": "Hennigan",
                                "given": "Tom"
                            },
                            {
                                "family": "Noland",
                                "given": "Eric"
                            },
                            {
                                "family": "Millican",
                                "given": "Katie"
                            },
                            {
                                "family": "Driessche",
                                "given": "George van den"
                            },
                            {
                                "family": "Damoc",
                                "given": "Bogdan"
                            },
                            {
                                "family": "Guy",
                                "given": "Aurelia"
                            },
                            {
                                "family": "Osindero",
                                "given": "Simon"
                            },
                            {
                                "family": "Simonyan",
                                "given": "Karen"
                            },
                            {
                                "family": "Elsen",
                                "given": "Erich"
                            },
                            {
                                "family": "Rae",
                                "given": "Jack W."
                            },
                            {
                                "family": "Vinyals",
                                "given": "Oriol"
                            },
                            {
                                "family": "Sifre",
                                "given": "Laurent"
                            }
                        ],
                        "id": "27937/G5ESJ8NI",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    3,
                                    29
                                ]
                            ]
                        },
                        "note": "arXiv:2203.15556 [cs]",
                        "number": "arXiv:2203.15556",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/G5ESJ8NI",
                        "title": "Training Compute-Optimal Large Language Models",
                        "type": "article"
                    },
                    "27937/G7XTPF45": {
                        "URL": "https://apex974.com/articles/literature-reviews-with-paper-qa-and-zotero?rel=",
                        "abstract": "Paper QA is a Python package that uses ChatGPT to perform Q&A and summarization based on scholarly articles. When combining it with pyZotero, we can streamline literature reviews based on one's Zotero library.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "id": "27937/G7XTPF45",
                        "language": "en",
                        "system_id": "zotero|27937/G7XTPF45",
                        "title": "Streamline Literature Reviews with Paper QA and Zotero",
                        "type": "webpage"
                    },
                    "27937/HAZWJTY7": {
                        "URL": "https://allaccess.collegeboard.org/ap-us-history-exam-2021-results",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "id": "27937/HAZWJTY7",
                        "language": "en",
                        "shortTitle": "AP U.S. History Exam",
                        "system_id": "zotero|27937/HAZWJTY7",
                        "title": "AP U.S. History Exam: 2021 Results \u2013 All Access | College Board",
                        "type": "webpage"
                    },
                    "27937/IDRV8C65": {
                        "URL": "http://thomasmorestudies.org/teaching/",
                        "abstract": "Free teaching materials on the life and works of Thomas more. Curriculum units on the Renaissance, Francis Bacon, Johanathan Swift, and more.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "container-title": "Thomas More Studies",
                        "id": "27937/IDRV8C65",
                        "language": "en-US",
                        "system_id": "zotero|27937/IDRV8C65",
                        "title": "Teaching",
                        "type": "post-weblog"
                    },
                    "27937/IJ5AI567": {
                        "URL": "https://github.com/hwchase17/langchain",
                        "abstract": "\u26a1 Building applications with LLMs through composability \u26a1",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Chase",
                                "given": "Harrison"
                            }
                        ],
                        "id": "27937/IJ5AI567",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    10
                                ]
                            ]
                        },
                        "note": "original-date: 2022-10-17T02:58:36Z",
                        "system_id": "zotero|27937/IJ5AI567",
                        "title": "LangChain",
                        "type": "book"
                    },
                    "27937/IJWETJM7": {
                        "URL": "https://docsouth.unc.edu/sohp/A-0339/menu.html",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "id": "27937/IJWETJM7",
                        "system_id": "zotero|27937/IJWETJM7",
                        "title": "John Hope Franklin and John Egerton, conducted by Oral History Interview with John Hope Franklin, July 27, 1990. Interview A-0339. Southern Oral History Program Collection (#4007).",
                        "type": "webpage"
                    },
                    "27937/KHF4TXQH": {
                        "DOI": "10.5860/CHOICE.42-0916",
                        "URL": "http://choicereviews.org/review/10.5860/CHOICE.42-0916",
                        "abstract": "Pamela McCorduck first went among the artificial intelligentsia when the field was fresh and new, and asked the scientists engaged in it what they were doing and why. She saw artificial intelligence as the scientific apotheosis of one of the most enduring, glorious, often amusing, and sometimes alarming, traditions of human culture: the endless fascination with artifacts that think. Machines Who Think was translated into many languages, became an international cult classic, and stayed in print for nearly twenty years. Now, Machines Who Think is back, along with an extended addition that brings the field up to date in the last quarter century, including its scientific and its public faces. McCorduck shows how, from a slightly dubious fringe science, artificial intelligence has moved slowly (though not always steadily) to a central place in our everyday lives, and how it will be even more crucial as the World Wide Web moves into its next generation.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "container-title": "Choice Reviews Online",
                        "id": "27937/KHF4TXQH",
                        "issued": {
                            "date-parts": [
                                [
                                    2004,
                                    10,
                                    1
                                ]
                            ]
                        },
                        "language": "en",
                        "page": "42-0916-42-0916",
                        "shortTitle": "Machines who think",
                        "system_id": "zotero|27937/KHF4TXQH",
                        "title": "Machines who think: a personal inquiry into the history and prospects of artificial intelligence",
                        "type": "paper-conference",
                        "volume": "42"
                    },
                    "27937/KNEK45E4": {
                        "URL": "http://arxiv.org/abs/2005.14165",
                        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Brown",
                                "given": "Tom B."
                            },
                            {
                                "family": "Mann",
                                "given": "Benjamin"
                            },
                            {
                                "family": "Ryder",
                                "given": "Nick"
                            },
                            {
                                "family": "Subbiah",
                                "given": "Melanie"
                            },
                            {
                                "family": "Kaplan",
                                "given": "Jared"
                            },
                            {
                                "family": "Dhariwal",
                                "given": "Prafulla"
                            },
                            {
                                "family": "Neelakantan",
                                "given": "Arvind"
                            },
                            {
                                "family": "Shyam",
                                "given": "Pranav"
                            },
                            {
                                "family": "Sastry",
                                "given": "Girish"
                            },
                            {
                                "family": "Askell",
                                "given": "Amanda"
                            },
                            {
                                "family": "Agarwal",
                                "given": "Sandhini"
                            },
                            {
                                "family": "Herbert-Voss",
                                "given": "Ariel"
                            },
                            {
                                "family": "Krueger",
                                "given": "Gretchen"
                            },
                            {
                                "family": "Henighan",
                                "given": "Tom"
                            },
                            {
                                "family": "Child",
                                "given": "Rewon"
                            },
                            {
                                "family": "Ramesh",
                                "given": "Aditya"
                            },
                            {
                                "family": "Ziegler",
                                "given": "Daniel M."
                            },
                            {
                                "family": "Wu",
                                "given": "Jeffrey"
                            },
                            {
                                "family": "Winter",
                                "given": "Clemens"
                            },
                            {
                                "family": "Hesse",
                                "given": "Christopher"
                            },
                            {
                                "family": "Chen",
                                "given": "Mark"
                            },
                            {
                                "family": "Sigler",
                                "given": "Eric"
                            },
                            {
                                "family": "Litwin",
                                "given": "Mateusz"
                            },
                            {
                                "family": "Gray",
                                "given": "Scott"
                            },
                            {
                                "family": "Chess",
                                "given": "Benjamin"
                            },
                            {
                                "family": "Clark",
                                "given": "Jack"
                            },
                            {
                                "family": "Berner",
                                "given": "Christopher"
                            },
                            {
                                "family": "McCandlish",
                                "given": "Sam"
                            },
                            {
                                "family": "Radford",
                                "given": "Alec"
                            },
                            {
                                "family": "Sutskever",
                                "given": "Ilya"
                            },
                            {
                                "family": "Amodei",
                                "given": "Dario"
                            }
                        ],
                        "id": "27937/KNEK45E4",
                        "issued": {
                            "date-parts": [
                                [
                                    2020,
                                    7,
                                    22
                                ]
                            ]
                        },
                        "note": "arXiv:2005.14165 [cs]",
                        "number": "arXiv:2005.14165",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/KNEK45E4",
                        "title": "Language Models are Few-Shot Learners",
                        "type": "article"
                    },
                    "27937/KPPP2BAQ": {
                        "URL": "https://www.nbcnews.com/tech/tech-news/chatgpt-gpt-chat-bot-ai-hitler-historical-figures-open-rcna66531",
                        "abstract": "The Historical Figures app is available in Apple's App Store and lets you chat with notable people from history re-animated by artificial intelligence.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "container-title": "NBC News",
                        "id": "27937/KPPP2BAQ",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    1,
                                    20
                                ]
                            ]
                        },
                        "language": "en",
                        "system_id": "zotero|27937/KPPP2BAQ",
                        "title": "Chatbot that lets you talk to Jesus and Hitler is latest AI controversy",
                        "type": "webpage"
                    },
                    "27937/L2ILKERU": {
                        "ISBN": "9781783266371",
                        "abstract": "The Digital Humanities have arrived at a moment when digital Big Data is becoming more readily available, opening exciting new avenues of inquiry but also new challenges. This pioneering book describes and demonstrates the ways these data can be explored to construct cultural heritage knowledge, for research and in teaching and learning. It helps humanities scholars to grasp Big Data in order to do their work, whether that means understanding the underlying algorithms at work in search engines, or designing and using their own tools to process large amounts of information.Demonstrating what digital tools have to offer and also what 'digital' does to how we understand the past, the authors introduce the many different tools and developing approaches in Big Data for historical and humanistic scholarship, show how to use them, what to be wary of, and discuss the kinds of questions and new perspectives this new macroscopic perspective opens up. Authored 'live' online with ongoing feedback from the wider digital history community, Exploring Big Historical Data breaks new ground and sets the direction for the conversation into the future. It represents the current state-of-the-art thinking in the field and exemplifies the way that digital work can enhance public engagement in the humanities.Exploring Big Historical Data should be the go-to resource for undergraduate and graduate students confronted by a vast corpus of data, and researchers encountering these methods for the first time. It will also offer a helping hand to the interested individual seeking to make sense of genealogical data or digitized newspapers, and even the local historical society who are trying to see the value in digitizing their holdings.",
                        "author": [
                            {
                                "family": "Graham",
                                "given": "Shawn"
                            },
                            {
                                "family": "Milligan",
                                "given": "Ian"
                            },
                            {
                                "family": "Weingart",
                                "given": "Scott"
                            }
                        ],
                        "edition": "Reprint edition",
                        "event-place": "London",
                        "id": "27937/L2ILKERU",
                        "issued": {
                            "date-parts": [
                                [
                                    2015,
                                    11,
                                    16
                                ]
                            ]
                        },
                        "language": "English",
                        "number-of-pages": "306",
                        "publisher": "Icp",
                        "publisher-place": "London",
                        "shortTitle": "Exploring Big Historical Data",
                        "system_id": "zotero|27937/L2ILKERU",
                        "title": "Exploring Big Historical Data: The Historian's Macroscope",
                        "type": "book"
                    },
                    "27937/LA73IHHD": {
                        "URL": "https://",
                        "abstract": "\"Martin Luther\u2019s Occasional Writings: Table Talk, Letters, and Prefaces\" published on  by Oxford University Press.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Sch\u00e4ufele",
                                "given": "Wolf-Friedrich"
                            }
                        ],
                        "container-title": "Oxford Research Encyclopedia of Religion",
                        "id": "27937/LA73IHHD",
                        "issued": {
                            "date-parts": [
                                [
                                    2017,
                                    3,
                                    29
                                ]
                            ]
                        },
                        "language": "en",
                        "note": "DOI: 10.1093/acrefore/9780199340378.013.294",
                        "shortTitle": "Martin Luther\u2019s Occasional Writings",
                        "system_id": "zotero|27937/LA73IHHD",
                        "title": "Martin Luther\u2019s Occasional Writings: Table Talk, Letters, and Prefaces",
                        "type": "webpage"
                    },
                    "27937/MVDFMR8K": {
                        "URL": "https://dl.acm.org/doi/10.1145/3442188.3445922",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Bender",
                                "given": "Emily"
                            },
                            {
                                "family": "Gebru",
                                "given": "Timnit"
                            },
                            {
                                "family": "McMillan-Major",
                                "given": "Angelina"
                            },
                            {
                                "family": "Mitchell",
                                "given": "Margaret"
                            }
                        ],
                        "id": "27937/MVDFMR8K",
                        "system_id": "zotero|27937/MVDFMR8K",
                        "title": "On the Dangers of Stochastic Parrots | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
                        "type": "webpage"
                    },
                    "27937/Q4U4YZPD": {
                        "URL": "http://arxiv.org/abs/2008.02275",
                        "abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    3
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Hendrycks",
                                "given": "Dan"
                            },
                            {
                                "family": "Burns",
                                "given": "Collin"
                            },
                            {
                                "family": "Basart",
                                "given": "Steven"
                            },
                            {
                                "family": "Critch",
                                "given": "Andrew"
                            },
                            {
                                "family": "Li",
                                "given": "Jerry"
                            },
                            {
                                "family": "Song",
                                "given": "Dawn"
                            },
                            {
                                "family": "Steinhardt",
                                "given": "Jacob"
                            }
                        ],
                        "id": "27937/Q4U4YZPD",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    2,
                                    17
                                ]
                            ]
                        },
                        "note": "arXiv:2008.02275 [cs]",
                        "number": "arXiv:2008.02275",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/Q4U4YZPD",
                        "title": "Aligning AI With Shared Human Values",
                        "type": "article"
                    },
                    "27937/R5P23ZWU": {
                        "URL": "https://nebnewspapers.unl.edu/lccn/2017270209/1890-10-04/ed-1/seq-1/",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "id": "27937/R5P23ZWU",
                        "system_id": "zotero|27937/R5P23ZWU",
                        "title": "The farmers' alliance. (Lincoln, Nebraska) 1889-1892, October 04, 1890, Image 1 \u00ab Nebraska Newspapers",
                        "type": "webpage"
                    },
                    "27937/U534FF7L": {
                        "URL": "http://arxiv.org/abs/2303.08774",
                        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    28
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "OpenAI",
                                "given": ""
                            }
                        ],
                        "id": "27937/U534FF7L",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    27
                                ]
                            ]
                        },
                        "note": "arXiv:2303.08774 [cs]",
                        "number": "arXiv:2303.08774",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/U534FF7L",
                        "title": "GPT-4 Technical Report",
                        "type": "article"
                    },
                    "27937/UJDUIP4T": {
                        "URL": "https://github.com/whitead/paper-qa",
                        "abstract": "LLM Chain for answering questions from documents with citations",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "White",
                                "given": "Andrew"
                            }
                        ],
                        "id": "27937/UJDUIP4T",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "note": "original-date: 2023-02-05T01:07:25Z",
                        "system_id": "zotero|27937/UJDUIP4T",
                        "title": "Paper QA",
                        "type": "book"
                    },
                    "27937/UYVGUT4C": {
                        "URL": "http://arxiv.org/abs/2103.00020",
                        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    28
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Radford",
                                "given": "Alec"
                            },
                            {
                                "family": "Kim",
                                "given": "Jong Wook"
                            },
                            {
                                "family": "Hallacy",
                                "given": "Chris"
                            },
                            {
                                "family": "Ramesh",
                                "given": "Aditya"
                            },
                            {
                                "family": "Goh",
                                "given": "Gabriel"
                            },
                            {
                                "family": "Agarwal",
                                "given": "Sandhini"
                            },
                            {
                                "family": "Sastry",
                                "given": "Girish"
                            },
                            {
                                "family": "Askell",
                                "given": "Amanda"
                            },
                            {
                                "family": "Mishkin",
                                "given": "Pamela"
                            },
                            {
                                "family": "Clark",
                                "given": "Jack"
                            },
                            {
                                "family": "Krueger",
                                "given": "Gretchen"
                            },
                            {
                                "family": "Sutskever",
                                "given": "Ilya"
                            }
                        ],
                        "id": "27937/UYVGUT4C",
                        "issued": {
                            "date-parts": [
                                [
                                    2021,
                                    2,
                                    26
                                ]
                            ]
                        },
                        "note": "arXiv:2103.00020 [cs]",
                        "number": "arXiv:2103.00020",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/UYVGUT4C",
                        "title": "Learning Transferable Visual Models From Natural Language Supervision",
                        "type": "article"
                    },
                    "27937/VEDFUUBA": {
                        "URL": "http://arxiv.org/abs/2303.13375",
                        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    28
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Nori",
                                "given": "Harsha"
                            },
                            {
                                "family": "King",
                                "given": "Nicholas"
                            },
                            {
                                "family": "McKinney",
                                "given": "Scott Mayer"
                            },
                            {
                                "family": "Carignan",
                                "given": "Dean"
                            },
                            {
                                "family": "Horvitz",
                                "given": "Eric"
                            }
                        ],
                        "id": "27937/VEDFUUBA",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    20
                                ]
                            ]
                        },
                        "note": "arXiv:2303.13375 [cs]",
                        "number": "arXiv:2303.13375",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/VEDFUUBA",
                        "title": "Capabilities of GPT-4 on Medical Challenge Problems",
                        "type": "article"
                    },
                    "27937/W9A3PKIN": {
                        "URL": "https://github.com/Dr-Hutchinson/jdh_article/blob/63bf01c69441d63aa0315474c972f59447c52a4f/media/GPT-3%20Dialogue%20with%20Martin%20Luther.txt.txt",
                        "abstract": "Repo for submission for Journal of Digital History.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    3
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Hutchinson",
                                "given": "Daniel"
                            }
                        ],
                        "id": "27937/W9A3PKIN",
                        "issued": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    3
                                ]
                            ]
                        },
                        "note": "original-date: 2023-04-03T04:04:06Z",
                        "system_id": "zotero|27937/W9A3PKIN",
                        "title": "Transcript of conversation with GPT-3 simulating Martin Luther.",
                        "type": "book"
                    },
                    "27937/XKINSLH3": {
                        "URL": "http://arxiv.org/abs/2301.00303",
                        "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "He",
                                "given": "Hangfeng"
                            },
                            {
                                "family": "Zhang",
                                "given": "Hongming"
                            },
                            {
                                "family": "Roth",
                                "given": "Dan"
                            }
                        ],
                        "id": "27937/XKINSLH3",
                        "issued": {
                            "date-parts": [
                                [
                                    2022,
                                    12,
                                    31
                                ]
                            ]
                        },
                        "note": "arXiv:2301.00303 [cs]",
                        "number": "arXiv:2301.00303",
                        "publisher": "arXiv",
                        "shortTitle": "Rethinking with Retrieval",
                        "system_id": "zotero|27937/XKINSLH3",
                        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
                        "type": "article"
                    },
                    "27937/YAX4VVR9": {
                        "URL": "https://python.langchain.com/en/latest/#",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    5
                                ]
                            ]
                        },
                        "id": "27937/YAX4VVR9",
                        "system_id": "zotero|27937/YAX4VVR9",
                        "title": "Welcome to LangChain \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.0.131",
                        "type": "webpage"
                    },
                    "27937/ZJW9AI49": {
                        "DOI": "10.3138/chr.694",
                        "URL": "https://www.utpjournals.press/doi/abs/10.3138/chr.694",
                        "abstract": "It all seems so orderly and comprehensive. Instead of firing up the microfilm reader to navigate the Globe and Mail or the Toronto Star, one needs only to log into online newspaper databases. A keyword search, for a particular event, person, or cultural phenomenon, brings up a list of research findings. Previously impossible research projects can now be attempted. This process has fundamentally reshaped Canadian historical scholarship. We can see this in Canadian history dissertations. In 1998, a year with 67 dissertations, the Toronto Star was cited 74 times. However it was cited 753 times in 2010, a year with 69 dissertations. Similar data appears in the Canadian Historical Review (CHR), a prestigious peer-reviewed journal. Databases are skewing our research. We are witnessing the application of commercial Optical Character Recognition (OCR) technology \u2013 originally and primarily designed for the efficient digitization of large reams of corporate and legal documents, conventionally formatted \u2013 to historical sources. The results are, unsurprisingly, a mixed bag. In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    3,
                                    31
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Milligan",
                                "given": "Ian"
                            }
                        ],
                        "container-title": "Canadian Historical Review",
                        "id": "27937/ZJW9AI49",
                        "issue": "4",
                        "issued": {
                            "date-parts": [
                                [
                                    2013,
                                    12
                                ]
                            ]
                        },
                        "page": "540-569",
                        "shortTitle": "Illusionary Order",
                        "system_id": "zotero|27937/ZJW9AI49",
                        "title": "Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997\u20132010",
                        "type": "article-journal",
                        "volume": "94"
                    },
                    "27937/ZMXKDKE8": {
                        "ISBN": "9780253001009",
                        "abstract": "The acquiescence of the German Protestant churches in Nazi oppression and murder of Jews is well documented. In this book, Christopher J. Probst demonstrates that a significant number of German theologians and clergy made use of the 16th-century writings by Martin Luther on Jews and Judaism to reinforce the racial antisemitism and religious anti-Judaism already present among Protestants. Focusing on key figures, Probst's study makes clear that a significant number of pastors, bishops, and theologians of varying theological and political persuasions employed Luther's texts with considerable effectiveness in campaigning for the creation of a \"de-Judaized\" form of Christianity. Probst shows that even the church most critical of Luther's anti-Jewish writings reaffirmed the antisemitic stereotyping that helped justify early Nazi measures against the Jews.",
                        "author": [
                            {
                                "family": "Probst",
                                "given": "Christopher J."
                            }
                        ],
                        "edition": "Illustrated edition",
                        "event-place": "Bloomington",
                        "id": "27937/ZMXKDKE8",
                        "issued": {
                            "date-parts": [
                                [
                                    2012,
                                    6,
                                    8
                                ]
                            ]
                        },
                        "language": "English",
                        "number-of-pages": "270",
                        "publisher": "Indiana University Press",
                        "publisher-place": "Bloomington",
                        "shortTitle": "Demonizing the Jews",
                        "system_id": "zotero|27937/ZMXKDKE8",
                        "title": "Demonizing the Jews: Luther and the Protestant Church in Nazi Germany",
                        "type": "book"
                    },
                    "27937/ZS9JDNGD": {
                        "URL": "http://arxiv.org/abs/2009.03300",
                        "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
                        "accessed": {
                            "date-parts": [
                                [
                                    2023,
                                    4,
                                    2
                                ]
                            ]
                        },
                        "author": [
                            {
                                "family": "Hendrycks",
                                "given": "Dan"
                            },
                            {
                                "family": "Burns",
                                "given": "Collin"
                            },
                            {
                                "family": "Basart",
                                "given": "Steven"
                            },
                            {
                                "family": "Zou",
                                "given": "Andy"
                            },
                            {
                                "family": "Mazeika",
                                "given": "Mantas"
                            },
                            {
                                "family": "Song",
                                "given": "Dawn"
                            },
                            {
                                "family": "Steinhardt",
                                "given": "Jacob"
                            }
                        ],
                        "id": "27937/ZS9JDNGD",
                        "issued": {
                            "date-parts": [
                                [
                                    2021,
                                    1,
                                    12
                                ]
                            ]
                        },
                        "note": "arXiv:2009.03300 [cs]",
                        "number": "arXiv:2009.03300",
                        "publisher": "arXiv",
                        "system_id": "zotero|27937/ZS9JDNGD",
                        "title": "Measuring Massive Multitask Language Understanding",
                        "type": "article"
                    }
                }
            }
        },
        "kernelspec": {
            "argv": [
                "/usr/bin/python3",
                "-m",
                "ipykernel",
                "--HistoryManager.enabled=False",
                "--matplotlib=inline",
                "-c",
                "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
                "-f",
                "{connection_file}"
            ],
            "display_name": "Python 3 (system-wide)",
            "env": {},
            "language": "python",
            "metadata": {
                "cocalc": {
                    "description": "Python 3 programming language",
                    "priority": 100,
                    "url": "https://www.python.org/"
                }
            },
            "name": "python3",
            "resource_dir": "/ext/jupyter/kernels/python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}