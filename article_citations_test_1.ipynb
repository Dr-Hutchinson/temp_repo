{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    " ### Contributor1FirstName  Contributor1LastName [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/ORCID_ID) \n",
    "Institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Contributor2FirstName  Contributor2LastName [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/ORCID_ID_IF_EXIST) \n",
    "Institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Contributor3FirstName  Contributor3LastName [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/ORCID_ID_IF_EXIST) \n",
    "Institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "cover"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAACWBAMAAABkyf1EAAAAG1BMVEXMzMyWlpacnJyqqqrFxcWxsbGjo6O3t7e+vr6He3KoAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEcElEQVR4nO2aTW/bRhCGh18ij1zKknMkbbf2UXITIEeyMhIfRaF1exQLA/JRclslRykO+rs7s7s0VwytNmhJtsA8gHZEcox9PTs7uysQgGEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmEYhmGYr2OWRK/ReIKI8Zt7Hb19wTcQ0uTkGh13bQupcw7gPOvdo12/5CzNtNR7xLUtNtT3CGBQ6g3InjY720pvofUec22LJPr8PhEp2OMPyI40PdwWUdronCu9yQpdPx53bQlfLKnfOVhlnDYRBXve4Ov+IZTeMgdedm0NR+xoXJeQvdJ3CvziykSukwil16W/Oe7aGjIjqc/9ib4jQlJy0uArtN4A0+cvXFvDkmUJ47sJ1Y1ATLDNVXZkNPIepQzxy1ki9fqiwbUj/I+64zxWNzyZnPuhvohJ9K70VvXBixpcu2SAHU+Xd9EKdEJDNpYP3AQr3bQSpPQ6Y6/4dl1z7ZDbArsszjA7L0g7ibB0CDcidUWVoErvIMKZh2Xs0LUzcLW6V5NfiUgNEbaYmAVL6bXl0nJRc+1S72ua/D/cTjGPlQj7eUqd7A096rYlRjdPYlhz7VIvxpVG3cemDKF+WAwLY/6XelOZKTXXzsC4xvDjjtSN6kHLhLke6PrwM8h1raf40qjrGO7H9aTEbduucjS04ZrYU/4iuS5Z2Hdt0rvCLFdmLEXcU30AGddST62o+sLcf5l6k7CP+ru4pLYqX/VFyxbm/utQbx/r22ZEbTb2f5I2kns1Y1OQR8ZyofX+TjJxj1Rz7QQVnf1QzR26Oth0ueJVYcRP6ZUPac/Rx/5M6ixO1dhSrT3Y1DpiYmx3tF4ZUdpz9LD/dSg9PXES0LB71BwcGjKROuV28lnvnv7HHJsezheBGH5+X2CfSfRbMKW+5aGs3JFjMrjGibJc0S7TJzqjHrh2hDybj9XRXNZa89Aro55XBdbW5wti2c/5WJ7jJ1RolVUn/HWpb0I58Tziup6Rx7Dm2hnbRP1GM9PW/NFmQ4PtVRVN63Wvxfmu5sowDMMwDMMwDMMwDMMwDMMwDMMwzL+CpT//F/6beoV8zb2Jmt4Qryx6lTUCsENQ75HOkhXAO3EPVgyQtKtUy3C/e+FJg17Zjnew1Xrdb9InbG4WqfUAftG+WhLwPVyfg536+MU7m4C1CMk4ZznpXZzDYI1PDL2nS1hpvc5cNd7E2sJg05Fe7/7d3Fln8Cvc3bwB616auxsKl4WPghjemHrDqyDWeu1UNW5s2btPnSQ75oOdunEwWazfwgVG0kqluYCM9OIjWOGnfA2b9G4Ha63XKpvQ8perTvTifJNhi6+WMWmi7smEZf6G8MmhlyGq+NqP8GV84TLuJr7UIQVx+bDEoEpRZIz42gs40OuN4Mv8hXzelV7KX1isH+ewTWckikyVv+CfHuqVF7I16gN0VKypX6wPsE+zFPzkinolU9UH8OMGvSpnZqKsv13p/RsMun6X5x/y2LeAr8O66lsBwzBMP/wJfyGq8pgBk6IAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"./media/placeholder.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "disclaimer"
    ]
   },
   "source": [
    " (optional) This article was orginally published (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "This article examines how digital historians are using large language models (LLMs) in their research and teaching, along with the critical and ethical debates surrounding their use. The article first assesses the historical capacities of LLMs as measured by machine learning benchmarks, and how such assessments can help historians understand the capacities and limits of these technologies. The utility of LLMs as digital tools are then demonstrated through a series of case studies using GPT-4 and other generative AI models. LLMs are tasked with a variety of tasks for streamlining data preparation, such as oral history transcription, correcting optical character recognition (OCR) errors, and metadata extraction. These case studies also demonstrate how frameworks for using LLMs, such as prompt engineering and retrieval augmented generation (RAG), are used to ground LLM outputs for consistency and greater accuracy. Acknowledging the significant ethical challenges posed by LLMs, the article emphasizes the need for critical engagement and the development of responsible frameworks for implementing these technologies in historical scholarship. By combining disciplinary expertise with innovative computational approaches, historians are discovering new ways to navigate the \"unheard-of historical abundance\" of the digital age, contributing to approaches to generative AI that enriches, rather than distorts, our understanding of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This is the first paragrah of running text with a citation example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "5w5sr": [
       {
        "id": "27937/XQYUJV5F",
        "source": "zotero"
       }
      ],
      "fgell": [
       {
        "id": "27937/CJYNFHVI",
        "source": "zotero"
       }
      ],
      "uo7pa": [
       {
        "id": "27937/L2ILKERU",
        "source": "zotero"
       }
      ]
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In 2003, Roy Rosenzweig predicted that digital historians would need to develop new techniques \"to research, write, and teach in a world of unheard-of historical abundance.\" (<cite id=\"fgell\"><a href=\"#zotero%7C27937%2FCJYNFHVI\">Rosenzweig, “Scarcity or Abundance?”</a></cite>) Over the past two decades historians have risen to this challenge, embracing digital mapping, network analysis, distant reading of large text collections, and machine learning as part of their growing methodological toolkit. (<cite id=\"uo7pa\"><a href=\"#zotero%7C27937%2FL2ILKERU\">Graham, Milligan, and Weingart, <i>Exploring Big Historical Data</i>.</a></cite>) Generative artificial intelligence (AI) has emerged as another potential tool that historians are using to explore the past, particularly large language models (LLMs), the most prominent form of this technology. These models possess striking capacities to generate, interpret, and manipulate data across a range of modalities. The rapidly-expanding scope of these capabilities and their limits remain intensely debated, as do their broader social, economic, cultural, and environmental impacts. Yet while still an emerging technology, historians are already demonstrating generative AI's potential as a versatile digital tool. Historians are also contributing to the critical discourse surrounding this new domain, raising key questions about how these models achieve their capabilities, their propensity to reinforce existing inequalities, and their potential to distort our understanding of the past. (<cite id=\"5w5sr\"><a href=\"#zotero%7C27937%2FXQYUJV5F\">Meadows and Sternfeld, “Artificial Intelligence and the Practice of History.”</a></cite>)\n",
    "\n",
    "This article contributes to this discourse by demonstrating how digital historians are using generative AI to explore the past, as well as the disciplinary opportunities historians can offer to these broader debates. We begin by assessing the metrics commonly used to measure the historical knowledge of LLMs, and examine how such metrics can give us insights into the capacities and limits this technology. We then examine how generative AI can be used in tasks as varied as preparing datasets, exploring text collections, and offering novel (and controversial) methods of representing the past. We conclude with a call to historians to continue to contribute to ongoing research and debates concerning the ethical use of generative AI. Given the rapid pace of innovation in this field, it is crucial that the profession addresses the implications of this technology for our research and teaching. Historians will have much to contribute in contextualizing the innovative and disruptive potential of these breakthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do AIs Know About History? Assessing LLMs for Historical Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "4e7tr": [
       {
        "id": "27937/9T2I7QLM",
        "source": "zotero"
       }
      ],
      "e95sf": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "ucor9": [
       {
        "id": "27937/56EE9N63",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "As historians explore the possibilities of generative AI, it is important to understand how these technologies are created and assessed. With this knowledge we can better evaluate their potential utility and their limits.\n",
    "\n",
    "\n",
    "<cite id=\"e95sf\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "As historians explore the possibilities of generative AI, it is important to understand how these technologies are created and assessed. With this knowledge we can better evaluate their potential utility and their limits.\n",
    "\n",
    "At the most fundamental level, generative AI models like LLMs are statistical representations of the datasets on which they are trained. Machine learning techniques like deep learning and recent innovations like the Transformer network architecture (<cite id=\"4e7tr\"><a href=\"#zotero%7C27937%2F9T2I7QLM\">Vaswani et al., “Attention Is All You Need.”</a></cite>) have enabled the creation of models capable of mimicking the data on which they are trained with a high degree of fidelity. But researchers have also discovered that with sufficient time and the application of (often immense) computational power, these models exhibit a range of “emergent” capabilities. (<cite id=\"ucor9\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al., “Emergent Abilities of Large Language Models.”</a></cite>) For example, LLMs can summarize texts, perform language translation, write working computer code, and compose informative responses on a wide array of subjects - all without specific training on how to perform such tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "citation-manager": {
     "citations": {
      "47zjl": [
       {
        "id": "27937/56EE9N63",
        "source": "zotero"
       }
      ],
      "drdmm": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "jxtri": [
       {
        "id": "27937/9T2I7QLM",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "outputs": [],
   "source": [
    "<cite id=\"jxtri\"><a href=\"#zotero%7C27937%2F9T2I7QLM\">Vaswani et al., “Attention Is All You Need.”</a></cite>\n",
    "\n",
    "<cite id=\"47zjl\"><a href=\"#zotero%7C27937%2F56EE9N63\">Wei et al.</a></cite>\n",
    "\n",
    "<cite id=\"drdmm\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "5rfo9": [
       {
        "id": "27937/RRLN9TF5",
        "source": "zotero"
       }
      ],
      "l99eo": [
       {
        "id": "27937/QPK6D3M9",
        "source": "zotero"
       }
      ],
      "nppps": [
       {
        "id": "27937/U534FF7L",
        "source": "zotero"
       }
      ],
      "q7whc": [
       {
        "id": "27937/ZICATXAV",
        "source": "zotero"
       }
      ],
      "r8lyr": [
       {
        "id": "27937/UYVGUT4C",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "1.2, Hermuntical 1, before\n",
    "\n",
    "While the Generative Pre-trained Transformer (GPT) series from OpenAI is the best known of these foundational models, there has been a rapid proliferation of commercial and open-source alternatives. Notable recent LLMs include Google’s Gemini, Anthropic’s Claude, and open-source models offered by Meta and Mistral.\n",
    "\n",
    "Foundational models are also emerging in other domains, such as image, video, and audio synthesis. Architectures like CLIP (<cite id=\"r8lyr\"><a href=\"#zotero%7C27937%2FUYVGUT4C\">Radford et al., “Learning Transferable Visual Models From Natural Language Supervision.”</a></cite>) enable the creation of synthetic imagery in models like OpenAI’s DALL-E, Midjourney, and the open-source community behind Stable Diffusion. Similar approaches for generating video, speech, and music have been developed by firms like Runway-XL, ElevenLabs, and Suno, along with open-source alternatives hosted on sites likes HuggingFace. Most notably, new forms of LLM-training have enabled the a combination of these capacities in multi-modal models capable of working across multiple domains, such as OpenAI’s GPT-4 series. (<cite id=\"nppps\"><a href=\"#zotero%7C27937%2FU534FF7L\">OpenAI, “GPT-4 Technical Report.”</a></cite>)\n",
    "\n",
    "An accessible way to stay abreast of recent innovations in this field is by following the leaderboards used to measure performance on standard LLM benchmarks. LLMArena’s Chatbot Arena (<cite id=\"q7whc\"><a href=\"#zotero%7C27937%2FZICATXAV\">“Chatbot Arena (Formerly LMSYS).”</a></cite>) offers an overview of leading contemporary models, while HuggingFace’s Open LLM Leaderboard (<cite id=\"l99eo\"><a href=\"#zotero%7C27937%2FQPK6D3M9\">“Open LLM Leaderboard 2 - a Hugging Face Space by Open-Llm-Leaderboard.”</a></cite>) and the Open Multilingual LLM Evaluation Leaderboard () offer specialized metrics for particular domains and use-cases. (<cite id=\"5rfo9\"><a href=\"#zotero%7C27937%2FRRLN9TF5\">“Open Multilingual Llm Leaderboard - a Hugging Face Space by Uonlp.”</a></cite>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "7zssq": [
       {
        "id": "27937/BXZEP65G",
        "source": "zotero"
       }
      ],
      "pl7am": [
       {
        "id": "27937/MVDFMR8K",
        "source": "zotero"
       }
      ],
      "sj7gk": [
       {
        "id": "27937/EZNK3CE3",
        "source": "zotero"
       }
      ],
      "xw5wn": [
       {
        "id": "27937/FMW5DCWM",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "1.2 Paragraph - While such claims...\n",
    "\n",
    "<cite id=\"pl7am\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">Bender et al., “On the Dangers of Stochastic Parrots | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.”</a></cite>\n",
    "\n",
    "<cite id=\"sj7gk\"><a href=\"#zotero%7C27937%2FEZNK3CE3\">McCorduck, <i>Machines Who Think a Personal Inquiry into the History and Prospects of Artificial Intelligence</i>.</a></cite>\n",
    "\n",
    "<cite id=\"7zssq\"><a href=\"#zotero%7C27937%2FBXZEP65G\">“What Is LaMDA and What Does It Want? | by Blake Lemoine | Medium.”</a></cite>\n",
    "\n",
    "<cite id=\"xw5wn\"><a href=\"#zotero%7C27937%2FFMW5DCWM\">Chowdhery et al., “PaLM.”</a></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "oy7fh": [
       {
        "id": "27937/5AL5LZ2K",
        "source": "zotero"
       }
      ],
      "q68mg": [
       {
        "id": "27937/A834FRJL",
        "source": "zotero"
       }
      ],
      "vge7c": [
       {
        "id": "27937/GSIXPJ7P",
        "source": "zotero"
       }
      ],
      "wfmit": [
       {
        "id": "27937/ZS9JDNGD",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "1.2 Paragraph - The MMLU Benchmarks\n",
    "\n",
    "<cite id=\"wfmit\"><a href=\"#zotero%7C27937%2FZS9JDNGD\">Hendrycks et al., “Measuring Massive Multitask Language Understanding.”</a></cite>\n",
    "\n",
    "Hermuntical section after on individual scores\n",
    "\n",
    "<cite id=\"q68mg\"><a href=\"#zotero%7C27937%2FA834FRJL\">Hendrycks, <i>Measuring Massive Multitask Language Understanding</i>.</a></cite>\n",
    "\n",
    "Hermunitical section after scores\n",
    "\n",
    "<cite id=\"vge7c\"><a href=\"#zotero%7C27937%2FGSIXPJ7P\">Mai and Liang, “Massive Multitask Language Understanding (MMLU) on HELM.”</a></cite>\n",
    "\n",
    "<cite id=\"oy7fh\"><a href=\"#zotero%7C27937%2F5AL5LZ2K\">“What Do AIs Know About History? A Digital History Experiment by Daniel Hutchinson · Streamlit.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "3unok": [
       {
        "id": "27937/YVTAGDKZ",
        "source": "zotero"
       }
      ],
      "55r4n": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "5a9qa": [
       {
        "id": "27937/BD8996H7",
        "source": "zotero"
       }
      ],
      "6cssb": [
       {
        "id": "27937/9GQG6VFM",
        "source": "zotero"
       }
      ],
      "6ph9l": [
       {
        "id": "27937/5YDNQS4V",
        "source": "zotero"
       }
      ],
      "8fjtz": [
       {
        "id": "27937/VEDFUUBA",
        "source": "zotero"
       }
      ],
      "97pas": [
       {
        "id": "27937/X4D92B7V",
        "source": "zotero"
       }
      ],
      "ahtmn": [
       {
        "id": "27937/BVBZMR66",
        "source": "zotero"
       }
      ],
      "c6t3w": [
       {
        "id": "27937/TPGPSRAI",
        "source": "zotero"
       }
      ],
      "fpott": [
       {
        "id": "27937/IEQ8GAVU",
        "source": "zotero"
       }
      ],
      "gzk2l": [
       {
        "id": "27937/R5P23ZWU",
        "source": "zotero"
       }
      ],
      "j1kj5": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "jm1mt": [
       {
        "id": "27937/5GTQD5W9",
        "source": "zotero"
       }
      ],
      "kba8r": [
       {
        "id": "27937/U534FF7L",
        "source": "zotero"
       }
      ],
      "r1ql3": [
       {
        "id": "27937/TGPDB8WX",
        "source": "zotero"
       }
      ],
      "vuott": [
       {
        "id": "27937/MVDFMR8K",
        "source": "zotero"
       }
      ],
      "xndfm": [
       {
        "id": "27937/NYNDVYMM",
        "source": "zotero"
       }
      ],
      "yh6j9": [
       {
        "id": "27937/S3ADX5DD",
        "source": "zotero"
       }
      ],
      "zsn16": [
       {
        "id": "27937/MHRIEHH8",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "1.2 Paragraph - Rapid improvement on this benchmark....\n",
    "\n",
    "<cite id=\"8fjtz\"><a href=\"#zotero%7C27937%2FVEDFUUBA\">Nori et al., “Capabilities of GPT-4 on Medical Challenge Problems.”</a></cite>\n",
    "\n",
    "<cite id=\"ahtmn\"><a href=\"#zotero%7C27937%2FBVBZMR66\">Katz, “GPT Takes the Bar Exam.”</a></cite>\n",
    "\n",
    "<cite id=\"kba8r\"><a href=\"#zotero%7C27937%2FU534FF7L\">OpenAI, “GPT-4 Technical Report.”</a></cite>\n",
    "\n",
    "<cite id=\"5a9qa\"><a href=\"#zotero%7C27937%2FBD8996H7\">“Program Summary Report.”</a></cite>\n",
    "\n",
    "<cite id=\"55r4n\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "<cite id=\"vuott\"><a href=\"#zotero%7C27937%2FMVDFMR8K\">Bender et al., “On the Dangers of Stochastic Parrots | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.”</a></cite>\n",
    "\n",
    "<cite id=\"6cssb\"><a href=\"#zotero%7C27937%2F9GQG6VFM\">Ji et al., “Survey of Hallucination in Natural Language Generation.”</a></cite>\n",
    "\n",
    "<cite id=\"j1kj5\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "<cite id=\"c6t3w\"><a href=\"#zotero%7C27937%2FTPGPSRAI\">Bender, “On NYT Magazine on AI.”</a></cite>\n",
    "\n",
    "<cite id=\"6ph9l\"><a href=\"#zotero%7C27937%2F5YDNQS4V\">Barton, “Algorithmic Bias Detection and Mitigation.”</a></cite>\n",
    "\n",
    "<cite id=\"xndfm\"><a href=\"#zotero%7C27937%2FNYNDVYMM\">“OpenAI’s GPT-3 Speaks! (Kindly Disregard Toxic Language) - IEEE Spectrum.”</a></cite>\n",
    "\n",
    "<cite id=\"fpott\"><a href=\"#zotero%7C27937%2FIEQ8GAVU\">Noble, <i>Algorithms of Oppression</i>.</a></cite>\n",
    "\n",
    "<cite id=\"yh6j9\"><a href=\"#zotero%7C27937%2FS3ADX5DD\">Gebru, “Race and Gender.”</a></cite>\n",
    "\n",
    "<cite id=\"97pas\"><a href=\"#zotero%7C27937%2FX4D92B7V\">Benjamin, <i>Race After Technology</i>.</a></cite>\n",
    "\n",
    "<cite id=\"3unok\"><a href=\"#zotero%7C27937%2FYVTAGDKZ\">Crawford, <i>Atlas of AI</i>.</a></cite>\n",
    "\n",
    "<cite id=\"jm1mt\"><a href=\"#zotero%7C27937%2F5GTQD5W9\">“Excavating AI.”</a></cite>\n",
    "\n",
    "<cite id=\"r1ql3\"><a href=\"#zotero%7C27937%2FTGPDB8WX\">Christiano et al., “Deep Reinforcement Learning from Human Preferences.”</a></cite>\n",
    "\n",
    "<cite id=\"zsn16\"><a href=\"#zotero%7C27937%2FMHRIEHH8\">Gehman et al., “RealToxicityPrompts.”</a></cite>\n",
    "\n",
    "<cite id=\"gzk2l\"><a href=\"#zotero%7C27937%2FR5P23ZWU\">“The Farmers’ Alliance. (Lincoln, Nebraska) 1889-1892, October 04, 1890, Image 1 « Nebraska Newspapers.”</a></cite>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "rxaxc": [
       {
        "id": "27937/G5ESJ8NI",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "1.2 Post-Hayseed\n",
    "\n",
    "<cite id=\"rxaxc\"><a href=\"#zotero%7C27937%2FG5ESJ8NI\">Hoffmann et al., “Training Compute-Optimal Large Language Models.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "c1hh3": [
       {
        "id": "27937/XEUKQDPE",
        "source": "zotero"
       }
      ],
      "dweo4": [
       {
        "id": "27937/IJWETJM7",
        "source": "zotero"
       }
      ],
      "xc7rl": [
       {
        "id": "27937/7VHKCH3M",
        "source": "zotero"
       }
      ],
      "zf8ru": [
       {
        "id": "27937/VXGSAGTI",
        "source": "zotero"
       }
      ],
      "zyqfq": [
       {
        "id": "27937/YWJAQ4V8",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "3 Case Studies - Intro & Oral History\n",
    "\n",
    "<cite id=\"zf8ru\"><a href=\"#zotero%7C27937%2FVXGSAGTI\">Dasu and Johnson, <i>Exploratory Data Mining and Data Cleaning</i>.</a></cite>\n",
    "\n",
    "<cite id=\"c1hh3\"><a href=\"#zotero%7C27937%2FXEUKQDPE\">Wickham, “Tidy Data.”</a></cite>\n",
    "\n",
    "<cite id=\"zyqfq\"><a href=\"#zotero%7C27937%2FYWJAQ4V8\">Ritchie, <i>Doing Oral History</i>.</a></cite>\n",
    "\n",
    "<cite id=\"xc7rl\"><a href=\"#zotero%7C27937%2F7VHKCH3M\">Radford et al., “Robust Speech Recognition via Large-Scale Weak Supervision.”</a></cite>\n",
    "\n",
    "<cite id=\"dweo4\"><a href=\"#zotero%7C27937%2FIJWETJM7\">“John Hope Franklin and John Egerton, Conducted by Oral History Interview with John Hope Franklin, July 27, 1990. Interview A-0339. Southern Oral History Program Collection (#4007).”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "9b61q": [
       {
        "id": "27937/GJ2EYJWT",
        "source": "zotero"
       }
      ],
      "kav0z": [
       {
        "id": "27937/I2BKP7MN",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "3 Case Studies - Oral History - Hermunutical\n",
    "\n",
    "<cite id=\"9b61q\"><a href=\"#zotero%7C27937%2FGJ2EYJWT\">“Whisper - a Hugging Face Space by Openai.”</a></cite>\n",
    "\n",
    "<cite id=\"kav0z\"><a href=\"#zotero%7C27937%2FI2BKP7MN\">Schultz, “[Tutorial] Using Whisper to Transcribe Oral Interviews – CSS @ IPP.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "awd3s": [
       {
        "id": "27937/UHZYQM3W",
        "source": "zotero"
       }
      ],
      "km6om": [
       {
        "id": "27937/KKDPZJYW",
        "source": "zotero"
       }
      ],
      "v20p9": [
       {
        "id": "27937/MYFQUX4C",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "3 - Case Studies - Oral History - conclusion\n",
    "\n",
    "<cite id=\"awd3s\"><a href=\"#zotero%7C27937%2FUHZYQM3W\">Somers, “Whispers of A.I.’s Modular Future | The New Yorker.”</a></cite>\n",
    "\n",
    "<cite id=\"km6om\"><a href=\"#zotero%7C27937%2FKKDPZJYW\">Lehečka et al., “Transformer-Based Speech Recognition Models for Oral History Archives in English, German, and Czech.”</a></cite>\n",
    "\n",
    "<cite id=\"v20p9\"><a href=\"#zotero%7C27937%2FMYFQUX4C\">“Artificial Intelligence Aids Cultural Heritage Researchers Documenting and Teaching Oral Histories.”</a></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "0dhwm": [
       {
        "id": "27937/9HF43E58",
        "source": "zotero"
       }
      ],
      "0jirx": [
       {
        "id": "27937/ZJW9AI49",
        "source": "zotero"
       }
      ],
      "b6864": [
       {
        "id": "27937/TIAJYHF6",
        "source": "zotero"
       }
      ],
      "g7pxd": [
       {
        "id": "27937/JV9GGCQA",
        "source": "zotero"
       }
      ],
      "kp7bi": [
       {
        "id": "27937/ZXTQBIJU",
        "source": "zotero"
       }
      ],
      "ltaz6": [
       {
        "id": "27937/58X69RSW",
        "source": "zotero"
       }
      ],
      "op3w5": [
       {
        "id": "27937/KNEK45E4",
        "source": "zotero"
       }
      ],
      "swwon": [
       {
        "id": "27937/5G5LJCLC",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "Case Studies - OCR beginning\n",
    "\n",
    "<cite id=\"b6864\"><a href=\"#zotero%7C27937%2FTIAJYHF6\">Muehlberger et al., “Transforming Scholarship in the Archives through Handwritten Text Recognition.”</a></cite>\n",
    "\n",
    "<cite id=\"0jirx\"><a href=\"#zotero%7C27937%2FZJW9AI49\">Milligan, “Illusionary Order.”</a></cite>\n",
    "\n",
    "After first image\n",
    "\n",
    "<cite id=\"op3w5\"><a href=\"#zotero%7C27937%2FKNEK45E4\">Brown et al., “Language Models Are Few-Shot Learners.”</a></cite>\n",
    "\n",
    "Prompt engineerin hemutical section\n",
    "\n",
    "<cite id=\"swwon\"><a href=\"#zotero%7C27937%2F5G5LJCLC\">Saravia, <i>Prompt Engineering Guide</i>.</a></cite>\n",
    "\n",
    "<cite id=\"0dhwm\"><a href=\"#zotero%7C27937%2F9HF43E58\">“Prompt Engineering Guide.”</a></cite>\n",
    "\n",
    "Final concluding section - \n",
    "\n",
    "<cite id=\"ltaz6\"><a href=\"#zotero%7C27937%2F58X69RSW\">Abdin et al., “Phi-3 Technical Report.”</a></cite>\n",
    "\n",
    "<cite id=\"g7pxd\"><a href=\"#zotero%7C27937%2FJV9GGCQA\">“Post-OCR-Correction.”</a></cite>\n",
    "\n",
    "<cite id=\"kp7bi\"><a href=\"#zotero%7C27937%2FZXTQBIJU\">“PleIAs/Post-OCR-Correction · Datasets at Hugging Face.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "0p0a5": [
       {
        "id": "27937/LC63DETW",
        "source": "zotero"
       }
      ],
      "3y4ir": [
       {
        "id": "27937/HIPL38QS",
        "source": "zotero"
       }
      ],
      "6161c": [
       {
        "id": "27937/68YHDUH6",
        "source": "zotero"
       }
      ],
      "k4zh5": [
       {
        "id": "27937/5ED45HQE",
        "source": "zotero"
       }
      ],
      "tqis9": [
       {
        "id": "27937/Z44J4BKC",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "3 Case Study - Data Extraction\n",
    "\n",
    "<cite id=\"3y4ir\"><a href=\"#zotero%7C27937%2FHIPL38QS\">Chastang, Aguilar, and Tannier, “A Named Entity Recognition Model for Medieval Latin Charters.”</a></cite>\n",
    "\n",
    "<cite id=\"6161c\"><a href=\"#zotero%7C27937%2F68YHDUH6\">“Using Named Entity Recognition to Enhance Access to a Museum Catalog – Document Blog.”</a></cite>\n",
    "\n",
    "after code block\n",
    "\n",
    "<cite id=\"k4zh5\"><a href=\"#zotero%7C27937%2F5ED45HQE\">González-Gallardo et al., “Leveraging Open Large Language Models for Historical Named Entity Recognition.”</a></cite>\n",
    "\n",
    "<cite id=\"0p0a5\"><a href=\"#zotero%7C27937%2FLC63DETW\">Dagdelen et al., “Structured Information Extraction from Scientific Text with Large Language Models.”</a></cite>\n",
    "\n",
    "<cite id=\"tqis9\"><a href=\"#zotero%7C27937%2FZ44J4BKC\">Hu et al., “Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "31iwl": [
       {
        "id": "27937/38C5LZCI",
        "source": "zotero"
       }
      ],
      "cwwxs": [
       {
        "id": "27937/P2KVKTMZ",
        "source": "zotero"
       }
      ],
      "j022d": [
       {
        "id": "27937/7D6BEHLB",
        "source": "zotero"
       }
      ],
      "j39fn": [
       {
        "id": "27937/MYBFNHF8",
        "source": "zotero"
       }
      ],
      "k7j4i": [
       {
        "id": "27937/ECQ4J8E9",
        "source": "zotero"
       }
      ],
      "rfj7w": [
       {
        "id": "27937/RJTNQXZP",
        "source": "zotero"
       }
      ],
      "usvx1": [
       {
        "id": "27937/HGR9QB96",
        "source": "zotero"
       }
      ],
      "yphfo": [
       {
        "id": "27937/ENMGUHSL",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "3 Case Study - RAG\n",
    "\n",
    "<cite id=\"k7j4i\"><a href=\"#zotero%7C27937%2FECQ4J8E9\">Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.”</a></cite>\n",
    "\n",
    "<cite id=\"j022d\"><a href=\"#zotero%7C27937%2F7D6BEHLB\">Blankenship, Connell, and Dombrowski, “Understanding and Creating Word Embeddings.”</a></cite>\n",
    "\n",
    "<cite id=\"cwwxs\"><a href=\"#zotero%7C27937%2FP2KVKTMZ\">Hutchinson, “Nicolay: Exploring the Speeches of Abraham Lincoln with AI.”</a></cite>\n",
    "\n",
    "<cite id=\"usvx1\"><a href=\"#zotero%7C27937%2FHGR9QB96\">“Presidential Speeches | Miller Center.”</a></cite>\n",
    "\n",
    "Post Nicolay Response\n",
    "\n",
    "<cite id=\"31iwl\"><a href=\"#zotero%7C27937%2F38C5LZCI\">“Google AI Search Tells Users to Glue Pizza and Eat Rocks.”</a></cite>\n",
    "\n",
    "<cite id=\"yphfo\"><a href=\"#zotero%7C27937%2FENMGUHSL\">Li, <i>Lifan0127/Ai-Research-Assistant</i>.</a></cite>\n",
    "\n",
    "<cite id=\"j39fn\"><a href=\"#zotero%7C27937%2FMYBFNHF8\">“WARC-GPT.”</a></cite>\n",
    "\n",
    "<cite id=\"rfj7w\"><a href=\"#zotero%7C27937%2FRJTNQXZP\">Shao et al., “Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models.”</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "7s56c": [
       {
        "id": "27937/CXURVMLQ",
        "source": "zotero"
       }
      ],
      "ansdl": [
       {
        "id": "27937/XQYUJV5F",
        "source": "zotero"
       }
      ],
      "b3ukt": [
       {
        "id": "27937/YZQEASUC",
        "source": "zotero"
       }
      ],
      "cekc5": [
       {
        "id": "27937/2LKXDMZD",
        "source": "zotero"
       }
      ],
      "ed4sh": [
       {
        "id": "27937/I363EKXY",
        "source": "zotero"
       }
      ],
      "gh8cw": [
       {
        "id": "27937/VHJBTADE",
        "source": "zotero"
       }
      ],
      "or8nq": [
       {
        "id": "27937/9CA225ZV",
        "source": "zotero"
       }
      ],
      "qqqlh": [
       {
        "id": "27937/XBWIZZJJ",
        "source": "zotero"
       }
      ],
      "ra4qf": [
       {
        "id": "27937/KPPP2BAQ",
        "source": "zotero"
       }
      ],
      "wnwa8": [
       {
        "id": "27937/BN44JR8V",
        "source": "zotero"
       }
      ],
      "xsk3k": [
       {
        "id": "27937/82YIALT5",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "Conclusion\n",
    "\n",
    "<cite id=\"wnwa8\"><a href=\"#zotero%7C27937%2FBN44JR8V\">Underwood, “Mapping the Latent Spaces of Culture.”</a></cite>\n",
    "\n",
    "Atlantic article \"Essay is Dead\"\n",
    "\n",
    "<cite id=\"b3ukt\"><a href=\"#zotero%7C27937%2FYZQEASUC\">“MLA-CCCC Joint Task Force on Writing and AI.”</a></cite>\n",
    "\n",
    "<cite id=\"ansdl\"><a href=\"#zotero%7C27937%2FXQYUJV5F\">Meadows and Sternfeld, “Artificial Intelligence and the Practice of History.”</a></cite>\n",
    "\n",
    "<cite id=\"7s56c\"><a href=\"#zotero%7C27937%2FCXURVMLQ\">Vee, Laquintano, and Schnitzler, “TextGenEd Exhibit.”</a></cite>\n",
    "\n",
    "<cite id=\"cekc5\"><a href=\"#zotero%7C27937%2F2LKXDMZD\">“Simulating History with ChatGPT - by Benjamin Breen.”</a></cite>\n",
    "\n",
    "<cite id=\"ra4qf\"><a href=\"#zotero%7C27937%2FKPPP2BAQ\">“Chatbot That Lets You Talk to Jesus and Hitler Is Latest AI Controversy.”</a></cite>\n",
    "\n",
    "<cite id=\"xsk3k\"><a href=\"#zotero%7C27937%2F82YIALT5\">Cui, Li, and Zhou, “Can AI Replace Human Subjects?”</a></cite>\n",
    "\n",
    "<cite id=\"ed4sh\"><a href=\"#zotero%7C27937%2FI363EKXY\">Xie et al., “Can Large Language Model Agents Simulate Human Trust Behaviors?”</a></cite>\n",
    "\n",
    "<cite id=\"or8nq\"><a href=\"#zotero%7C27937%2F9CA225ZV\">Stade et al., “Large Language Models Could Change the Future of Behavioral Healthcare.”</a></cite>\n",
    "\n",
    "<cite id=\"qqqlh\"><a href=\"#zotero%7C27937%2FXBWIZZJJ\">Park et al., “Generative Agents.”</a></cite>\n",
    "\n",
    "<cite id=\"gh8cw\"><a href=\"#zotero%7C27937%2FVHJBTADE\">Lepore, <i>If Then</i>.</a></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "This is a hermeneutic paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "table-1"
    ]
   },
   "source": [
    "Editor|1641|1798|1916\n",
    "---|---|---|---\n",
    "Senan|0.55|0.4|0.3\n",
    "Henry|0.71|0.5|0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "hidden"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your Python version\n",
    "from platform import python_version\n",
    "python_version()\n",
    "\n",
    "#!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas package needs to be added to the requirements.txt 's file \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PredominantDegree</th>\n",
       "      <th>HighestDegree</th>\n",
       "      <th>FundingModel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Geography</th>\n",
       "      <th>AdmissionRate</th>\n",
       "      <th>ACTMedian</th>\n",
       "      <th>SATAverage</th>\n",
       "      <th>AverageCost</th>\n",
       "      <th>Expenditure</th>\n",
       "      <th>AverageFacultySalary</th>\n",
       "      <th>MedianDebt</th>\n",
       "      <th>AverageAgeofEntry</th>\n",
       "      <th>MedianFamilyIncome</th>\n",
       "      <th>MedianEarnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>17</td>\n",
       "      <td>823</td>\n",
       "      <td>18888</td>\n",
       "      <td>7459</td>\n",
       "      <td>7079</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>20.629999</td>\n",
       "      <td>29039.0</td>\n",
       "      <td>27000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Alabama at Birmingham</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>25</td>\n",
       "      <td>1146</td>\n",
       "      <td>19990</td>\n",
       "      <td>17208</td>\n",
       "      <td>10170</td>\n",
       "      <td>16250.0</td>\n",
       "      <td>22.670000</td>\n",
       "      <td>34909.0</td>\n",
       "      <td>37200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Alabama in Huntsville</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>26</td>\n",
       "      <td>1180</td>\n",
       "      <td>20306</td>\n",
       "      <td>9352</td>\n",
       "      <td>9341</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>23.190001</td>\n",
       "      <td>39766.0</td>\n",
       "      <td>41500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama State University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>17</td>\n",
       "      <td>830</td>\n",
       "      <td>17400</td>\n",
       "      <td>7393</td>\n",
       "      <td>6557</td>\n",
       "      <td>15854.5</td>\n",
       "      <td>20.889999</td>\n",
       "      <td>24029.5</td>\n",
       "      <td>22400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The University of Alabama</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>26</td>\n",
       "      <td>1171</td>\n",
       "      <td>26717</td>\n",
       "      <td>9817</td>\n",
       "      <td>9605</td>\n",
       "      <td>17750.0</td>\n",
       "      <td>20.770000</td>\n",
       "      <td>58976.0</td>\n",
       "      <td>39200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>University of Connecticut-Avery Point</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>24</td>\n",
       "      <td>1020</td>\n",
       "      <td>12946</td>\n",
       "      <td>11730</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>University of Connecticut-Stamford</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>New England</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>21</td>\n",
       "      <td>1017</td>\n",
       "      <td>13028</td>\n",
       "      <td>4958</td>\n",
       "      <td>14803</td>\n",
       "      <td>18983.0</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>86510.0</td>\n",
       "      <td>49700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>California State University-Channel Islands</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Public</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Mid-size Suburb</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>20</td>\n",
       "      <td>954</td>\n",
       "      <td>22570</td>\n",
       "      <td>12026</td>\n",
       "      <td>8434</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>24.850000</td>\n",
       "      <td>32103.0</td>\n",
       "      <td>35800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>DigiPen Institute of Technology</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Far West</td>\n",
       "      <td>Small City</td>\n",
       "      <td>0.6635</td>\n",
       "      <td>28</td>\n",
       "      <td>1225</td>\n",
       "      <td>37848</td>\n",
       "      <td>5998</td>\n",
       "      <td>7659</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>68233.0</td>\n",
       "      <td>72800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>Neumont University</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Private For-Profit</td>\n",
       "      <td>Rocky Mountains</td>\n",
       "      <td>Mid-size City</td>\n",
       "      <td>0.7997</td>\n",
       "      <td>25</td>\n",
       "      <td>1104</td>\n",
       "      <td>37379</td>\n",
       "      <td>3298</td>\n",
       "      <td>6991</td>\n",
       "      <td>22313.0</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>39241.0</td>\n",
       "      <td>37300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1294 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name PredominantDegree  \\\n",
       "0                        Alabama A & M University        Bachelor's   \n",
       "1             University of Alabama at Birmingham        Bachelor's   \n",
       "2             University of Alabama in Huntsville        Bachelor's   \n",
       "3                        Alabama State University        Bachelor's   \n",
       "4                       The University of Alabama        Bachelor's   \n",
       "...                                           ...               ...   \n",
       "1289        University of Connecticut-Avery Point        Bachelor's   \n",
       "1290           University of Connecticut-Stamford        Bachelor's   \n",
       "1291  California State University-Channel Islands        Bachelor's   \n",
       "1292              DigiPen Institute of Technology        Bachelor's   \n",
       "1293                           Neumont University        Bachelor's   \n",
       "\n",
       "     HighestDegree        FundingModel           Region        Geography  \\\n",
       "0         Graduate              Public        Southeast    Mid-size City   \n",
       "1         Graduate              Public        Southeast    Mid-size City   \n",
       "2         Graduate              Public        Southeast    Mid-size City   \n",
       "3         Graduate              Public        Southeast    Mid-size City   \n",
       "4         Graduate              Public        Southeast       Small City   \n",
       "...            ...                 ...              ...              ...   \n",
       "1289      Graduate              Public      New England  Mid-size Suburb   \n",
       "1290      Graduate              Public      New England    Mid-size City   \n",
       "1291      Graduate              Public         Far West  Mid-size Suburb   \n",
       "1292      Graduate  Private For-Profit         Far West       Small City   \n",
       "1293    Bachelor's  Private For-Profit  Rocky Mountains    Mid-size City   \n",
       "\n",
       "      AdmissionRate  ACTMedian  SATAverage  AverageCost  Expenditure  \\\n",
       "0            0.8989         17         823        18888         7459   \n",
       "1            0.8673         25        1146        19990        17208   \n",
       "2            0.8062         26        1180        20306         9352   \n",
       "3            0.5125         17         830        17400         7393   \n",
       "4            0.5655         26        1171        26717         9817   \n",
       "...             ...        ...         ...          ...          ...   \n",
       "1289         0.5940         24        1020        12946        11730   \n",
       "1290         0.4107         21        1017        13028         4958   \n",
       "1291         0.6443         20         954        22570        12026   \n",
       "1292         0.6635         28        1225        37848         5998   \n",
       "1293         0.7997         25        1104        37379         3298   \n",
       "\n",
       "      AverageFacultySalary  MedianDebt  AverageAgeofEntry  MedianFamilyIncome  \\\n",
       "0                     7079     19500.0          20.629999             29039.0   \n",
       "1                    10170     16250.0          22.670000             34909.0   \n",
       "2                     9341     16500.0          23.190001             39766.0   \n",
       "3                     6557     15854.5          20.889999             24029.5   \n",
       "4                     9605     17750.0          20.770000             58976.0   \n",
       "...                    ...         ...                ...                 ...   \n",
       "1289                 14803     18983.0          20.120001             86510.0   \n",
       "1290                 14803     18983.0          20.120001             86510.0   \n",
       "1291                  8434     12500.0          24.850000             32103.0   \n",
       "1292                  7659     19000.0          21.209999             68233.0   \n",
       "1293                  6991     22313.0          24.750000             39241.0   \n",
       "\n",
       "      MedianEarnings  \n",
       "0              27000  \n",
       "1              37200  \n",
       "2              41500  \n",
       "3              22400  \n",
       "4              39200  \n",
       "...              ...  \n",
       "1289           49700  \n",
       "1290           49700  \n",
       "1291           35800  \n",
       "1292           72800  \n",
       "1293           37300  \n",
       "\n",
       "[1294 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/college.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {
    "zotero": {
     "27937/2LKXDMZD": {
      "URL": "https://resobscura.substack.com/p/simulating-history-with-chatgpt",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "id": "27937/2LKXDMZD",
      "system_id": "zotero|27937/2LKXDMZD",
      "title": "Simulating History with ChatGPT - by Benjamin Breen",
      "type": "article"
     },
     "27937/38C5LZCI": {
      "URL": "https://www.bbc.com/news/articles/cd11gzejgz4o",
      "abstract": "Google has defended the answers given by AI Overview, describing them as \"isolated examples\".",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "id": "27937/38C5LZCI",
      "language": "en-GB",
      "system_id": "zotero|27937/38C5LZCI",
      "title": "Google AI search tells users to glue pizza and eat rocks",
      "type": "webpage"
     },
     "27937/56EE9N63": {
      "URL": "http://arxiv.org/abs/2206.07682",
      "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Wei",
        "given": "Jason"
       },
       {
        "family": "Tay",
        "given": "Yi"
       },
       {
        "family": "Bommasani",
        "given": "Rishi"
       },
       {
        "family": "Raffel",
        "given": "Colin"
       },
       {
        "family": "Zoph",
        "given": "Barret"
       },
       {
        "family": "Borgeaud",
        "given": "Sebastian"
       },
       {
        "family": "Yogatama",
        "given": "Dani"
       },
       {
        "family": "Bosma",
        "given": "Maarten"
       },
       {
        "family": "Zhou",
        "given": "Denny"
       },
       {
        "family": "Metzler",
        "given": "Donald"
       },
       {
        "family": "Chi",
        "given": "Ed H."
       },
       {
        "family": "Hashimoto",
        "given": "Tatsunori"
       },
       {
        "family": "Vinyals",
        "given": "Oriol"
       },
       {
        "family": "Liang",
        "given": "Percy"
       },
       {
        "family": "Dean",
        "given": "Jeff"
       },
       {
        "family": "Fedus",
        "given": "William"
       }
      ],
      "id": "27937/56EE9N63",
      "issued": {
       "date-parts": [
        [
         2022,
         10,
         26
        ]
       ]
      },
      "note": "arXiv:2206.07682 [cs]",
      "number": "arXiv:2206.07682",
      "publisher": "arXiv",
      "system_id": "zotero|27937/56EE9N63",
      "title": "Emergent Abilities of Large Language Models",
      "type": "article"
     },
     "27937/58X69RSW": {
      "URL": "http://arxiv.org/abs/2404.14219",
      "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         14
        ]
       ]
      },
      "author": [
       {
        "family": "Abdin",
        "given": "Marah"
       },
       {
        "family": "Aneja",
        "given": "Jyoti"
       },
       {
        "family": "Awadalla",
        "given": "Hany"
       },
       {
        "family": "Awadallah",
        "given": "Ahmed"
       },
       {
        "family": "Awan",
        "given": "Ammar Ahmad"
       },
       {
        "family": "Bach",
        "given": "Nguyen"
       },
       {
        "family": "Bahree",
        "given": "Amit"
       },
       {
        "family": "Bakhtiari",
        "given": "Arash"
       },
       {
        "family": "Bao",
        "given": "Jianmin"
       },
       {
        "family": "Behl",
        "given": "Harkirat"
       },
       {
        "family": "Benhaim",
        "given": "Alon"
       },
       {
        "family": "Bilenko",
        "given": "Misha"
       },
       {
        "family": "Bjorck",
        "given": "Johan"
       },
       {
        "family": "Bubeck",
        "given": "Sébastien"
       },
       {
        "family": "Cai",
        "given": "Martin"
       },
       {
        "family": "Cai",
        "given": "Qin"
       },
       {
        "family": "Chaudhary",
        "given": "Vishrav"
       },
       {
        "family": "Chen",
        "given": "Dong"
       },
       {
        "family": "Chen",
        "given": "Dongdong"
       },
       {
        "family": "Chen",
        "given": "Weizhu"
       },
       {
        "family": "Chen",
        "given": "Yen-Chun"
       },
       {
        "family": "Chen",
        "given": "Yi-Ling"
       },
       {
        "family": "Cheng",
        "given": "Hao"
       },
       {
        "family": "Chopra",
        "given": "Parul"
       },
       {
        "family": "Dai",
        "given": "Xiyang"
       },
       {
        "family": "Dixon",
        "given": "Matthew"
       },
       {
        "family": "Eldan",
        "given": "Ronen"
       },
       {
        "family": "Fragoso",
        "given": "Victor"
       },
       {
        "family": "Gao",
        "given": "Jianfeng"
       },
       {
        "family": "Gao",
        "given": "Mei"
       },
       {
        "family": "Gao",
        "given": "Min"
       },
       {
        "family": "Garg",
        "given": "Amit"
       },
       {
        "family": "Giorno",
        "given": "Allie Del"
       },
       {
        "family": "Goswami",
        "given": "Abhishek"
       },
       {
        "family": "Gunasekar",
        "given": "Suriya"
       },
       {
        "family": "Haider",
        "given": "Emman"
       },
       {
        "family": "Hao",
        "given": "Junheng"
       },
       {
        "family": "Hewett",
        "given": "Russell J."
       },
       {
        "family": "Hu",
        "given": "Wenxiang"
       },
       {
        "family": "Huynh",
        "given": "Jamie"
       },
       {
        "family": "Iter",
        "given": "Dan"
       },
       {
        "family": "Jacobs",
        "given": "Sam Ade"
       },
       {
        "family": "Javaheripi",
        "given": "Mojan"
       },
       {
        "family": "Jin",
        "given": "Xin"
       },
       {
        "family": "Karampatziakis",
        "given": "Nikos"
       },
       {
        "family": "Kauffmann",
        "given": "Piero"
       },
       {
        "family": "Khademi",
        "given": "Mahoud"
       },
       {
        "family": "Kim",
        "given": "Dongwoo"
       },
       {
        "family": "Kim",
        "given": "Young Jin"
       },
       {
        "family": "Kurilenko",
        "given": "Lev"
       },
       {
        "family": "Lee",
        "given": "James R."
       },
       {
        "family": "Lee",
        "given": "Yin Tat"
       },
       {
        "family": "Li",
        "given": "Yuanzhi"
       },
       {
        "family": "Li",
        "given": "Yunsheng"
       },
       {
        "family": "Liang",
        "given": "Chen"
       },
       {
        "family": "Liden",
        "given": "Lars"
       },
       {
        "family": "Lin",
        "given": "Xihui"
       },
       {
        "family": "Lin",
        "given": "Zeqi"
       },
       {
        "family": "Liu",
        "given": "Ce"
       },
       {
        "family": "Liu",
        "given": "Liyuan"
       },
       {
        "family": "Liu",
        "given": "Mengchen"
       },
       {
        "family": "Liu",
        "given": "Weishung"
       },
       {
        "family": "Liu",
        "given": "Xiaodong"
       },
       {
        "family": "Luo",
        "given": "Chong"
       },
       {
        "family": "Madan",
        "given": "Piyush"
       },
       {
        "family": "Mahmoudzadeh",
        "given": "Ali"
       },
       {
        "family": "Majercak",
        "given": "David"
       },
       {
        "family": "Mazzola",
        "given": "Matt"
       },
       {
        "family": "Mendes",
        "given": "Caio César Teodoro"
       },
       {
        "family": "Mitra",
        "given": "Arindam"
       },
       {
        "family": "Modi",
        "given": "Hardik"
       },
       {
        "family": "Nguyen",
        "given": "Anh"
       },
       {
        "family": "Norick",
        "given": "Brandon"
       },
       {
        "family": "Patra",
        "given": "Barun"
       },
       {
        "family": "Perez-Becker",
        "given": "Daniel"
       },
       {
        "family": "Portet",
        "given": "Thomas"
       },
       {
        "family": "Pryzant",
        "given": "Reid"
       },
       {
        "family": "Qin",
        "given": "Heyang"
       },
       {
        "family": "Radmilac",
        "given": "Marko"
       },
       {
        "family": "Ren",
        "given": "Liliang"
       },
       {
        "family": "Rosa",
        "given": "Gustavo de"
       },
       {
        "family": "Rosset",
        "given": "Corby"
       },
       {
        "family": "Roy",
        "given": "Sambudha"
       },
       {
        "family": "Ruwase",
        "given": "Olatunji"
       },
       {
        "family": "Saarikivi",
        "given": "Olli"
       },
       {
        "family": "Saied",
        "given": "Amin"
       },
       {
        "family": "Salim",
        "given": "Adil"
       },
       {
        "family": "Santacroce",
        "given": "Michael"
       },
       {
        "family": "Shah",
        "given": "Shital"
       },
       {
        "family": "Shang",
        "given": "Ning"
       },
       {
        "family": "Sharma",
        "given": "Hiteshi"
       },
       {
        "family": "Shen",
        "given": "Yelong"
       },
       {
        "family": "Shukla",
        "given": "Swadheen"
       },
       {
        "family": "Song",
        "given": "Xia"
       },
       {
        "family": "Tanaka",
        "given": "Masahiro"
       },
       {
        "family": "Tupini",
        "given": "Andrea"
       },
       {
        "family": "Vaddamanu",
        "given": "Praneetha"
       },
       {
        "family": "Wang",
        "given": "Chunyu"
       },
       {
        "family": "Wang",
        "given": "Guanhua"
       },
       {
        "family": "Wang",
        "given": "Lijuan"
       },
       {
        "family": "Wang",
        "given": "Shuohang"
       },
       {
        "family": "Wang",
        "given": "Xin"
       },
       {
        "family": "Wang",
        "given": "Yu"
       },
       {
        "family": "Ward",
        "given": "Rachel"
       },
       {
        "family": "Wen",
        "given": "Wen"
       },
       {
        "family": "Witte",
        "given": "Philipp"
       },
       {
        "family": "Wu",
        "given": "Haiping"
       },
       {
        "family": "Wu",
        "given": "Xiaoxia"
       },
       {
        "family": "Wyatt",
        "given": "Michael"
       },
       {
        "family": "Xiao",
        "given": "Bin"
       },
       {
        "family": "Xu",
        "given": "Can"
       },
       {
        "family": "Xu",
        "given": "Jiahang"
       },
       {
        "family": "Xu",
        "given": "Weijian"
       },
       {
        "family": "Xue",
        "given": "Jilong"
       },
       {
        "family": "Yadav",
        "given": "Sonali"
       },
       {
        "family": "Yang",
        "given": "Fan"
       },
       {
        "family": "Yang",
        "given": "Jianwei"
       },
       {
        "family": "Yang",
        "given": "Yifan"
       },
       {
        "family": "Yang",
        "given": "Ziyi"
       },
       {
        "family": "Yu",
        "given": "Donghan"
       },
       {
        "family": "Yuan",
        "given": "Lu"
       },
       {
        "family": "Zhang",
        "given": "Chenruidong"
       },
       {
        "family": "Zhang",
        "given": "Cyril"
       },
       {
        "family": "Zhang",
        "given": "Jianwen"
       },
       {
        "family": "Zhang",
        "given": "Li Lyna"
       },
       {
        "family": "Zhang",
        "given": "Yi"
       },
       {
        "family": "Zhang",
        "given": "Yue"
       },
       {
        "family": "Zhang",
        "given": "Yunan"
       },
       {
        "family": "Zhou",
        "given": "Xiren"
       }
      ],
      "id": "27937/58X69RSW",
      "issued": {
       "date-parts": [
        [
         2024,
         8,
         30
        ]
       ]
      },
      "note": "arXiv:2404.14219",
      "number": "arXiv:2404.14219",
      "publisher": "arXiv",
      "shortTitle": "Phi-3 Technical Report",
      "system_id": "zotero|27937/58X69RSW",
      "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
      "type": "article"
     },
     "27937/5AL5LZ2K": {
      "URL": "https://dr-hutchinson-what-do-ais-know-about-history-app-i3l5jo.streamlit.app/",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         1
        ]
       ]
      },
      "id": "27937/5AL5LZ2K",
      "system_id": "zotero|27937/5AL5LZ2K",
      "title": "What Do AIs Know About History? A Digital History Experiment by Daniel Hutchinson · Streamlit",
      "type": "webpage"
     },
     "27937/5ED45HQE": {
      "URL": "https://univ-rochelle.hal.science/hal-04662000",
      "abstract": "The efficacy of large-scale language models (LLMs) as few-shot learners has dominated the field of natural language processing, achieving state-of-the-art performance in most tasks, including named entity recognition (NER) for contemporary texts. However, exploration of NER in historical collections (e.g., historical newspapers and classical commentaries) remains limited. This presents a greater challenge as historical texts are often noisy due to storage conditions, OCR extraction, and spelling variation. In this paper, we conduct an empirical evaluation comparing different Instruct variants of open-access and open-sourced LLMs using prompt engineering through deductive (with guidelines) and inductive (without guidelines) approaches against the fully supervised benchmarks. In addition, we study how the interaction between the Instruct model and the user impacts the entity prediction. We conduct reproducible experiments using an easy-to-implement mechanism on publicly available historical collections covering three languages (i.e., English, French, and German) with code-switching on Ancient Greek and four open Instruct models. The results show that Instruct models encounter multiple difficulties handling the noisy input documents, scoring lower than fine-tuned dedicated NER systems, yet the resulting predictions provide entities that can be used in further tagging processes by human annotators.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "González-Gallardo",
        "given": "Carlos-Emiliano"
       },
       {
        "family": "Hanh",
        "given": "Tran Thi Hong"
       },
       {
        "family": "Hamdi",
        "given": "Ahmed"
       },
       {
        "family": "Doucet",
        "given": "Antoine"
       }
      ],
      "event": "The 28th International Conference on Theory and Practice of Digital Libraries",
      "id": "27937/5ED45HQE",
      "issued": {
       "date-parts": [
        [
         2024,
         9,
         24
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/5ED45HQE",
      "title": "Leveraging Open Large Language Models for Historical Named Entity Recognition",
      "type": "paper-conference"
     },
     "27937/5G5LJCLC": {
      "URL": "https://github.com/dair-ai/Prompt-Engineering-Guide",
      "abstract": "🐙 Guides, papers, lecture, notebooks and resources for prompt engineering",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Saravia",
        "given": "Elvis"
       }
      ],
      "id": "27937/5G5LJCLC",
      "issued": {
       "date-parts": [
        [
         2022,
         12
        ]
       ]
      },
      "note": "Publication Title: https://github.com/dair-ai/Prompt-Engineering-Guide\noriginal-date: 2022-12-16T16:04:50Z",
      "system_id": "zotero|27937/5G5LJCLC",
      "title": "Prompt Engineering Guide",
      "type": "book"
     },
     "27937/5GTQD5W9": {
      "URL": "https://excavating.ai",
      "abstract": "An investigation into the politics of training sets, and the fundamental problems with classifying humans.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "container-title": "-",
      "id": "27937/5GTQD5W9",
      "language": "en-US",
      "system_id": "zotero|27937/5GTQD5W9",
      "title": "Excavating AI",
      "type": "webpage"
     },
     "27937/5YDNQS4V": {
      "URL": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/",
      "abstract": "Algorithms must be responsibly created to avoid discrimination and unethical applications.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Barton",
        "given": "Nicol Turner Lee, Paul Resnick, and Genie"
       }
      ],
      "container-title": "Brookings",
      "id": "27937/5YDNQS4V",
      "issued": {
       "date-parts": [
        [
         2019,
         5,
         22
        ]
       ]
      },
      "language": "en-US",
      "shortTitle": "Algorithmic bias detection and mitigation",
      "system_id": "zotero|27937/5YDNQS4V",
      "title": "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms",
      "type": "post-weblog"
     },
     "27937/68YHDUH6": {
      "URL": "https://blog.ehri-project.eu/2018/08/27/named-entity-recognition/",
      "abstract": "The European Holocaust Research Infrastructure Document Blog",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         15
        ]
       ]
      },
      "id": "27937/68YHDUH6",
      "issued": {
       "date-parts": [
        [
         2018,
         8,
         27
        ]
       ]
      },
      "language": "en-GB",
      "system_id": "zotero|27937/68YHDUH6",
      "title": "Using Named Entity Recognition to Enhance Access to a Museum Catalog – Document Blog",
      "type": "post-weblog"
     },
     "27937/7D6BEHLB": {
      "URL": "https://programminghistorian.org/en/lessons/understanding-creating-word-embeddings",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Blankenship",
        "given": "Avery"
       },
       {
        "family": "Connell",
        "given": "Sarah"
       },
       {
        "family": "Dombrowski",
        "given": "Quinn"
       }
      ],
      "container-title": "Programming Historian",
      "id": "27937/7D6BEHLB",
      "issued": {
       "date-parts": [
        [
         2024,
         1,
         31
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/7D6BEHLB",
      "title": "Understanding and Creating Word Embeddings",
      "type": "article-journal"
     },
     "27937/7VHKCH3M": {
      "URL": "http://arxiv.org/abs/2212.04356",
      "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         29
        ]
       ]
      },
      "author": [
       {
        "family": "Radford",
        "given": "Alec"
       },
       {
        "family": "Kim",
        "given": "Jong Wook"
       },
       {
        "family": "Xu",
        "given": "Tao"
       },
       {
        "family": "Brockman",
        "given": "Greg"
       },
       {
        "family": "McLeavey",
        "given": "Christine"
       },
       {
        "family": "Sutskever",
        "given": "Ilya"
       }
      ],
      "id": "27937/7VHKCH3M",
      "issued": {
       "date-parts": [
        [
         2022,
         12,
         6
        ]
       ]
      },
      "note": "arXiv:2212.04356 [cs, eess]",
      "number": "arXiv:2212.04356",
      "publisher": "arXiv",
      "system_id": "zotero|27937/7VHKCH3M",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "type": "article"
     },
     "27937/82YIALT5": {
      "DOI": "10.48550/arXiv.2409.00128",
      "URL": "http://arxiv.org/abs/2409.00128",
      "abstract": "Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) like GPT-4 have shown promise in replicating human-like responses in various psychological experiments. However, the extent to which LLMs can effectively replace human subjects across diverse experimental contexts remains unclear. Here, we conduct a large-scale study replicating 154 psychological experiments from top social science journals with 618 main effects and 138 interaction effects using GPT-4 as a simulated participant. We find that GPT-4 successfully replicates 76.0 percent of main effects and 47.0 percent of interaction effects observed in the original studies, closely mirroring human responses in both direction and significance. However, only 19.44 percent of GPT-4's replicated confidence intervals contain the original effect sizes, with the majority of replicated effect sizes exceeding the 95 percent confidence interval of the original studies. Additionally, there is a 71.6 percent rate of unexpected significant results where the original studies reported null findings, suggesting potential overestimation or false positives. Our results demonstrate the potential of LLMs as powerful tools in psychological research but also emphasize the need for caution in interpreting AI-driven findings. While LLMs can complement human studies, they cannot yet fully replace the nuanced insights provided by human subjects.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Cui",
        "given": "Ziyan"
       },
       {
        "family": "Li",
        "given": "Ning"
       },
       {
        "family": "Zhou",
        "given": "Huaikang"
       }
      ],
      "id": "27937/82YIALT5",
      "issued": {
       "date-parts": [
        [
         2024,
         9,
         4
        ]
       ]
      },
      "note": "arXiv:2409.00128",
      "number": "arXiv:2409.00128",
      "publisher": "arXiv",
      "shortTitle": "Can AI Replace Human Subjects?",
      "system_id": "zotero|27937/82YIALT5",
      "title": "Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs",
      "type": "article"
     },
     "27937/9CA225ZV": {
      "DOI": "10.1038/s44184-024-00056-z",
      "URL": "https://www.nature.com/articles/s44184-024-00056-z",
      "abstract": "Large language models (LLMs) such as Open AI’s GPT-4 (which power ChatGPT) and Google’s Gemini, built on artificial intelligence, hold immense potential to support, augment, or even eventually automate psychotherapy. Enthusiasm about such applications is mounting in the field as well as industry. These developments promise to address insufficient mental healthcare system capacity and scale individual access to personalized treatments. However, clinical psychology is an uncommonly high stakes application domain for AI systems, as responsible and evidence-based therapy requires nuanced expertise. This paper provides a roadmap for the ambitious yet responsible application of clinical LLMs in psychotherapy. First, a technical overview of clinical LLMs is presented. Second, the stages of integration of LLMs into psychotherapy are discussed while highlighting parallels to the development of autonomous vehicle technology. Third, potential applications of LLMs in clinical care, training, and research are discussed, highlighting areas of risk given the complex nature of psychotherapy. Fourth, recommendations for the responsible development and evaluation of clinical LLMs are provided, which include centering clinical science, involving robust interdisciplinary collaboration, and attending to issues like assessment, risk detection, transparency, and bias. Lastly, a vision is outlined for how LLMs might enable a new generation of studies of evidence-based interventions at scale, and how these studies may challenge assumptions about psychotherapy.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Stade",
        "given": "Elizabeth C."
       },
       {
        "family": "Stirman",
        "given": "Shannon Wiltsey"
       },
       {
        "family": "Ungar",
        "given": "Lyle H."
       },
       {
        "family": "Boland",
        "given": "Cody L."
       },
       {
        "family": "Schwartz",
        "given": "H. Andrew"
       },
       {
        "family": "Yaden",
        "given": "David B."
       },
       {
        "family": "Sedoc",
        "given": "João"
       },
       {
        "family": "DeRubeis",
        "given": "Robert J."
       },
       {
        "family": "Willer",
        "given": "Robb"
       },
       {
        "family": "Eichstaedt",
        "given": "Johannes C."
       }
      ],
      "container-title": "npj Mental Health Research",
      "id": "27937/9CA225ZV",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2024,
         4,
         2
        ]
       ]
      },
      "journalAbbreviation": "npj Mental Health Res",
      "language": "en",
      "note": "Publisher: Nature Publishing Group",
      "page": "1-12",
      "shortTitle": "Large language models could change the future of behavioral healthcare",
      "system_id": "zotero|27937/9CA225ZV",
      "title": "Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation",
      "type": "article-journal",
      "volume": "3"
     },
     "27937/9GQG6VFM": {
      "DOI": "10.1145/3571730",
      "URL": "http://arxiv.org/abs/2202.03629",
      "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         3
        ]
       ]
      },
      "author": [
       {
        "family": "Ji",
        "given": "Ziwei"
       },
       {
        "family": "Lee",
        "given": "Nayeon"
       },
       {
        "family": "Frieske",
        "given": "Rita"
       },
       {
        "family": "Yu",
        "given": "Tiezheng"
       },
       {
        "family": "Su",
        "given": "Dan"
       },
       {
        "family": "Xu",
        "given": "Yan"
       },
       {
        "family": "Ishii",
        "given": "Etsuko"
       },
       {
        "family": "Bang",
        "given": "Yejin"
       },
       {
        "family": "Dai",
        "given": "Wenliang"
       },
       {
        "family": "Madotto",
        "given": "Andrea"
       },
       {
        "family": "Fung",
        "given": "Pascale"
       }
      ],
      "container-title": "ACM Computing Surveys",
      "id": "27937/9GQG6VFM",
      "issue": "12",
      "issued": {
       "date-parts": [
        [
         2023,
         12,
         31
        ]
       ]
      },
      "journalAbbreviation": "ACM Comput. Surv.",
      "note": "arXiv:2202.03629 [cs]",
      "page": "1-38",
      "system_id": "zotero|27937/9GQG6VFM",
      "title": "Survey of Hallucination in Natural Language Generation",
      "type": "article-journal",
      "volume": "55"
     },
     "27937/9HF43E58": {
      "URL": "https://www.promptingguide.ai/",
      "abstract": "A Comprehensive Overview of Prompt Engineering",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "id": "27937/9HF43E58",
      "issued": {
       "date-parts": [
        [
         2024,
         9,
         19
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/9HF43E58",
      "title": "Prompt Engineering Guide",
      "type": "webpage"
     },
     "27937/9T2I7QLM": {
      "URL": "http://arxiv.org/abs/1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Vaswani",
        "given": "Ashish"
       },
       {
        "family": "Shazeer",
        "given": "Noam"
       },
       {
        "family": "Parmar",
        "given": "Niki"
       },
       {
        "family": "Uszkoreit",
        "given": "Jakob"
       },
       {
        "family": "Jones",
        "given": "Llion"
       },
       {
        "family": "Gomez",
        "given": "Aidan N."
       },
       {
        "family": "Kaiser",
        "given": "Lukasz"
       },
       {
        "family": "Polosukhin",
        "given": "Illia"
       }
      ],
      "id": "27937/9T2I7QLM",
      "issued": {
       "date-parts": [
        [
         2023,
         8,
         2
        ]
       ]
      },
      "note": "arXiv:1706.03762",
      "number": "arXiv:1706.03762",
      "publisher": "arXiv",
      "system_id": "zotero|27937/9T2I7QLM",
      "title": "Attention Is All You Need",
      "type": "article"
     },
     "27937/A834FRJL": {
      "URL": "https://github.com/hendrycks/test",
      "abstract": "Measuring Massive Multitask Language Understanding | ICLR 2021",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "author": [
       {
        "family": "Hendrycks",
        "given": "Dan"
       }
      ],
      "id": "27937/A834FRJL",
      "issued": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "note": "original-date: 2020-09-07T23:02:57Z",
      "system_id": "zotero|27937/A834FRJL",
      "title": "Measuring Massive Multitask Language Understanding",
      "type": "book"
     },
     "27937/BD8996H7": {
      "URL": "https://reports.collegeboard.org/media/pdf/program-summary-report-2022.pdf",
      "container-title": "AP Exam Administration Data Archive",
      "id": "27937/BD8996H7",
      "issued": {
       "date-parts": [
        [
         2022
        ]
       ]
      },
      "system_id": "zotero|27937/BD8996H7",
      "title": "Program Summary Report",
      "type": "webpage"
     },
     "27937/BN44JR8V": {
      "URL": "https://hcommons.org/deposits/item/hc:41973/",
      "abstract": "As neural language models begin to change aspects of everyday life, they understandably attract criticism. This position paper was commissioned for a roundtable at Princeton University, dedicated to one of the most influential critiques: \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\" by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. \n\nMy paper agrees that neural language models pose a variety of dangers, starting with and not limited to the list in \"Stochastic Parrots.\" But to understand those dangers, I think we need to look beyond the premise that these models mimic \"language understanding\" on an individual level. That may have been what linguists and computer scientists intended them to do. But the models' actual potential (for both good and ill) is more interesting, and will be easier to grasp if we approach them as models of culture. Science-fictional scenarios about robots that become autonomous (or remain mere \"parrots\") are less useful here than humanistic cultural theory.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Underwood",
        "given": "Ted"
       }
      ],
      "id": "27937/BN44JR8V",
      "issued": {
       "date-parts": [
        [
         "2021",
         10,
         20
        ]
       ]
      },
      "language": "en-US",
      "system_id": "zotero|27937/BN44JR8V",
      "title": "Mapping the Latent Spaces of Culture",
      "type": "article-journal"
     },
     "27937/BVBZMR66": {
      "DOI": "10.48550/arXiv.2212.14402",
      "URL": "http://arxiv.org/abs/2212.14402",
      "abstract": "Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as \"the Bar Exam,\" as a precondition for law practice. To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school. In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try. In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in \"AI?\" In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam. While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance. For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Katz",
        "given": "Daniel Martin"
       }
      ],
      "id": "27937/BVBZMR66",
      "issued": {
       "date-parts": [
        [
         2022,
         12,
         29
        ]
       ]
      },
      "note": "arXiv:2212.14402",
      "number": "arXiv:2212.14402",
      "publisher": "arXiv",
      "system_id": "zotero|27937/BVBZMR66",
      "title": "GPT Takes the Bar Exam",
      "type": "article"
     },
     "27937/BXZEP65G": {
      "URL": "https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "id": "27937/BXZEP65G",
      "system_id": "zotero|27937/BXZEP65G",
      "title": "What is LaMDA and What Does it Want? | by Blake Lemoine | Medium",
      "type": "webpage"
     },
     "27937/CJYNFHVI": {
      "DOI": "10.1086/ahr/108.3.735",
      "URL": "https://doi.org/10.1086/ahr/108.3.735",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Rosenzweig",
        "given": "Roy"
       }
      ],
      "container-title": "The American Historical Review",
      "id": "27937/CJYNFHVI",
      "issue": "3",
      "issued": {
       "date-parts": [
        [
         2003,
         6,
         1
        ]
       ]
      },
      "journalAbbreviation": "The American Historical Review",
      "page": "735-762",
      "shortTitle": "Scarcity or Abundance?",
      "system_id": "zotero|27937/CJYNFHVI",
      "title": "Scarcity or Abundance? Preserving the Past in a Digital Era",
      "type": "article-journal",
      "volume": "108"
     },
     "27937/CXURVMLQ": {
      "DOI": "10.37514/TWR-J.2023.1.1.02",
      "URL": "https://wac.colostate.edu/repository/collections/textgened/",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Vee",
        "given": "Annette"
       },
       {
        "family": "Laquintano",
        "given": "Tim"
       },
       {
        "family": "Schnitzler",
        "given": "Carly"
       }
      ],
      "container-title": "The WAC Repository",
      "id": "27937/CXURVMLQ",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2023
        ]
       ]
      },
      "language": "en",
      "page": "1-100",
      "system_id": "zotero|27937/CXURVMLQ",
      "title": "TextGenEd Exhibit",
      "type": "article-journal",
      "volume": "1"
     },
     "27937/ECQ4J8E9": {
      "DOI": "10.48550/arXiv.2005.11401",
      "URL": "http://arxiv.org/abs/2005.11401",
      "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Lewis",
        "given": "Patrick"
       },
       {
        "family": "Perez",
        "given": "Ethan"
       },
       {
        "family": "Piktus",
        "given": "Aleksandra"
       },
       {
        "family": "Petroni",
        "given": "Fabio"
       },
       {
        "family": "Karpukhin",
        "given": "Vladimir"
       },
       {
        "family": "Goyal",
        "given": "Naman"
       },
       {
        "family": "Küttler",
        "given": "Heinrich"
       },
       {
        "family": "Lewis",
        "given": "Mike"
       },
       {
        "family": "Yih",
        "given": "Wen-tau"
       },
       {
        "family": "Rocktäschel",
        "given": "Tim"
       },
       {
        "family": "Riedel",
        "given": "Sebastian"
       },
       {
        "family": "Kiela",
        "given": "Douwe"
       }
      ],
      "id": "27937/ECQ4J8E9",
      "issued": {
       "date-parts": [
        [
         2021,
         4,
         12
        ]
       ]
      },
      "note": "arXiv:2005.11401",
      "number": "arXiv:2005.11401",
      "publisher": "arXiv",
      "system_id": "zotero|27937/ECQ4J8E9",
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "type": "article"
     },
     "27937/ENMGUHSL": {
      "URL": "https://github.com/lifan0127/ai-research-assistant",
      "abstract": "Aria is Your AI Research Assistant Powered by GPT Large Language Models",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Li",
        "given": "Fan"
       }
      ],
      "id": "27937/ENMGUHSL",
      "issued": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "note": "original-date: 2023-03-28T01:30:13Z",
      "system_id": "zotero|27937/ENMGUHSL",
      "title": "lifan0127/ai-research-assistant",
      "type": "book"
     },
     "27937/EZNK3CE3": {
      "URL": "http://site.ebrary.com/id/10158052",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "McCorduck",
        "given": "Pamela"
       }
      ],
      "edition": "25th anniversary update",
      "event-place": "Natick, Mass.",
      "id": "27937/EZNK3CE3",
      "issued": {
       "date-parts": [
        [
         2004
        ]
       ]
      },
      "language": "eng",
      "note": "OCLC: 748860627",
      "number-of-pages": "1",
      "publisher": "A.K. Peters",
      "publisher-place": "Natick, Mass.",
      "system_id": "zotero|27937/EZNK3CE3",
      "title": "Machines who think a personal inquiry into the history and prospects of artificial intelligence",
      "type": "book"
     },
     "27937/FMW5DCWM": {
      "URL": "http://arxiv.org/abs/2204.02311",
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Chowdhery",
        "given": "Aakanksha"
       },
       {
        "family": "Narang",
        "given": "Sharan"
       },
       {
        "family": "Devlin",
        "given": "Jacob"
       },
       {
        "family": "Bosma",
        "given": "Maarten"
       },
       {
        "family": "Mishra",
        "given": "Gaurav"
       },
       {
        "family": "Roberts",
        "given": "Adam"
       },
       {
        "family": "Barham",
        "given": "Paul"
       },
       {
        "family": "Chung",
        "given": "Hyung Won"
       },
       {
        "family": "Sutton",
        "given": "Charles"
       },
       {
        "family": "Gehrmann",
        "given": "Sebastian"
       },
       {
        "family": "Schuh",
        "given": "Parker"
       },
       {
        "family": "Shi",
        "given": "Kensen"
       },
       {
        "family": "Tsvyashchenko",
        "given": "Sasha"
       },
       {
        "family": "Maynez",
        "given": "Joshua"
       },
       {
        "family": "Rao",
        "given": "Abhishek"
       },
       {
        "family": "Barnes",
        "given": "Parker"
       },
       {
        "family": "Tay",
        "given": "Yi"
       },
       {
        "family": "Shazeer",
        "given": "Noam"
       },
       {
        "family": "Prabhakaran",
        "given": "Vinodkumar"
       },
       {
        "family": "Reif",
        "given": "Emily"
       },
       {
        "family": "Du",
        "given": "Nan"
       },
       {
        "family": "Hutchinson",
        "given": "Ben"
       },
       {
        "family": "Pope",
        "given": "Reiner"
       },
       {
        "family": "Bradbury",
        "given": "James"
       },
       {
        "family": "Austin",
        "given": "Jacob"
       },
       {
        "family": "Isard",
        "given": "Michael"
       },
       {
        "family": "Gur-Ari",
        "given": "Guy"
       },
       {
        "family": "Yin",
        "given": "Pengcheng"
       },
       {
        "family": "Duke",
        "given": "Toju"
       },
       {
        "family": "Levskaya",
        "given": "Anselm"
       },
       {
        "family": "Ghemawat",
        "given": "Sanjay"
       },
       {
        "family": "Dev",
        "given": "Sunipa"
       },
       {
        "family": "Michalewski",
        "given": "Henryk"
       },
       {
        "family": "Garcia",
        "given": "Xavier"
       },
       {
        "family": "Misra",
        "given": "Vedant"
       },
       {
        "family": "Robinson",
        "given": "Kevin"
       },
       {
        "family": "Fedus",
        "given": "Liam"
       },
       {
        "family": "Zhou",
        "given": "Denny"
       },
       {
        "family": "Ippolito",
        "given": "Daphne"
       },
       {
        "family": "Luan",
        "given": "David"
       },
       {
        "family": "Lim",
        "given": "Hyeontaek"
       },
       {
        "family": "Zoph",
        "given": "Barret"
       },
       {
        "family": "Spiridonov",
        "given": "Alexander"
       },
       {
        "family": "Sepassi",
        "given": "Ryan"
       },
       {
        "family": "Dohan",
        "given": "David"
       },
       {
        "family": "Agrawal",
        "given": "Shivani"
       },
       {
        "family": "Omernick",
        "given": "Mark"
       },
       {
        "family": "Dai",
        "given": "Andrew M."
       },
       {
        "family": "Pillai",
        "given": "Thanumalayan Sankaranarayana"
       },
       {
        "family": "Pellat",
        "given": "Marie"
       },
       {
        "family": "Lewkowycz",
        "given": "Aitor"
       },
       {
        "family": "Moreira",
        "given": "Erica"
       },
       {
        "family": "Child",
        "given": "Rewon"
       },
       {
        "family": "Polozov",
        "given": "Oleksandr"
       },
       {
        "family": "Lee",
        "given": "Katherine"
       },
       {
        "family": "Zhou",
        "given": "Zongwei"
       },
       {
        "family": "Wang",
        "given": "Xuezhi"
       },
       {
        "family": "Saeta",
        "given": "Brennan"
       },
       {
        "family": "Diaz",
        "given": "Mark"
       },
       {
        "family": "Firat",
        "given": "Orhan"
       },
       {
        "family": "Catasta",
        "given": "Michele"
       },
       {
        "family": "Wei",
        "given": "Jason"
       },
       {
        "family": "Meier-Hellstern",
        "given": "Kathy"
       },
       {
        "family": "Eck",
        "given": "Douglas"
       },
       {
        "family": "Dean",
        "given": "Jeff"
       },
       {
        "family": "Petrov",
        "given": "Slav"
       },
       {
        "family": "Fiedel",
        "given": "Noah"
       }
      ],
      "id": "27937/FMW5DCWM",
      "issued": {
       "date-parts": [
        [
         2022,
         10,
         5
        ]
       ]
      },
      "note": "arXiv:2204.02311 [cs]",
      "number": "arXiv:2204.02311",
      "publisher": "arXiv",
      "shortTitle": "PaLM",
      "system_id": "zotero|27937/FMW5DCWM",
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "type": "article"
     },
     "27937/G5ESJ8NI": {
      "URL": "http://arxiv.org/abs/2203.15556",
      "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "author": [
       {
        "family": "Hoffmann",
        "given": "Jordan"
       },
       {
        "family": "Borgeaud",
        "given": "Sebastian"
       },
       {
        "family": "Mensch",
        "given": "Arthur"
       },
       {
        "family": "Buchatskaya",
        "given": "Elena"
       },
       {
        "family": "Cai",
        "given": "Trevor"
       },
       {
        "family": "Rutherford",
        "given": "Eliza"
       },
       {
        "family": "Casas",
        "given": "Diego de Las"
       },
       {
        "family": "Hendricks",
        "given": "Lisa Anne"
       },
       {
        "family": "Welbl",
        "given": "Johannes"
       },
       {
        "family": "Clark",
        "given": "Aidan"
       },
       {
        "family": "Hennigan",
        "given": "Tom"
       },
       {
        "family": "Noland",
        "given": "Eric"
       },
       {
        "family": "Millican",
        "given": "Katie"
       },
       {
        "family": "Driessche",
        "given": "George van den"
       },
       {
        "family": "Damoc",
        "given": "Bogdan"
       },
       {
        "family": "Guy",
        "given": "Aurelia"
       },
       {
        "family": "Osindero",
        "given": "Simon"
       },
       {
        "family": "Simonyan",
        "given": "Karen"
       },
       {
        "family": "Elsen",
        "given": "Erich"
       },
       {
        "family": "Rae",
        "given": "Jack W."
       },
       {
        "family": "Vinyals",
        "given": "Oriol"
       },
       {
        "family": "Sifre",
        "given": "Laurent"
       }
      ],
      "id": "27937/G5ESJ8NI",
      "issued": {
       "date-parts": [
        [
         2022,
         3,
         29
        ]
       ]
      },
      "note": "arXiv:2203.15556 [cs]",
      "number": "arXiv:2203.15556",
      "publisher": "arXiv",
      "system_id": "zotero|27937/G5ESJ8NI",
      "title": "Training Compute-Optimal Large Language Models",
      "type": "article"
     },
     "27937/GJ2EYJWT": {
      "URL": "https://huggingface.co/spaces/openai/whisper",
      "abstract": "Discover amazing ML apps made by the community",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         31
        ]
       ]
      },
      "id": "27937/GJ2EYJWT",
      "system_id": "zotero|27937/GJ2EYJWT",
      "title": "Whisper - a Hugging Face Space by openai",
      "type": "webpage"
     },
     "27937/GSIXPJ7P": {
      "URL": "https://crfm.stanford.edu/2024/05/01/helm-mmlu.html",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         1
        ]
       ]
      },
      "author": [
       {
        "family": "Mai",
        "given": "Yifan"
       },
       {
        "family": "Liang",
        "given": "Percy"
       }
      ],
      "container-title": "Center for Research on Foundation Models, Stanford University",
      "genre": "Blog",
      "id": "27937/GSIXPJ7P",
      "issued": {
       "date-parts": [
        [
         2024,
         5,
         1
        ]
       ]
      },
      "system_id": "zotero|27937/GSIXPJ7P",
      "title": "Massive Multitask Language Understanding (MMLU) on HELM",
      "type": "webpage"
     },
     "27937/HGR9QB96": {
      "URL": "https://millercenter.org/the-presidency/presidential-speeches",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "id": "27937/HGR9QB96",
      "issued": {
       "date-parts": [
        [
         2016,
         11,
         21
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/HGR9QB96",
      "title": "Presidential Speeches | Miller Center",
      "type": "webpage"
     },
     "27937/HIPL38QS": {
      "URL": "https://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html",
      "abstract": "Named entity recognition is an advantageous technique with an increasing presence in digital humanities. In theory, automatic detection and recovery of named entities can provide new ways of looking up unedited information in edited sources and can allow the parsing of a massive amount of data in a short time for supporting historical hypotheses. In this paper, we detail the implementation of a model for automatic named entity recognition in medieval Latin sources and we test its robustness on different datasets. Different models were trained on a vast dataset of Burgundian diplomatic charters from the 9th to 14th centuries and validated by using general and century ad hoc models tested on short sets of Parisian, English, Italian and Spanish charters. We present the results of cross-validation in each case and we discuss the implications of these results for the history of medieval place-names and personal names.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         15
        ]
       ]
      },
      "author": [
       {
        "family": "Chastang",
        "given": "Pierre"
       },
       {
        "family": "Aguilar",
        "given": "Sergio Torres"
       },
       {
        "family": "Tannier",
        "given": "Xavier"
       }
      ],
      "container-title": "Digital Humanities Quarterly",
      "id": "27937/HIPL38QS",
      "issue": "4",
      "issued": {
       "date-parts": [
        [
         2021
        ]
       ]
      },
      "system_id": "zotero|27937/HIPL38QS",
      "title": "A Named Entity Recognition Model for Medieval Latin Charters",
      "type": "article-journal",
      "volume": "15"
     },
     "27937/I2BKP7MN": {
      "URL": "https://www.css.cnrs.fr/using-whisper-to-transcribe-oral-interviews/",
      "abstract": "Site web de l'axe sciences sociales computationnelles du CREST-CNRS. Cours et tutoriels pour l'analyse des données numériques en sciences sociales.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Schultz",
        "given": "Emilien"
       }
      ],
      "id": "27937/I2BKP7MN",
      "issued": {
       "date-parts": [
        [
         2024,
         2,
         12
        ]
       ]
      },
      "language": "en-US",
      "system_id": "zotero|27937/I2BKP7MN",
      "title": "[Tutorial] Using Whisper to Transcribe Oral Interviews – CSS @ IPP",
      "type": "post-weblog"
     },
     "27937/I363EKXY": {
      "DOI": "10.48550/arXiv.2402.04559",
      "URL": "http://arxiv.org/abs/2402.04559",
      "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, particularly for GPT-4, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications of our discoveries for various scenarios where trust is paramount. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Xie",
        "given": "Chengxing"
       },
       {
        "family": "Chen",
        "given": "Canyu"
       },
       {
        "family": "Jia",
        "given": "Feiran"
       },
       {
        "family": "Ye",
        "given": "Ziyu"
       },
       {
        "family": "Shu",
        "given": "Kai"
       },
       {
        "family": "Bibi",
        "given": "Adel"
       },
       {
        "family": "Hu",
        "given": "Ziniu"
       },
       {
        "family": "Torr",
        "given": "Philip"
       },
       {
        "family": "Ghanem",
        "given": "Bernard"
       },
       {
        "family": "Li",
        "given": "Guohao"
       }
      ],
      "id": "27937/I363EKXY",
      "issued": {
       "date-parts": [
        [
         2024,
         3,
         10
        ]
       ]
      },
      "note": "arXiv:2402.04559",
      "number": "arXiv:2402.04559",
      "publisher": "arXiv",
      "system_id": "zotero|27937/I363EKXY",
      "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
      "type": "article"
     },
     "27937/IEQ8GAVU": {
      "ISBN": "9781479837243",
      "abstract": "A revealing look at how negative biases against women of color are embedded in search engine results and algorithms Run a Google search for “black girls”―what will you find? “Big Booty” and other sexually explicit terms are likely to come up as top search terms. But, if you type in “white girls,” the results are radically different. The suggested porn sites and un-moderated discussions about “why black women are so sassy” or “why black women are so angry” presents a disturbing portrait of black womanhood in modern society.In Algorithms of Oppression, Safiya Umoja Noble challenges the idea that search engines like Google offer an equal playing field for all forms of ideas, identities, and activities. Data discrimination is a real social problem; Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of color, specifically women of color.Through an analysis of textual and media searches as well as extensive research on paid online advertising, Noble exposes a culture of racism and sexism in the way discoverability is created online. As search engines and their related companies grow in importance―operating as a source for email, a major vehicle for primary and secondary school learning, and beyond―understanding and reversing these disquieting trends and discriminatory practices is of utmost importance.An original, surprising and, at times, disturbing account of bias on the internet, Algorithms of Oppression contributes to our understanding of how racism is created, maintained, and disseminated in the 21st century.",
      "author": [
       {
        "family": "Noble",
        "given": "Safiya Umoja"
       }
      ],
      "edition": "Illustrated edition",
      "event-place": "New York",
      "id": "27937/IEQ8GAVU",
      "issued": {
       "date-parts": [
        [
         2018,
         2,
         20
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "248",
      "publisher": "NYU Press",
      "publisher-place": "New York",
      "shortTitle": "Algorithms of Oppression",
      "system_id": "zotero|27937/IEQ8GAVU",
      "title": "Algorithms of Oppression: How Search Engines Reinforce Racism",
      "type": "book"
     },
     "27937/IJWETJM7": {
      "URL": "https://docsouth.unc.edu/sohp/A-0339/menu.html",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         31
        ]
       ]
      },
      "id": "27937/IJWETJM7",
      "system_id": "zotero|27937/IJWETJM7",
      "title": "John Hope Franklin and John Egerton, conducted by Oral History Interview with John Hope Franklin, July 27, 1990. Interview A-0339. Southern Oral History Program Collection (#4007).",
      "type": "webpage"
     },
     "27937/JV9GGCQA": {
      "URL": "https://huggingface.co/blog/Pclanglais/post-ocr-correction",
      "abstract": "A Blog post by Pierre-Carl Langlais on Hugging Face",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         14
        ]
       ]
      },
      "id": "27937/JV9GGCQA",
      "shortTitle": "Post-OCR-Correction",
      "system_id": "zotero|27937/JV9GGCQA",
      "title": "Post-OCR-Correction: 1 billion words dataset of automated OCR correction by LLM",
      "type": "webpage"
     },
     "27937/KKDPZJYW": {
      "DOI": "10.21437/Interspeech.2023-872",
      "URL": "https://www.isca-archive.org/interspeech_2023/lehecka23_interspeech.html",
      "abstract": "This paper is a step forward in our effort to make vast oral history archives more accessible to the public and researchers by breaking down the decoding barriers between the knowledge encoded in the spoken testimonies and users who want to search for the information of their interest. We present new Transformer-based monolingual models suitable for speech recognition of oral history archives in English, German, and Czech. Our experiments show that although the all-purpose speech recognition systems have recently made tremendous progress, the transcription of oral history archives is still a challenging task for them; our tailored models significantly outperformed larger public multilingual models and scored new stateof-the-art results on all tested datasets. Due to the 2-phase finetuning process, our models are robust and can be used for oral history archives of various domains. We publicly release our models within a public speech recognition service.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         10
        ]
       ]
      },
      "author": [
       {
        "family": "Lehečka",
        "given": "Jan"
       },
       {
        "family": "Švec",
        "given": "Jan"
       },
       {
        "family": "Psutka",
        "given": "Josef V."
       },
       {
        "family": "Ircing",
        "given": "Pavel"
       }
      ],
      "container-title": "INTERSPEECH 2023",
      "event": "INTERSPEECH 2023",
      "id": "27937/KKDPZJYW",
      "issued": {
       "date-parts": [
        [
         2023,
         8,
         20
        ]
       ]
      },
      "language": "en",
      "page": "201-205",
      "publisher": "ISCA",
      "system_id": "zotero|27937/KKDPZJYW",
      "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
      "type": "paper-conference"
     },
     "27937/KNEK45E4": {
      "URL": "http://arxiv.org/abs/2005.14165",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Brown",
        "given": "Tom B."
       },
       {
        "family": "Mann",
        "given": "Benjamin"
       },
       {
        "family": "Ryder",
        "given": "Nick"
       },
       {
        "family": "Subbiah",
        "given": "Melanie"
       },
       {
        "family": "Kaplan",
        "given": "Jared"
       },
       {
        "family": "Dhariwal",
        "given": "Prafulla"
       },
       {
        "family": "Neelakantan",
        "given": "Arvind"
       },
       {
        "family": "Shyam",
        "given": "Pranav"
       },
       {
        "family": "Sastry",
        "given": "Girish"
       },
       {
        "family": "Askell",
        "given": "Amanda"
       },
       {
        "family": "Agarwal",
        "given": "Sandhini"
       },
       {
        "family": "Herbert-Voss",
        "given": "Ariel"
       },
       {
        "family": "Krueger",
        "given": "Gretchen"
       },
       {
        "family": "Henighan",
        "given": "Tom"
       },
       {
        "family": "Child",
        "given": "Rewon"
       },
       {
        "family": "Ramesh",
        "given": "Aditya"
       },
       {
        "family": "Ziegler",
        "given": "Daniel M."
       },
       {
        "family": "Wu",
        "given": "Jeffrey"
       },
       {
        "family": "Winter",
        "given": "Clemens"
       },
       {
        "family": "Hesse",
        "given": "Christopher"
       },
       {
        "family": "Chen",
        "given": "Mark"
       },
       {
        "family": "Sigler",
        "given": "Eric"
       },
       {
        "family": "Litwin",
        "given": "Mateusz"
       },
       {
        "family": "Gray",
        "given": "Scott"
       },
       {
        "family": "Chess",
        "given": "Benjamin"
       },
       {
        "family": "Clark",
        "given": "Jack"
       },
       {
        "family": "Berner",
        "given": "Christopher"
       },
       {
        "family": "McCandlish",
        "given": "Sam"
       },
       {
        "family": "Radford",
        "given": "Alec"
       },
       {
        "family": "Sutskever",
        "given": "Ilya"
       },
       {
        "family": "Amodei",
        "given": "Dario"
       }
      ],
      "id": "27937/KNEK45E4",
      "issued": {
       "date-parts": [
        [
         2020,
         7,
         22
        ]
       ]
      },
      "note": "arXiv:2005.14165 [cs]",
      "number": "arXiv:2005.14165",
      "publisher": "arXiv",
      "system_id": "zotero|27937/KNEK45E4",
      "title": "Language Models are Few-Shot Learners",
      "type": "article"
     },
     "27937/KPPP2BAQ": {
      "URL": "https://www.nbcnews.com/tech/tech-news/chatgpt-gpt-chat-bot-ai-hitler-historical-figures-open-rcna66531",
      "abstract": "The Historical Figures app is available in Apple's App Store and lets you chat with notable people from history re-animated by artificial intelligence.",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "container-title": "NBC News",
      "id": "27937/KPPP2BAQ",
      "issued": {
       "date-parts": [
        [
         2023,
         1,
         20
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|27937/KPPP2BAQ",
      "title": "Chatbot that lets you talk to Jesus and Hitler is latest AI controversy",
      "type": "webpage"
     },
     "27937/L2ILKERU": {
      "ISBN": "9781783266371",
      "abstract": "The Digital Humanities have arrived at a moment when digital Big Data is becoming more readily available, opening exciting new avenues of inquiry but also new challenges. This pioneering book describes and demonstrates the ways these data can be explored to construct cultural heritage knowledge, for research and in teaching and learning. It helps humanities scholars to grasp Big Data in order to do their work, whether that means understanding the underlying algorithms at work in search engines, or designing and using their own tools to process large amounts of information.Demonstrating what digital tools have to offer and also what 'digital' does to how we understand the past, the authors introduce the many different tools and developing approaches in Big Data for historical and humanistic scholarship, show how to use them, what to be wary of, and discuss the kinds of questions and new perspectives this new macroscopic perspective opens up. Authored 'live' online with ongoing feedback from the wider digital history community, Exploring Big Historical Data breaks new ground and sets the direction for the conversation into the future. It represents the current state-of-the-art thinking in the field and exemplifies the way that digital work can enhance public engagement in the humanities.Exploring Big Historical Data should be the go-to resource for undergraduate and graduate students confronted by a vast corpus of data, and researchers encountering these methods for the first time. It will also offer a helping hand to the interested individual seeking to make sense of genealogical data or digitized newspapers, and even the local historical society who are trying to see the value in digitizing their holdings.",
      "author": [
       {
        "family": "Graham",
        "given": "Shawn"
       },
       {
        "family": "Milligan",
        "given": "Ian"
       },
       {
        "family": "Weingart",
        "given": "Scott"
       }
      ],
      "edition": "Reprint edition",
      "event-place": "London",
      "id": "27937/L2ILKERU",
      "issued": {
       "date-parts": [
        [
         2015,
         11,
         16
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "306",
      "publisher": "Icp",
      "publisher-place": "London",
      "shortTitle": "Exploring Big Historical Data",
      "system_id": "zotero|27937/L2ILKERU",
      "title": "Exploring Big Historical Data: The Historian's Macroscope",
      "type": "book"
     },
     "27937/LC63DETW": {
      "DOI": "10.1038/s41467-024-45563-x",
      "URL": "https://www.nature.com/articles/s41467-024-45563-x",
      "abstract": "Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         15
        ]
       ]
      },
      "author": [
       {
        "family": "Dagdelen",
        "given": "John"
       },
       {
        "family": "Dunn",
        "given": "Alexander"
       },
       {
        "family": "Lee",
        "given": "Sanghoon"
       },
       {
        "family": "Walker",
        "given": "Nicholas"
       },
       {
        "family": "Rosen",
        "given": "Andrew S."
       },
       {
        "family": "Ceder",
        "given": "Gerbrand"
       },
       {
        "family": "Persson",
        "given": "Kristin A."
       },
       {
        "family": "Jain",
        "given": "Anubhav"
       }
      ],
      "container-title": "Nature Communications",
      "id": "27937/LC63DETW",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2024,
         2,
         15
        ]
       ]
      },
      "journalAbbreviation": "Nat Commun",
      "language": "en",
      "note": "Publisher: Nature Publishing Group",
      "page": "1418",
      "system_id": "zotero|27937/LC63DETW",
      "title": "Structured information extraction from scientific text with large language models",
      "type": "article-journal",
      "volume": "15"
     },
     "27937/MHRIEHH8": {
      "URL": "http://arxiv.org/abs/2009.11462",
      "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning \"bad\" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Gehman",
        "given": "Samuel"
       },
       {
        "family": "Gururangan",
        "given": "Suchin"
       },
       {
        "family": "Sap",
        "given": "Maarten"
       },
       {
        "family": "Choi",
        "given": "Yejin"
       },
       {
        "family": "Smith",
        "given": "Noah A."
       }
      ],
      "id": "27937/MHRIEHH8",
      "issued": {
       "date-parts": [
        [
         2020,
         9,
         25
        ]
       ]
      },
      "note": "arXiv:2009.11462 [cs]",
      "number": "arXiv:2009.11462",
      "publisher": "arXiv",
      "shortTitle": "RealToxicityPrompts",
      "system_id": "zotero|27937/MHRIEHH8",
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "type": "article"
     },
     "27937/MVDFMR8K": {
      "URL": "https://dl.acm.org/doi/10.1145/3442188.3445922",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Bender",
        "given": "Emily"
       },
       {
        "family": "Gebru",
        "given": "Timnit"
       },
       {
        "family": "McMillan-Major",
        "given": "Angelina"
       },
       {
        "family": "Mitchell",
        "given": "Margaret"
       }
      ],
      "id": "27937/MVDFMR8K",
      "system_id": "zotero|27937/MVDFMR8K",
      "title": "On the Dangers of Stochastic Parrots | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
      "type": "webpage"
     },
     "27937/MYBFNHF8": {
      "URL": "https://lil.law.harvard.edu/blog/2024/02/12/warc-gpt-an-open-source-tool-for-exploring-web-archives-with-ai/",
      "abstract": "Today we’re releasing WARC-GPT: an open-source, highly-customizable Retrieval Augmented Generation tool the web archiving community can use to explore the in...",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "id": "27937/MYBFNHF8",
      "issued": {
       "date-parts": [
        [
         2024,
         2,
         12
        ]
       ]
      },
      "language": "en",
      "shortTitle": "WARC-GPT",
      "system_id": "zotero|27937/MYBFNHF8",
      "title": "WARC-GPT: An Open-Source Tool for Exploring Web Archives Using AI | Library Innovation Lab",
      "type": "webpage"
     },
     "27937/MYFQUX4C": {
      "URL": "https://www.rit.edu/news/artificial-intelligence-aids-cultural-heritage-researchers-documenting-and-teaching-oral",
      "abstract": "The application of artificial intelligence (AI) continues to expand as more people experiment with the technology. Scholars in RIT’s College of Liberal Arts, the RIT Archives, and the Research Computing services are exploring how AI can aid scholars working with oral histories.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         10
        ]
       ]
      },
      "container-title": "RIT",
      "id": "27937/MYFQUX4C",
      "language": "en",
      "system_id": "zotero|27937/MYFQUX4C",
      "title": "Artificial intelligence aids cultural heritage researchers documenting and teaching oral histories",
      "type": "webpage"
     },
     "27937/NYNDVYMM": {
      "URL": "https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business",
      "abstract": "OpenAI's language model, GPT-3, is being used in commercial products and services, but experts worry about embedded bias and toxic language generation",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "id": "27937/NYNDVYMM",
      "language": "en",
      "system_id": "zotero|27937/NYNDVYMM",
      "title": "OpenAI's GPT-3 Speaks! (Kindly Disregard Toxic Language) - IEEE Spectrum",
      "type": "webpage"
     },
     "27937/P2KVKTMZ": {
      "URL": "https://nicolay-honestabes-info.streamlit.app/",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Hutchinson",
        "given": "Daniel"
       }
      ],
      "container-title": "Abraham Gibson, ed., Honest Abe’s Information Emporium.",
      "id": "27937/P2KVKTMZ",
      "issued": {
       "date-parts": [
        [
         2023
        ]
       ]
      },
      "system_id": "zotero|27937/P2KVKTMZ",
      "title": "Nicolay: Exploring the Speeches of Abraham Lincoln with AI",
      "type": "webpage"
     },
     "27937/QPK6D3M9": {
      "URL": "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard",
      "abstract": "Track, rank and evaluate open LLMs and chatbots",
      "accessed": {
       "date-parts": [
        [
         2024,
         9,
         30
        ]
       ]
      },
      "id": "27937/QPK6D3M9",
      "system_id": "zotero|27937/QPK6D3M9",
      "title": "Open LLM Leaderboard 2 - a Hugging Face Space by open-llm-leaderboard",
      "type": "webpage"
     },
     "27937/R5P23ZWU": {
      "URL": "https://nebnewspapers.unl.edu/lccn/2017270209/1890-10-04/ed-1/seq-1/",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "id": "27937/R5P23ZWU",
      "system_id": "zotero|27937/R5P23ZWU",
      "title": "The farmers' alliance. (Lincoln, Nebraska) 1889-1892, October 04, 1890, Image 1 « Nebraska Newspapers",
      "type": "webpage"
     },
     "27937/RJTNQXZP": {
      "DOI": "10.48550/arXiv.2402.14207",
      "URL": "http://arxiv.org/abs/2402.14207",
      "abstract": "We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Shao",
        "given": "Yijia"
       },
       {
        "family": "Jiang",
        "given": "Yucheng"
       },
       {
        "family": "Kanell",
        "given": "Theodore A."
       },
       {
        "family": "Xu",
        "given": "Peter"
       },
       {
        "family": "Khattab",
        "given": "Omar"
       },
       {
        "family": "Lam",
        "given": "Monica S."
       }
      ],
      "id": "27937/RJTNQXZP",
      "issued": {
       "date-parts": [
        [
         2024,
         4,
         8
        ]
       ]
      },
      "note": "arXiv:2402.14207",
      "number": "arXiv:2402.14207",
      "publisher": "arXiv",
      "system_id": "zotero|27937/RJTNQXZP",
      "title": "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models",
      "type": "article"
     },
     "27937/RRLN9TF5": {
      "URL": "https://huggingface.co/spaces/uonlp/open_multilingual_llm_leaderboard",
      "abstract": "Discover amazing ML apps made by the community",
      "accessed": {
       "date-parts": [
        [
         2024,
         9,
         30
        ]
       ]
      },
      "id": "27937/RRLN9TF5",
      "system_id": "zotero|27937/RRLN9TF5",
      "title": "Open Multilingual Llm Leaderboard - a Hugging Face Space by uonlp",
      "type": "webpage"
     },
     "27937/S3ADX5DD": {
      "ISBN": "9780190067397",
      "URL": "https://doi.org/10.1093/oxfordhb/9780190067397.013.16",
      "abstract": "This chapter discusses the role of race and gender in artificial intelligence (AI). The rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial automated facial analysis systems have much higher error rates for dark-skinned women, while having minimal errors on light-skinned men. Moreover, a 2016 ProPublica investigation uncovered that machine learning–based tools that assess crime recidivism rates in the United States are biased against African Americans. Other studies show that natural language–processing tools trained on news articles exhibit societal biases. While many technical solutions have been proposed to alleviate bias in machine learning systems, a holistic and multifaceted approach must be taken. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Gebru",
        "given": "Timnit"
       }
      ],
      "container-title": "The Oxford Handbook of Ethics of AI",
      "editor": [
       {
        "family": "Dubber",
        "given": "Markus D."
       },
       {
        "family": "Pasquale",
        "given": "Frank"
       },
       {
        "family": "Das",
        "given": "Sunit"
       }
      ],
      "id": "27937/S3ADX5DD",
      "issued": {
       "date-parts": [
        [
         2020,
         7,
         9
        ]
       ]
      },
      "note": "DOI: 10.1093/oxfordhb/9780190067397.013.16",
      "page": "0",
      "publisher": "Oxford University Press",
      "system_id": "zotero|27937/S3ADX5DD",
      "title": "Race and Gender",
      "type": "chapter"
     },
     "27937/TGPDB8WX": {
      "DOI": "10.48550/arXiv.1706.03741",
      "URL": "http://arxiv.org/abs/1706.03741",
      "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         1
        ]
       ]
      },
      "author": [
       {
        "family": "Christiano",
        "given": "Paul"
       },
       {
        "family": "Leike",
        "given": "Jan"
       },
       {
        "family": "Brown",
        "given": "Tom B."
       },
       {
        "family": "Martic",
        "given": "Miljan"
       },
       {
        "family": "Legg",
        "given": "Shane"
       },
       {
        "family": "Amodei",
        "given": "Dario"
       }
      ],
      "id": "27937/TGPDB8WX",
      "issued": {
       "date-parts": [
        [
         2023,
         2,
         17
        ]
       ]
      },
      "note": "arXiv:1706.03741 [cs, stat]",
      "number": "arXiv:1706.03741",
      "publisher": "arXiv",
      "system_id": "zotero|27937/TGPDB8WX",
      "title": "Deep reinforcement learning from human preferences",
      "type": "article"
     },
     "27937/TIAJYHF6": {
      "DOI": "10.1108/JD-07-2018-0114",
      "URL": "https://doi.org/10.1108/JD-07-2018-0114",
      "abstract": "Purpose An overview of the current use of handwritten text recognition (HTR) on archival manuscript material, as provided by the EU H2020 funded Transkribus platform. It explains HTR, demonstrates Transkribus, gives examples of use cases, highlights the affect HTR may have on scholarship, and evidences this turning point of the advanced use of digitised heritage content. The paper aims to discuss these issues. Design/methodology/approach This paper adopts a case study approach, using the development and delivery of the one openly available HTR platform for manuscript material. Findings Transkribus has demonstrated that HTR is now a useable technology that can be employed in conjunction with mass digitisation to generate accurate transcripts of archival material. Use cases are demonstrated, and a cooperative model is suggested as a way to ensure sustainability and scaling of the platform. However, funding and resourcing issues are identified. Research limitations/implications The paper presents results from projects: further user studies could be undertaken involving interviews, surveys, etc. Practical implications Only HTR provided via Transkribus is covered: however, this is the only publicly available platform for HTR on individual collections of historical documents at time of writing and it represents the current state-of-the-art in this field. Social implications The increased access to information contained within historical texts has the potential to be transformational for both institutions and individuals. Originality/value This is the first published overview of how HTR is used by a wide archival studies community, reporting and showcasing current application of handwriting technology in the cultural heritage sector.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Muehlberger",
        "given": "Guenter"
       },
       {
        "family": "Seaward",
        "given": "Louise"
       },
       {
        "family": "Terras",
        "given": "Melissa"
       },
       {
        "family": "Ares Oliveira",
        "given": "Sofia"
       },
       {
        "family": "Bosch",
        "given": "Vicente"
       },
       {
        "family": "Bryan",
        "given": "Maximilian"
       },
       {
        "family": "Colutto",
        "given": "Sebastian"
       },
       {
        "family": "Déjean",
        "given": "Hervé"
       },
       {
        "family": "Diem",
        "given": "Markus"
       },
       {
        "family": "Fiel",
        "given": "Stefan"
       },
       {
        "family": "Gatos",
        "given": "Basilis"
       },
       {
        "family": "Greinoecker",
        "given": "Albert"
       },
       {
        "family": "Grüning",
        "given": "Tobias"
       },
       {
        "family": "Hackl",
        "given": "Guenter"
       },
       {
        "family": "Haukkovaara",
        "given": "Vili"
       },
       {
        "family": "Heyer",
        "given": "Gerhard"
       },
       {
        "family": "Hirvonen",
        "given": "Lauri"
       },
       {
        "family": "Hodel",
        "given": "Tobias"
       },
       {
        "family": "Jokinen",
        "given": "Matti"
       },
       {
        "family": "Kahle",
        "given": "Philip"
       },
       {
        "family": "Kallio",
        "given": "Mario"
       },
       {
        "family": "Kaplan",
        "given": "Frederic"
       },
       {
        "family": "Kleber",
        "given": "Florian"
       },
       {
        "family": "Labahn",
        "given": "Roger"
       },
       {
        "family": "Lang",
        "given": "Eva Maria"
       },
       {
        "family": "Laube",
        "given": "Sören"
       },
       {
        "family": "Leifert",
        "given": "Gundram"
       },
       {
        "family": "Louloudis",
        "given": "Georgios"
       },
       {
        "family": "McNicholl",
        "given": "Rory"
       },
       {
        "family": "Meunier",
        "given": "Jean-Luc"
       },
       {
        "family": "Michael",
        "given": "Johannes"
       },
       {
        "family": "Mühlbauer",
        "given": "Elena"
       },
       {
        "family": "Philipp",
        "given": "Nathanael"
       },
       {
        "family": "Pratikakis",
        "given": "Ioannis"
       },
       {
        "family": "Puigcerver Pérez",
        "given": "Joan"
       },
       {
        "family": "Putz",
        "given": "Hannelore"
       },
       {
        "family": "Retsinas",
        "given": "George"
       },
       {
        "family": "Romero",
        "given": "Verónica"
       },
       {
        "family": "Sablatnig",
        "given": "Robert"
       },
       {
        "family": "Sánchez",
        "given": "Joan Andreu"
       },
       {
        "family": "Schofield",
        "given": "Philip"
       },
       {
        "family": "Sfikas",
        "given": "Giorgos"
       },
       {
        "family": "Sieber",
        "given": "Christian"
       },
       {
        "family": "Stamatopoulos",
        "given": "Nikolaos"
       },
       {
        "family": "Strauß",
        "given": "Tobias"
       },
       {
        "family": "Terbul",
        "given": "Tamara"
       },
       {
        "family": "Toselli",
        "given": "Alejandro Héctor"
       },
       {
        "family": "Ulreich",
        "given": "Berthold"
       },
       {
        "family": "Villegas",
        "given": "Mauricio"
       },
       {
        "family": "Vidal",
        "given": "Enrique"
       },
       {
        "family": "Walcher",
        "given": "Johanna"
       },
       {
        "family": "Weidemann",
        "given": "Max"
       },
       {
        "family": "Wurster",
        "given": "Herbert"
       },
       {
        "family": "Zagoris",
        "given": "Konstantinos"
       }
      ],
      "container-title": "Journal of Documentation",
      "id": "27937/TIAJYHF6",
      "issue": "5",
      "issued": {
       "date-parts": [
        [
         2019,
         1,
         1
        ]
       ]
      },
      "page": "954-976",
      "shortTitle": "Transforming scholarship in the archives through handwritten text recognition",
      "system_id": "zotero|27937/TIAJYHF6",
      "title": "Transforming scholarship in the archives through handwritten text recognition: Transkribus as a case study",
      "type": "article-journal",
      "volume": "75"
     },
     "27937/TPGPSRAI": {
      "URL": "https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd",
      "abstract": "[Now available as an “audiopaper” on my soundcloud. (Please excuse occasional noise from airplanes overhead + my inconsistency about…",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Bender",
        "given": "Emily M."
       }
      ],
      "container-title": "Medium",
      "id": "27937/TPGPSRAI",
      "issued": {
       "date-parts": [
        [
         2022,
         5,
         2
        ]
       ]
      },
      "language": "en",
      "shortTitle": "On NYT Magazine on AI",
      "system_id": "zotero|27937/TPGPSRAI",
      "title": "On NYT Magazine on AI: Resist the Urge to be Impressed",
      "type": "post-weblog"
     },
     "27937/U534FF7L": {
      "URL": "http://arxiv.org/abs/2303.08774",
      "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "OpenAI",
        "given": ""
       }
      ],
      "id": "27937/U534FF7L",
      "issued": {
       "date-parts": [
        [
         2023,
         3,
         27
        ]
       ]
      },
      "note": "arXiv:2303.08774 [cs]",
      "number": "arXiv:2303.08774",
      "publisher": "arXiv",
      "system_id": "zotero|27937/U534FF7L",
      "title": "GPT-4 Technical Report",
      "type": "article"
     },
     "27937/UHZYQM3W": {
      "URL": "https://www.newyorker.com/tech/annals-of-technology/whispers-of-ais-modular-future",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         10
        ]
       ]
      },
      "author": [
       {
        "family": "Somers",
        "given": "James"
       }
      ],
      "id": "27937/UHZYQM3W",
      "issued": {
       "date-parts": [
        [
         2023,
         2,
         1
        ]
       ]
      },
      "system_id": "zotero|27937/UHZYQM3W",
      "title": "Whispers of A.I.’s Modular Future | The New Yorker",
      "type": "webpage"
     },
     "27937/UYVGUT4C": {
      "URL": "http://arxiv.org/abs/2103.00020",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Radford",
        "given": "Alec"
       },
       {
        "family": "Kim",
        "given": "Jong Wook"
       },
       {
        "family": "Hallacy",
        "given": "Chris"
       },
       {
        "family": "Ramesh",
        "given": "Aditya"
       },
       {
        "family": "Goh",
        "given": "Gabriel"
       },
       {
        "family": "Agarwal",
        "given": "Sandhini"
       },
       {
        "family": "Sastry",
        "given": "Girish"
       },
       {
        "family": "Askell",
        "given": "Amanda"
       },
       {
        "family": "Mishkin",
        "given": "Pamela"
       },
       {
        "family": "Clark",
        "given": "Jack"
       },
       {
        "family": "Krueger",
        "given": "Gretchen"
       },
       {
        "family": "Sutskever",
        "given": "Ilya"
       }
      ],
      "id": "27937/UYVGUT4C",
      "issued": {
       "date-parts": [
        [
         2021,
         2,
         26
        ]
       ]
      },
      "note": "arXiv:2103.00020 [cs]",
      "number": "arXiv:2103.00020",
      "publisher": "arXiv",
      "system_id": "zotero|27937/UYVGUT4C",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "type": "article"
     },
     "27937/VEDFUUBA": {
      "URL": "http://arxiv.org/abs/2303.13375",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Nori",
        "given": "Harsha"
       },
       {
        "family": "King",
        "given": "Nicholas"
       },
       {
        "family": "McKinney",
        "given": "Scott Mayer"
       },
       {
        "family": "Carignan",
        "given": "Dean"
       },
       {
        "family": "Horvitz",
        "given": "Eric"
       }
      ],
      "id": "27937/VEDFUUBA",
      "issued": {
       "date-parts": [
        [
         2023,
         3,
         20
        ]
       ]
      },
      "note": "arXiv:2303.13375 [cs]",
      "number": "arXiv:2303.13375",
      "publisher": "arXiv",
      "system_id": "zotero|27937/VEDFUUBA",
      "title": "Capabilities of GPT-4 on Medical Challenge Problems",
      "type": "article"
     },
     "27937/VHJBTADE": {
      "ISBN": "978-1-324-09112-7",
      "abstract": "\"A brilliant, revelatory account of the Cold War origins of the data-mad, algorithmic twenty-first century, from the author of the acclaimed international bestseller, These Truths. The Simulmatics Corporation, founded in 1959, mined data, targeted voters, accelerated news, manipulated consumers, destabilized politics, and disordered knowledge--decades before Facebook, Google, Amazon, and Cambridge Analytica. Silicon Valley likes to imagine it has no past but the scientists of Simulmatics are the long-dead grandfathers of Mark Zuckerberg and Elon Musk. Borrowing from psychological warfare, they used computers to predict and direct human behavior, deploying their \"People Machine\" from New York, Cambridge, and Saigon for clients that included John Kennedy's presidential campaign, the New York Times, Young & Rubicam, and, during the Vietnam War, the Department of Defense. Jill Lepore, distinguished Harvard historian and New Yorker staff writer, unearthed from the archives the almost unbelievable story of this long-vanished corporation, and of the women hidden behind it. In the 1950s and 1960s, Lepore argues, Simulmatics invented the future by building the machine in which the world now finds itself trapped and tormented, algorithm by algorithm\"-- Provided by publisher",
      "author": [
       {
        "family": "Lepore",
        "given": "Jill"
       }
      ],
      "event-place": "New York, NY",
      "id": "27937/VHJBTADE",
      "issued": {
       "date-parts": [
        [
         2021
        ]
       ]
      },
      "language": "eng",
      "note": "OCLC: 1233267158",
      "number-of-pages": "415",
      "publisher": "Liveright Publishing Corporation, a division of W.W. Norton & Company",
      "publisher-place": "New York, NY",
      "shortTitle": "If then",
      "system_id": "zotero|27937/VHJBTADE",
      "title": "If then: how the Simulmatics Corporation invented the future",
      "type": "book"
     },
     "27937/VXGSAGTI": {
      "ISBN": "9780471268512",
      "abstract": "Written for practitioners of data mining, data cleaning and database management.Presents a technical treatment of data quality including process, metrics, tools and algorithms.Focuses on developing an evolving modeling strategy through an iterative data exploration loop and incorporation of domain knowledge.Addresses methods of detecting, quantifying and correcting data quality issues that can have a significant impact on findings and decisions, using commercially available tools as well as new algorithmic approaches.Uses case studies to illustrate applications in real life scenarios.Highlights new approaches and methodologies, such as the DataSphere space partitioning and summary based analysis techniques.Exploratory Data Mining and Data Cleaning will serve as an important reference for serious data analysts who need to analyze large amounts of unfamiliar data, managers of operations databases, and students in undergraduate or graduate level courses dealing with large scale data analys is and data mining.",
      "author": [
       {
        "family": "Dasu",
        "given": "Tamraparni"
       },
       {
        "family": "Johnson",
        "given": "Theodore"
       }
      ],
      "edition": "1st edition",
      "event-place": "New York",
      "id": "27937/VXGSAGTI",
      "issued": {
       "date-parts": [
        [
         2003,
         5,
         9
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "224",
      "publisher": "Wiley-Interscience",
      "publisher-place": "New York",
      "system_id": "zotero|27937/VXGSAGTI",
      "title": "Exploratory Data Mining and Data Cleaning",
      "type": "book"
     },
     "27937/X4D92B7V": {
      "ISBN": "9781509526406",
      "abstract": "From everyday apps to complex algorithms, Ruha Benjamin cuts through tech-industry hype to understand how emerging technologies can reinforce White supremacy and deepen social inequity.Benjamin argues that automation, far from being a sinister story of racist programmers scheming on the dark web, has the potential to hide, speed up, and deepen discrimination while appearing neutral and even benevolent when compared to the racism of a previous era. Presenting the concept of the “New Jim Code,” she shows how a range of discriminatory designs encode inequity by explicitly amplifying racial hierarchies; by ignoring but thereby replicating social divisions; or by aiming to fix racial bias but ultimately doing quite the opposite. Moreover, she makes a compelling case for race itself as a kind of technology, designed to stratify and sanctify social injustice in the architecture of everyday life.This illuminating guide provides conceptual tools for decoding tech promises with sociologically informed skepticism. In doing so, it challenges us to question not only the technologies we are sold but also the ones we ourselves manufacture.",
      "author": [
       {
        "family": "Benjamin",
        "given": "Ruha"
       }
      ],
      "edition": "1st edition",
      "event-place": "Medford, MA",
      "id": "27937/X4D92B7V",
      "issued": {
       "date-parts": [
        [
         2019,
         6,
         17
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "172",
      "publisher": "Polity",
      "publisher-place": "Medford, MA",
      "shortTitle": "Race After Technology",
      "system_id": "zotero|27937/X4D92B7V",
      "title": "Race After Technology: Abolitionist Tools for the New Jim Code",
      "type": "book"
     },
     "27937/XBWIZZJJ": {
      "URL": "http://arxiv.org/abs/2304.03442",
      "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Park",
        "given": "Joon Sung"
       },
       {
        "family": "O'Brien",
        "given": "Joseph C."
       },
       {
        "family": "Cai",
        "given": "Carrie J."
       },
       {
        "family": "Morris",
        "given": "Meredith Ringel"
       },
       {
        "family": "Liang",
        "given": "Percy"
       },
       {
        "family": "Bernstein",
        "given": "Michael S."
       }
      ],
      "id": "27937/XBWIZZJJ",
      "issued": {
       "date-parts": [
        [
         2023,
         8,
         6
        ]
       ]
      },
      "note": "arXiv:2304.03442",
      "number": "arXiv:2304.03442",
      "publisher": "arXiv",
      "shortTitle": "Generative Agents",
      "system_id": "zotero|27937/XBWIZZJJ",
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "type": "article"
     },
     "27937/XEUKQDPE": {
      "DOI": "10.18637/jss.v059.i10",
      "URL": "https://doi.org/10.18637/jss.v059.i10",
      "abstract": "A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         28
        ]
       ]
      },
      "author": [
       {
        "family": "Wickham",
        "given": "Hadley"
       }
      ],
      "container-title": "Journal of Statistical Software",
      "id": "27937/XEUKQDPE",
      "issued": {
       "date-parts": [
        [
         2014,
         9,
         12
        ]
       ]
      },
      "language": "en",
      "page": "1-23",
      "system_id": "zotero|27937/XEUKQDPE",
      "title": "Tidy Data",
      "type": "article-journal",
      "volume": "59"
     },
     "27937/XQYUJV5F": {
      "DOI": "10.1093/ahr/rhad362",
      "URL": "https://academic.oup.com/ahr/article/128/3/1345/7282240",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         25
        ]
       ]
      },
      "author": [
       {
        "family": "Meadows",
        "given": "R. Darrell"
       },
       {
        "family": "Sternfeld",
        "given": "Joshua"
       }
      ],
      "container-title": "The American Historical Review",
      "id": "27937/XQYUJV5F",
      "issue": "3",
      "issued": {
       "date-parts": [
        [
         2023,
         9,
         26
        ]
       ]
      },
      "language": "en",
      "page": "1345-1349",
      "system_id": "zotero|27937/XQYUJV5F",
      "title": "Artificial Intelligence and the Practice of History",
      "type": "article-journal",
      "volume": "128"
     },
     "27937/YVTAGDKZ": {
      "ISBN": "9780300209570",
      "abstract": "The hidden costs of artificial intelligence—from natural resources and labor to privacy, equality, and freedom\"This study argues that [artificial intelligence] is neither artificial nor particularly intelligent. . . . A fascinating history of the data on which machine-learning systems are trained.\"—New Yorker\"A valuable corrective to much of the hype surrounding AI and a useful instruction manual for the future.\"—John Thornhill, Financial Times\"It’s a masterpiece, and I haven’t been able to stop thinking about it.\"—Karen Hao, senior editor, MIT Tech Review What happens when artificial intelligence saturates political life and depletes the planet? How is AI shaping our understanding of ourselves and our societies? Drawing on more than a decade of research, award‑winning scholar Kate Crawford reveals how AI is a technology of extraction: from the minerals drawn from the earth, to the labor pulled from low-wage information workers, to the data taken from every action and expression. This book reveals how this planetary network is fueling a shift toward undemocratic governance and increased inequity. Rather than taking a narrow focus on code and algorithms, Crawford offers us a material and political perspective on what it takes to make AI and how it centralizes power. This is an urgent account of what is at stake as technology companies use artificial intelligence to reshape the world.",
      "author": [
       {
        "family": "Crawford",
        "given": "Kate"
       }
      ],
      "event-place": "New Haven",
      "id": "27937/YVTAGDKZ",
      "issued": {
       "date-parts": [
        [
         2021,
         4,
         6
        ]
       ]
      },
      "language": "English",
      "number-of-pages": "336",
      "publisher": "Yale University Press",
      "publisher-place": "New Haven",
      "shortTitle": "Atlas of AI",
      "system_id": "zotero|27937/YVTAGDKZ",
      "title": "Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence",
      "type": "book"
     },
     "27937/YWJAQ4V8": {
      "ISBN": "978-0-19-803513-8",
      "abstract": "Oral history is vital to our understanding of the cultures and experiences of the past. Unlike written history, oral history forever captures people's feelings, expressions, and nuances of language. But what exactly is oral history? How reliable is the information gathered by oral history? And what does it take to become an oral historian? Donald A. Ritchie, a leading expert in the field, answers these questions and in particular, explains the principles and guidelines created by the Oral History Association to ensure the professional standards of oral historians. Doing Oral History has becom",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         9
        ]
       ]
      },
      "author": [
       {
        "family": "Ritchie",
        "given": "Donald A."
       }
      ],
      "edition": "2nd ed",
      "event-place": "Cary",
      "id": "27937/YWJAQ4V8",
      "issued": {
       "date-parts": [
        [
         2003
        ]
       ]
      },
      "language": "eng",
      "note": "OCLC: 1049804116",
      "number-of-pages": "290",
      "publisher": "Oxford University Press, USA",
      "publisher-place": "Cary",
      "shortTitle": "Doing Oral History",
      "system_id": "zotero|27937/YWJAQ4V8",
      "title": "Doing Oral History: a Practical Guide",
      "type": "book"
     },
     "27937/YZQEASUC": {
      "URL": "https://aiandwriting.hcommons.org/",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         23
        ]
       ]
      },
      "id": "27937/YZQEASUC",
      "language": "en-US",
      "system_id": "zotero|27937/YZQEASUC",
      "title": "MLA-CCCC Joint Task Force on Writing and AI",
      "type": "post-weblog"
     },
     "27937/Z44J4BKC": {
      "DOI": "10.1093/jamia/ocad259",
      "URL": "https://doi.org/10.1093/jamia/ocad259",
      "abstract": "The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets.This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance.We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT.Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed.The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings.While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         16
        ]
       ]
      },
      "author": [
       {
        "family": "Hu",
        "given": "Yan"
       },
       {
        "family": "Chen",
        "given": "Qingyu"
       },
       {
        "family": "Du",
        "given": "Jingcheng"
       },
       {
        "family": "Peng",
        "given": "Xueqing"
       },
       {
        "family": "Keloth",
        "given": "Vipina Kuttichi"
       },
       {
        "family": "Zuo",
        "given": "Xu"
       },
       {
        "family": "Zhou",
        "given": "Yujia"
       },
       {
        "family": "Li",
        "given": "Zehan"
       },
       {
        "family": "Jiang",
        "given": "Xiaoqian"
       },
       {
        "family": "Lu",
        "given": "Zhiyong"
       },
       {
        "family": "Roberts",
        "given": "Kirk"
       },
       {
        "family": "Xu",
        "given": "Hua"
       }
      ],
      "container-title": "Journal of the American Medical Informatics Association",
      "id": "27937/Z44J4BKC",
      "issue": "9",
      "issued": {
       "date-parts": [
        [
         2024,
         9,
         1
        ]
       ]
      },
      "journalAbbreviation": "Journal of the American Medical Informatics Association",
      "page": "1812-1820",
      "system_id": "zotero|27937/Z44J4BKC",
      "title": "Improving large language models for clinical named entity recognition via prompt engineering",
      "type": "article-journal",
      "volume": "31"
     },
     "27937/ZICATXAV": {
      "URL": "https://gradio.app/",
      "accessed": {
       "date-parts": [
        [
         2024,
         9,
         30
        ]
       ]
      },
      "id": "27937/ZICATXAV",
      "language": "en-US",
      "shortTitle": "Chatbot Arena (formerly LMSYS)",
      "system_id": "zotero|27937/ZICATXAV",
      "title": "Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots",
      "type": "webpage"
     },
     "27937/ZJW9AI49": {
      "DOI": "10.3138/chr.694",
      "URL": "https://www.utpjournals.press/doi/abs/10.3138/chr.694",
      "abstract": "It all seems so orderly and comprehensive. Instead of firing up the microfilm reader to navigate the Globe and Mail or the Toronto Star, one needs only to log into online newspaper databases. A keyword search, for a particular event, person, or cultural phenomenon, brings up a list of research findings. Previously impossible research projects can now be attempted. This process has fundamentally reshaped Canadian historical scholarship. We can see this in Canadian history dissertations. In 1998, a year with 67 dissertations, the Toronto Star was cited 74 times. However it was cited 753 times in 2010, a year with 69 dissertations. Similar data appears in the Canadian Historical Review (CHR), a prestigious peer-reviewed journal. Databases are skewing our research. We are witnessing the application of commercial Optical Character Recognition (OCR) technology – originally and primarily designed for the efficient digitization of large reams of corporate and legal documents, conventionally formatted – to historical sources. The results are, unsurprisingly, a mixed bag. In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely – if ever – made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.",
      "accessed": {
       "date-parts": [
        [
         2023,
         3,
         31
        ]
       ]
      },
      "author": [
       {
        "family": "Milligan",
        "given": "Ian"
       }
      ],
      "container-title": "Canadian Historical Review",
      "id": "27937/ZJW9AI49",
      "issue": "4",
      "issued": {
       "date-parts": [
        [
         2013,
         12
        ]
       ]
      },
      "page": "540-569",
      "shortTitle": "Illusionary Order",
      "system_id": "zotero|27937/ZJW9AI49",
      "title": "Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010",
      "type": "article-journal",
      "volume": "94"
     },
     "27937/ZS9JDNGD": {
      "URL": "http://arxiv.org/abs/2009.03300",
      "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
      "accessed": {
       "date-parts": [
        [
         2023,
         4,
         2
        ]
       ]
      },
      "author": [
       {
        "family": "Hendrycks",
        "given": "Dan"
       },
       {
        "family": "Burns",
        "given": "Collin"
       },
       {
        "family": "Basart",
        "given": "Steven"
       },
       {
        "family": "Zou",
        "given": "Andy"
       },
       {
        "family": "Mazeika",
        "given": "Mantas"
       },
       {
        "family": "Song",
        "given": "Dawn"
       },
       {
        "family": "Steinhardt",
        "given": "Jacob"
       }
      ],
      "id": "27937/ZS9JDNGD",
      "issued": {
       "date-parts": [
        [
         2021,
         1,
         12
        ]
       ]
      },
      "note": "arXiv:2009.03300 [cs]",
      "number": "arXiv:2009.03300",
      "publisher": "arXiv",
      "system_id": "zotero|27937/ZS9JDNGD",
      "title": "Measuring Massive Multitask Language Understanding",
      "type": "article"
     },
     "27937/ZXTQBIJU": {
      "URL": "https://huggingface.co/datasets/PleIAs/Post-OCR-Correction",
      "abstract": "We’re on a journey to advance and democratize artificial intelligence through open source and open science.",
      "accessed": {
       "date-parts": [
        [
         2024,
         10,
         14
        ]
       ]
      },
      "id": "27937/ZXTQBIJU",
      "issued": {
       "date-parts": [
        [
         2024,
         6,
         9
        ]
       ]
      },
      "system_id": "zotero|27937/ZXTQBIJU",
      "title": "PleIAs/Post-OCR-Correction · Datasets at Hugging Face",
      "type": "webpage"
     }
    }
   },
   "style": "chicago-note-bibliography.csl"
  },
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
